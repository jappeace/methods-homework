\documentclass{article}
\usepackage{txfonts}
\usepackage{booktabs}
\usepackage{color}
\usepackage{bussproofs}
\usepackage{graphicx}
\usepackage{pifont}
\usepackage{qtree}
\usepackage{tikz}
\usepackage{listings}
\usepackage{hyperref}
\newenvironment{scprooftree}[1]%
{\gdef\scalefactor{#1}\begin{center}\proofSkipAmount \leavevmode}%
{\scalebox{\scalefactor}{\DisplayProof}\proofSkipAmount \end{center} }

\begin{document}
\lstset{language=Java}
\author{Jappie Klooster}
\title{Agents and you, a brief summary}
\maketitle

\section{Introduction}
This will be an introduction to agents coming from someone who followed
` hbo informatica' as pre study.

In here I will write down the things I think are important.

\section{Modal logic}


\section{APL}
Medhi dastani taught about the agent programming langauge (APL).
\subsection{Agent data structure}
This represents the mental state of a agent. \\
\begin{tabular}{ll}
	Name & Does what \\ \toprule
	Beliefs & Information available to the agent \\
	Goals & Objectives that the agent wants to achieve \\
	Events & Obsevations of (environmental) changes \\
	Capabilities & Actions that the agent can perform \\
	Plans & Procedures to achieve objectives \\
	Reasoning rules & Reason about goal and plans \\
\end{tabular} \\

\noindent
The rules can be:

\begin{itemize}
	\item Planning rules (goal $\to$ plan)
	\item Procedural rules (events $\to$ plan)
	\item Plan repair rules (plan $\to$ plan)
\end{itemize}

\subsection{Processing mental states}

\begin{itemize}
	\item Generate plans for received events.
	\item Generate plans for goals.
	\item Process exceptions and handle failures.
	\item Repair plans.
	\item Select plans for execution.
	\item Execute plans.
\end{itemize}

\subsection{Operational semantics}
Operational semantics defines the computation steps a program
configuration may take. Operational semantics allows studying
of programming constructs in a rigorous manner. It facilitates proving
general properties about a language. It lies close to the implementation
of an interpreter. It facilitates model checking.

\subsubsection{Basic syntax}
The configuration $C$ evolves into configuration $C'$:

\[C \to C'\]

If premise $P$ holds transition $C \to C'$ can be derived:

\begin{prooftree}
	\AxiomC{$P$}
	\UnaryInfC{$C \to C'$}
\end{prooftree}

\subsection{Configuration}
configurations only mention things that change.
Multi-agent configuration where $A$ is an agent and $\chi$ is the
environment:

\[\langle \{A_1, \cdots, A_n\}, \chi\rangle \]

Individual agent configuration where $i$ is the agent number, $\sigma$ the
believes, $\gamma$ the goals, $\Pi$ the plans $\theta$ the assignments and
$\xi$ the events:

\[A_i = \langle i, \sigma_i, \gamma_i, \Pi_i, \theta_i, \xi_i \rangle \]

\subsection{Transitions}
The confiugrations described \emph{what} can change where transition
will describe \emph{how} they can change.

$\vDash$ Describes a first order entailment relation. As I understand
this means $A \vDash B$ there exsits a $B$ in $A$ which has an entailment
to $A$. So $A$ is a set of things and a $B$ like thing exists in $A$.
\subsubsection{Belief update}
Execution of a belief update action $\alpha$ modifies the belief and goal
bases. $Update$ is an update function that is specified by the believe
update actions and $\gamma'=\gamma - \{\psi \in \gamma | \sigma' \vDash \psi\}$.
Which basically says that $\gamma'$ is equal to $\gamma$ minus the update.

\begin{prooftree}
	\AxiomC{$Update(\sigma, \alpha\theta)=\sigma'$}
	\UnaryInfC{$\langle i, \sigma, \gamma, \{(\alpha,id)\}, \theta, 
		\xi\rangle \to 
	\langle \langle i, \sigma', \gamma', \{\}, \theta, \xi\rangle$}

\end{prooftree}

So on the left side of the arrow, the plan is to do a believeUpdate $\alpha$
on the right side this is done so its removed from the plans and the believes
\emph{and} goals are modified. The goals are modified to because if a believe
with the same name as a goal becomes true than the goal is removed. (this is
a design decision btw, its just how 2apl works).

\subsubsection{Adopta goal}
Execution of a goal adopt action adopta(g) adds the goal to the beginning of
the goal base. If the goal $g$ trough assignment $\theta$ does not already
exists in goalbase.

\begin{prooftree}
	\AxiomC{$\sigma \nvDash g\theta$}
	\UnaryInfC{$\langle 
		i, \sigma, [\gamma_1, \cdots, y_n], \{(adopta(g),id)\}, \theta, \xi, 
	\rangle \to \langle 
		i, \sigma, [g\theta,\gamma_1, \cdots, y_n], \{\}, \theta, \xi, 
	\rangle$}
\end{prooftree}

So the agent comes accross a adopta method in the plan base $\Pi$, if
the associated believe of the goal is not true then you can start adding
it.
\subsubsection{Applying planning goal rules}
The structure of a PG-rule in 2apl is as follows:

\[\kappa <- \beta | \pi \]

Where $\kappa$ is the name of the plan.
$\beta$ is the condition that is applied before
executing the plan (basically a big if around the plan). $\pi$ is
the body of the plan, a set of instructions.

\begin{prooftree}
	\AxiomC{$\gamma \vDash \kappa \tau_1 \quad \& \quad
	\sigma \vDash \beta \tau_1 \tau_2$}
	\UnaryInfC{$\langle
		i, \sigma, \gamma, \Pi, \theta, \xi
	\rangle \to \langle
	i, \sigma, \gamma, \Pi \cup \{ \pi \tau_1 \tau_2 \}, \theta, \xi
	\rangle$}
\end{prooftree}

The transition can be executed if $\kappa$ is an active goal (this valuation
is stored in $\tau_1$ \emph{and} $\beta$ is true according to the believe 
base (this eveluation is stored in $\tau_2$). If all these things are true
then $\pi$ is added to the plan base $\Pi$ \emph{but} it will only be executed
if $\tau_1$ and $\tau_2$ return true.
\subsection{Exam}
some notes I gathered for possible exam questions:

What happens if $\gamma = pos(2,3)$ and $\sigma = pos(4,5)$, resolve with
state transitions.

\section{Markov}
Computerised decision makers must deal with an uncertain world.

\subsection{Expectation}
Excpectation is the average value you excpect to happen. For example if you
throw a dice the average value you should get is 3.5. Because:

\[1 * 1/6 + 2 * 1/6 + 3 * 1/6 + 4 * 1/6 + 5 * 1/6 + 6 * 1/6 = 3.5\]

You multiple all possible values time the chance of them happening and then
calculate the sum of them. Formally it is written like this:

\[ E[X]=Def \sum_{all x} xP\{X=x\} \]

So you can fill it in like:
\[E[X]= 1 * P\{X=1\} + \cdots + 6 *P\{X=6\}\]
\[E[X]= 1 * 1/6 + \cdots + 6 *1/6 = 3.5\]

\subsubsection{Properties of expectation}

\begin{itemize}
	\item $E[c] = c$
	\item $E[X+Y] = E[X] + E[Y]$
	\item $E[cX] = cE[X]$
	\item $E[XY] = E[X]E[Y]$ if and only if $X$ and $Y$ are independent.
	\item $E[X] \le E[Y]$, if $X \le Y$ almost surely.
\end{itemize}

\paragraph{Law of the onconcious statistician}
You have to prove this, its \emph{not} a property:

\[E[g(X)]=\sum_{all x} g(x)P{X=x}\]

where $g$ is any real-valued function. (It returns a value that is part
of the real set of numbers).

\subsection{Conditional expectation}
The expectation of $X | Y = y$ is called the \emph{conditional expectation}
of $X$ given $Y$, and is denoted by:

\[E[X|Y=y]\]

$E[X|Y=y]$ Is a number that depends on $y \in Y$. So you only get an
expectation $X$ if you have an y that satisfies the condition.
This yields a random variable $E[X|Y]$. Because of this it has all the
properties of a random variable.

\subsubsection{Probability mass functions}
The standart notation for probability mass functions is:

\[p_x(x): D(X) \to [0,1]: x \mapsto P\{X=x\} \]
\[p_y(y): D(Y) \to [0,1]: y \mapsto P\{Y=y\} \]
\[p_{xy}(x,y): D(X) \times D(Y) \to [0,1]: (x,y) \mapsto P\{X=x,Y=y\} \]
\[p_{x|y}(x|y): D(X) \times D(Y) \to [0,1]: (x,y) \mapsto P\{X=x|Y=y\} \]

For example:

\[ P\{X=x|Y=y\}=\frac{P\{X=x,Y=y\}}{P\{Y=y\}}\]

Is often written as:

\[P_{x|y}(x|y)=\frac{P_{xy}(x,y)}{P_y(y)}\]

\subsection{Total expectation}
The tower property, which is awesome:

Let $X$ and $Y$ be random variables then:

\[ E[E[X|Y]] = E[X] \]

The proof is in the slides.
\subsection{Exam}
 For a given decision problem with a certain
discount factor.A pure policy sufices.
Do policy iteration, (a single iteration), do value iteration
(single iteration).

\end{document}
