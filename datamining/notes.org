so we're training a,b,c,d...

* Linear regression
Only thing linear is input variables (a,b,c,d), can do non-linear on x


* Classes
try to make splits which give as much information as possible.
So given some record, calculate the error after getting the record your prediction
has. The best choice is found by "brute force".


** Resubstitution error


$\Delta i(s,t) = \frac{2}{7} - (\frac{3}{7} \dot \frac{1}{3} + \frac{4}{7} \dot 0 = \frac{1}{7}$

*** Resubstitution
is concave but not stritctly concave

s_1 = (300,100), (100,300)
s_2 = (200,400), (200,0)

resubstition says $s_1 = s_2$, but $s_2$ is better because you already have
something pure.

*** GNI
$i(l) = 

$\phi(p)=p(1-p)=p-p^2$
$\phi^{'}(p)=1-2p \phi^{''}(p)=-2$

p(0|l) = probability of class 0 in left child, p(0|r) = right child.

** Pruning
2^{l-1}-1 because don't count splits that result in themselves (ie split nothing).

$ \lfloor f\rfloor = $ the largest integer that is smaller than $f$. ie cast to int ??

r(t) = error at node t, for example (30|10) has an error of 1/4 because you're
gonna geuss the highest probability, so 30, so from the total 40 you're gonna
be wrong 1/4 of the times.

\alpha , smaller is better. Choose the minimum value and prune every node that has
that. Then *recalculate* the \alpha and you can prune again. So you get a finite
sequence of pruned trees.

** The saturated model
The high number is unrealistic because you don't have enough observations.

*** Independence model
X and Y are independent iff P(X,Y) = P(X)P(Y) for all values of X and Y.

This assumption is made.. to try and make the dimensionality problem less
problematic

$P(Y|X)=\frac{P(X,Y)}{P(X)}$

*** Conditional independence
$X\rlap Y = $ independent.

in the example, if I already know someone smokes, a nicotine stain won't
give you more information.

In the ice cream example, you first say if more poeple go to the beach
so there are more icecream sales, but if you start considering the weather
the association disappears since you say both variables are caused by the
weather. You know the weather so you can explain icecream sales and beach
visits.

*** Graphs
Local markov property, if I know the boundary of X_2 and X_3 I can predict X_1
without considering the rest of the graph (see the example graph slides)

*** Example
$\overset{^}{n}(clinic1,more,no)= \frac{297 \times 7}{476} = 4.37$

cpr = 1 then x and y are independent

If you condition on clinic than the relation between care and mortality
disaears, but if you collapse on that then there is an association. So
its very dangerous to remove variables.
* Excersizes...

** one
- 9
- 
  - c|bbaabeede
  - cbb|aabeede
  - cbbaa|beede
  - cbbaab|eede
  - cbbaabee|de
  - cbbaabeed|e
- 9
- 37.5,
  42.5,
  48.5
- 0.25 + (0.25 - 2/7*5/7) = 0.29591836734
  (0.25-(5/6*1/6)) + (0.25-(5/6*1/6)) = 0.22222222222
  (0.25 - 2/7*5/7) + 0.25 = 0.29591836734
  so its the 42.5 split



** two

$i(t)=1-\sum^C_{j=1} p(j|t)^2$

- 10,13,16
- its 16, with an impurity reduction of 0.56-0.375=0.185
  - total impurity: 1-((6/10)^2+(2/10)^2+(2/10)^2) = 0.56
  - 10: 1-((4/8)^2+(2/8)^2+(2/8)^2) = 0.625
  - 13: (1-((1/5)^2+(4/5)^2) =         0.32
        (1-((2/5)^2+(2/5)^2+(1/5)^2)) = 0.64
                                    --- +
                                    0.96
  - 16: 1-((2/8)^2+(6/8)^2) = 0.375
- its 13, but you're making things worse..


