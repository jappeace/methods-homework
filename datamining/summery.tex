\documentclass[8pt,landscape]{extarticle}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}
\usepackage{graphicx}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\vDash$}}
\usepackage{amssymb}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\pdfinfo{
  /Title (Datamining cheatsheet)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Jappie Klooster)
  /Subject (Evulotionary computing)
  /Keywords (Evolutionary, Computing, Gentic, Algorithm, Heuristics, Strategy)}

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\geometry{top=0.2cm,left=0.2cm,right=0.2cm,bottom=0.2cm}

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-0.8ex plus -.5ex minus -.2ex}%
                                {0.2ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-0ex plus -.5ex minus -.2ex}%
                                {0.1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\underline{Datamining}} \\
\end{center}
\section{Classification trees}
\subsection{Building classification trees}
The objective of tree construction is to finally obtain nodes that are pure,
in the sense that they contain cases of a single class only.
\subsubsection{Impurity and quality of a split}
Impurity is a function:
$i(t)=\phi(p_1,p_2,...,p_J)$
where $p_j(j=1,...,J)$ is the relative frequency of the $J$ different classes.
$\phi$ is the impurity function with the following constraints:
An impurity measure of a node should be at maximum when the observations
are distributed evenly over all classes in that node,
it should be at a minimum when all observations belong to a single class,
$\phi$ is a symmetric function of $p_1,...,p_J$.
A symmetric function of n variables is one whose value at any n-tuple of
arguments is the same as its value at any permutation of that n-tuple.
The quality of a split is defined as follows:
\[\Delta i(s,t)=i(t)-\phi(l)i(l)-\phi(r)i(r)\]
Where $\phi$ is a function that determines the proportion of cases for a node
and $l$ and $r$ are the left and right nodes of $t$.
\paragraph{Resubstitution error}
It measures what fraction of the class in a node is classified incorrectly if
we assign every case to the majority class in that node.
\paragraph{Gini} $i(t)=\sum_j p(j|t)(1-p(j|t))$
\paragraph{entropy}$i(t)=\sum_j p(j|t)\log p(j|t)$

\subsubsection{Overfiting and pruning}
There are two strategies to avoid overfiting in tree construction:
Stopping rules, don't expand a node if impurity reduction below some threshold
and pruning, grow a large tree than later merge back nodes.
Stopping rules have a disadvantage that they sometimes require you to make
a weak split to get a good split.
\paragraph{Cost complexity pruning}
A problem with pruning is that the number of pruned subtree is $O(e^n)$.
So cost complexity pruning only considers trees that are
``the best of their kind''.
$T$ denotes the tree with leaf nodes $\overset{\sim}{T}$  and leaf node count
$|\overset{\sim}{T}|$
$R(T)$ denotes the fraction of cases in the training sample that are
misclassified by $T$, the re-substitution error of $T$.
We define the total cost: $C_\alpha(T)=R(T)+\alpha |\overset{\sim}{T}|$
The total cost of tree $T$ consists of the $R(T)$ and a penalty for complexity
$\alpha|\overset{\sim}{T}|$ where $\alpha$ is the parameter that determines the
complexity penalty.
a complex tree that has no errors may have a higher cost than a mall tree with
a number of errors depending on the value of $\alpha (\geq 0)$.
$T_{max}$ denotes the large tree that should be pruned to the right size.
If we fix the value of $\alpha$, therer is a smallest minimizing subtree $T(\alpha)$
of $T_{max}$ that fulfills:
$C_\alpha(T(\alpha))=min T \leq T_{max} C_\alpha(T)$ which says that there is no
subtree of $T_[max]$ with lower cost than $T(\alpha)$, and
If $C_\alpha (T) = C_\alpha(T(\alpha))$ then $T(\alpha) \leq T$ which says that
fi there is a tie then we pick the smallest tree (the one that is a subtree of
all other that achieve the minimum).
It can be shown that for every value of $\alpha$ there is  such a smallest
minimizing subtree. But this is non trivial.
Even though $\alpha$ goes trough a continum of values there is only a finite
numer of sutrees of $T_{max}$. We can construct a descreasing sequence of
subtrees of $T_{max}$: $T_1 > T_2 > ... > {t_1} $ where $t_1$ is the root node,
such tat $T_k$ is the smallest minimizing subtree for
$\alpha \in (\alpha_k, \alpha_{k+1})$. $T_1$ is the smallest subtree of $T_{max}$
with the same resubstitution error as $T_{max}$ ($T_1=T(\alpha=0)$).
the algorithm to find $T_1$ is: find any pair of leaf nodes with a common parent
that can be merged back withoutincreasing te resubtition error. Continue until
no more such pair can be found.
\[g(t)=\frac{R(t)-R(T_{1,t})}{|\overset{\sim}{T}_{1,t}|-1}\]
is a function to calculate the next value for $\alpha$ at which $T_k - T_t$
becomes better than $T_k$.
The algorithm for cost complexity is as follows: We start by computing $T_1$ from $T_{max}$
by using $\alpha = 0$. Then we repeat until we reach the root node:
For each node $t$ in he current tree $T_k$ we compute $g_k(t)$. Then we prune
$T_k$ in all nodes for which $g_k$ achieves the minimum to obtain $T_{k+1}$.
\subsubsection{Selection of the final tree}
$R^{ts}(T)$ denotes the estimated error rate of tree $T$. You can use
the entire set and just pick the one with the lowest estimated error.
There is also cross validation, which can be used if the data set is smaller.

\subsection{Missing data}
The easiest way is  to throw away records with missing data.
This has the following disadvantages: Potential bias and loss of power (which
means that our precision is reduced because we loose a lot of examples).
If we want to use a tree with incomplete data we have the following problems:
How to determine the quality of a spiet? Which way gets an observation send
with a missing value?
If the class labe misses its useless anyway so it should be thrown away.
A solution is ignoring the quality aspect and add surogate splits.
\subsection{Computational efficiency}
A categorical attribute with $L$ is distinct values has $2^{L-1}-1$ possible
distinct splits.
For binary class problems and impurity measures that belong to class $\mathcal{F}$
there is a more efficient algorithm for finding the optimal split.
$p(0|x=b_l)$  denotes the realitve frequency of class 0 for observations in the
current node with $x=b_l$. Order $p(0|x=b_l)$. Then one of the $L$ subsets
is the optimal split. Thus the search is reduced to $O(L-1)$
\subsubsection{Tree construction}
   Asuming all $p$ attributes $x_1, ..., x_p$ are numeric we have to sort
   them per attribute, which takes $O(n \log n)$ time.
   Best case (where each split is a perfect subdivision) we traverse the tree
   and have to consider $O(n)$ splits per level resulting in $O(n \log n)$ time.
   Worst case it becomes $O(n^2)$
\section{Graphical models}
\subsection{Probability rules}
sum rule $P(X)=\sum_y P(X,Y)$, product rule $P(X,Y)=P(X|Y)P(Y)$.
To estimate the joint distrubtion of $X$ and $Y$ we use $\overset{\wedge}{P}(x,y)=\frac{n(x,y)}{n}$
\subsection{Independence}
Random vectors $X$ and $Y$ are independent iff $P(x,y)=P(x)P(y) \text{ for all } (x,y)$
Therefore $P(x|y)=P(x)$. Independence can be written as $X \indep Y$.
This gives the factorization criterion for independent random vectors:
random vectors $X$ and $Y$ are independent iff there exists two functions $g$
and $h$ such that: $P(x,y) = g(x)h(y)$ for all $(x,y)$.
A ``log version'' is: $\log P(x,y)=g'(x)+h'(y)$ for all $(x,y)$.
Random vectors $X$ and $Y$ re independent given $Z$ iff
$P(x,y|z)=P(x|z)P(y|z)$ for all $(x,y)$ and for all $z$ for which $P(z)>0$.
This is the same as:
$P(x,y,z)=P(x,z)P(y,z)/P(z)$.
With marginal independence we can state a simple factorization criterion
to establish conditional independence: random vectors $X$ and $Y$ are
conditionally independent given $Z,X \indep Y | Z$ iff there exists
functions $g$ and $h$ such that $P(x,y,z)=g(x,z)h(y,z)$.
\subsection{Independence graphs}
Conditional independence relations can be represented as a conditional
independence graph. Let $X=(X_1,X_2,\dots, X_k)$ be a $k$-dimensional random
vector. The conditional independence graph of $X$ is the undirected graph
$G=(K,E)$ with $K={1,2,...,k}$ and where ${i,j}$ is not in the edge set $E$ 
iff $X_i \indep X_j|rest$.
Global markov property: $a$ seperates $b$ from $c$ ($a,b,c$ disjoint) iff
$X_b \indep X_c | X_a$ where $X_a=(X_i;i\in a)$ and $a$ seperates $b$ from
$c$ if for all $i \in b, j\in c:$ $a$ seperates $i$ from $j$.
Is equal to local markov property: $X_i \indep$ rest | boundary$(i)$ where
the boundary of a vertex $i$ is simply the set of adjecent vertices.
\subsection{Log linear models}
\subsubsection{for binary data}
A random experiment that only disguises between two possible outcomes is
called a \emph{Bernoulli} experiment. Random variable $X$ denotes the number of
successes in a Bernoulli experiment; $X$ therefore has possible values 0 and 1.
The probability distribution of $X$ is completely determined by the probablity of
success which we denote by $p$  and is $P(X=0) = 1 - p $ and $ P(X=1)=p$.
$X$ has the probability function $P(x)=p^x(1-p)^{1-x}$ for $x=0,1$ and
$0 \leq p \leq 1$.
log properities: $\log a^b=b \log a$, $\log ab = \log a + \log b$,
$\log \frac{a}{b}=\log a - \log b$.
The cartesion product 
${0,1} \times {0,1}$ creates a table of the bivariate Bernoulli random
vector $(X_1,X_2)$.
\begin{tabular}{llll}
  $P(x_1,x_2)$ & $x_2=0$ & $x_1=1$ & Total \\
  $x_1=0$ & $p(0,0)$ & $p(0,1)$ & $p_1(0)$ \\
  $x_1=1$ & $p(1,0)$ & $p(1,1)$ & $p_1(1)$ \\
  Total & $p_2(0)$ & $p_2(1)$ & $1$ \\
\end{tabular}
which can be written as
$P(x_1,x_2)=p(0,0)^{(1-x_1)(1-x_2)}p(0,1)^{(1-x_1)x_2}p(1,0)^{x_1(1-x_2)}p(1,1)^{x_1x_2}$
for $x_1=0,1$ and $x_2=0,1$.
Taking logarithms of this identity for $P$ and collecting terms in$x_1$ and
$x_2$ gives:
$\log P(x_1,x_2)= \begin{matrix}
    \log p(0,0) + x_1 \log \frac{p(1,0)}{p(0,0)} +  \\
    x_2 \log \frac{p(0,1)}{p(0,0)} + x_1 x_2 \log \frac{p(1,1)p(0,0)}{p(0,1)p(1,0)}
    \end{matrix}
$
reparameterizing the right hand side leads tothe so-caled \emph{log-linear expension}
$\log P(x_1,x_2)=u_\emptyset+ x_1u_1 + x_2 u_2 + x_1 x_2 u_{12}$
the $u$ coefficients are called $u$-terms, they substitue the log part.
The coefficient of the product $x_1x_2$ is the logarithm of the cross product
ratio:
$u_{12}=\log cpr(X_1,X_2)$
The random variables $X_1$ and $X_2$ are independent iff $u_{12}=0$
If $u_{12}=0$ we can take $g(x_1)=u_\emptyset + x_1 u_1$ and $h(x_2)= u_2 x_2$.
This is not posible with $u_{12}\neq 0$.
We can enforce (conditional) independence constraints by etting the right
$u$-terms to zero.
\subsection{non binary data}
To work with non binary data we make the u terms functions rather than constants:
$log P(x_1,x_2)=u_\emptyset + u_1(x_1) + u_2(x_2) + u_{12}(x_1,x_2)$
This produces to many parameters, and to identify them we have to impose
an extra constraint: $u_a(x_a)=0$ whenever $x_i=0$ and $i\in a$. Here we
assume that if $x_i$ has $d_i$ possible values, these are numbered
$0,1,...,d_i-1$, but not ordered.
\subsection{Hierarchical and graphical}
% TODO: perhaps delete this section?
The log-linear expension of the crossclassified multinominal density function
$P_K$ is $\log P_K(x)=\underset{a \subseteq K}{\sum} u_a(x_a)$ where the sum is
taken over all possible subsets of $K={1,2,...,k}$ and where the $u$-terms are
coorinate projection functions, so that $u_a(x)=u_a(x_a)$ and also satisfy the
constraint that $u_a(x)=0$ whenever $x_i = 0$ and $i\in a$.
\subsubsection{Independence and the u-terms}
$If(X_a,X_b,X_c)$ is a partitioned multinominal random vector then
$X_b \indep X_c|X_a$ if and only if all u-terms in the log-linear expension with
one or more coordinate in $b$ and one or more coordinate in $c$ are zero.
The proof is direct application of the factorisation theorom for conditional
independence.
\subsubsection{Graphical model}
Given an independence graph $G=(K,E)$ the cross-classified multinomal
distrubtion for the random vector $X$ is a graphical model for $X$ if the
distribution of $X$ is an arbitrary apart from constraints of the form that for
all pairs of coordinates not in the edge set $E$ of $G$ the $u$ terms containing
the selected coordinates are identically zero.
\subsection{Maximum likelhood estimation}
The maximum likelihood estimator of gaphical log linear model $M$ satisfies the
likelihood equations: $\overset{\wedge}{n}_a=N\overset{\wedge}{P}_a=n_a$
\subsubsection{Iterative proportional fitting}
Is a converging algorithm
Start with a uniform table. Then set the row margins to the fitted counts
and distribute them proportionally. Then do the same for the columns, keep
doing this until the values converged.

\subsection{Decomosable graphical models}
Decomposable models are graphical models that have explicit formulas for the
maximum likelihood estimates.
This is a convenient property from a computational viewpoint.
If we only have to fit one model this is perhaps not so important,
but when we have little prior knowledge we typically have to search a
potentially large space of possible models.
Decomposable models are very easy to characterize by their independence graphs.
They have triangulated independence graphs: their independence graphs have no
chordless cycles of length greater than three. A cycle is called chordless
if no other than successive pairs of vertices in the cycle are adjacent. 
\subsection{Deviance and likelhood ratio test}
The deviance of a fitted model compares the log-likelihood of the fitted model
to the log-likelihood of the saturated model.
The larger the model deviance, the poorer the fit.
The likelihood of a model $M$ is
$L(\overset{\wedge}{P}(x);n(x))=\underset{x}{\prod}\overset{\wedge}{P}(x)^{n(x)}$
and the log likelihood of a model $M$ is
$\underset{x}{\sum} n(x)\log \overset{\wedge}{P}(x)$
$L^i=L(\overset{\wedge}{P}^{M_i})$ is the value of the log likelihood function
valutated at $\overset{\wedge}{P}^{M_i}$; the ML estimates of $P$ under $M_i$.
Let $M_0 \subseteq M_1$; ie $M_0$ can be obtained from $M_1$ by setting
additional $u$-terms to zero (more restrictions). The deviance between $M_0$ and
$M_1$ is $dev(M_0) - dev(M_1) = 2(L^1 - L^0)$.
For large $N$ $2(L^1-L^0) \approx M_0 \chi^2_v$ where the degrees of freedom $v$
is equal to the number of additional restrictions of $M_0$. 
We reject the null hypothesis that $M_0$ is the true model when:
$2(L^1-L^0)>\chi^2_{v;\alpha}$. 
\section{Baysian networks}
\subsection{Definition and notation}
A directed graph is a pair $G = (K, E)$, where $K$ is a set of vertices and $E$
is a set of edges with ordered pairs of vertices.
If there is an arrow from $i$ to $j$,
then we write this as $i \to j$, or equivalently as $(ij) \in E$.
We restrict attention to directed graphs with no directed cycles,
i.e. acyclic directed graphs (DAGs). If $i \to j$,
then $X_i$ is called a parent of $X_j$ ,
and $X_j$ is called a child of $X_i$ .
The set of coordinates of the parents of $X_j$ is denoted $pa(j)$, so
$X_{pa(j)}$ denotes the set of parents of $X_j$ .
If there is a directed path from $i$ to $j$,
then $X_i$ is called an ancestor of $X_j$ .
The set of coordinates of the ancestors of $X_j$ is denoted an $(j)$.
The absence of any directed cycles is equivalent to the existence of an
ordering of the nodes ${1, 2, \dots , k}$ such that $i \to j$ only when $i < j$.
In other words, there exists an ordering of the nodes such that arrows point
only from lower-numbered nodes to higher-numbered nodes.
Suppose that a priori knowledge tells us the variables can be labeled
$X_1 , X_2 , \dots , X_k$ such that $X_i$ is prior to $X_{i+1}$.
Corresponding to this ordering we can factorize the joint density of
$X_1 , X_2 , \dots , X_k$ as $P(X)=P(X_1)P(X_2|X_1)\dots P(X_k|X_{k-1},X_{k-2},\dots,X_1)$
In contructing a DAG, an arrow is drawn from $i$ to $j$, where $i<j$, unless
$P(X_j|X_{j-1}\dots,X_1)$ does not depend on $X_i$.
This is the key difference between DAGs and undirected graphs.
In both types of graph a missing edge between $X_i$ and $X_j$ is equivalent to
a conditional independence relation between $X_i$ and $X_j$.
In undirected graphs they are conditionally independent given all the remaining
variables, whereas in DAGs they are conditionally independent given all prior
variables. 
We can write the joint density:
$P(X)=\underset{i=1}{\overset{k}{\prod}}P(X_i|X_{pa(i)})$.
and the conditional indepencence as: $i \indep j|an({i,j})$
\subsection{Interpretation}
Given a DAG we construct the moral graph $G^m$ by marrying parents and deleting
directions, that is:
For each $i \in K$ we connect all vertices in $pa(i)$ with lines, we replace all
arrows in $E$ with lines.
\subsection{Maximum likelyhood estimation}
\subsubsection{ML estimation for Multinomal distribution}
We want to estimate the probabilities $p1 , p2 , \dots , p_J$
of getting outcomes $1, 2, \dots , J$ If in $n$ trials,
we observe n1 outcomes $1, n_2$ of $2, \dots, n_J$ of $J$,
then the obvious guess is to estimate $p_j , j = 1, \dots , J$, by $n_j /n$.
This is also the maximum likelihood estimate.
\subsection{ML estimation for bayesian nets}
The joint probability for $n$ independent observation in log likelyhood becomes
$L=\underset{i=1}{\overset{k}{\sum}}\underset{x_i,x_{pa(i)}}{\sum}n(x_i,x_{pa(i)})
\log p(x_i|x_{pa(i)})$
where $p$ stands for network parameters, $n(x_i,x_{pa(i)})$ for the number of
obsevations with $X_i=x_i$ and $X_{pa(i)}=x_{pa(i)}$.
If the parameters are not related this is just a bunch of independent
multinomalestimation problems. So we can say
$\overset{\wedge}{p}(x_i|x_{pa(i)})=\frac{n(x_i,x_{pa(i)})}{n(x_{pa(i))}}$,
which can be substituted in the $L$ formula.
\subsection{Estimation from incomplete data}
We paritition the complete data in an observed part $X_{obs}$ and a missing part
$X_{miss}$. For observation $j$ we write $X^{(j)}=(X_{obs}^{(j)},X_{mis}^{(j)})$.
We want to find those parameter values that maximize the probability of the
observed data.
This means that if some values are missing,
we have to obtain the marginal probability of the observed data by summing out
the missing data. For observation $j$,
the probability thus is:
$P(X^{j}_{obs})=\underset{X^{(j)}_{mis}}{\sum}P(X^{(j)})$
For all observations it becomes a product.
Direct maximization of the observed data likelihood is complicated because we get
a sum of parameters inside a log.
There is an algorithm iterative scheme to compute the ML estimate called
expectation maximization, it alternates between expectation step and maximization
steps. 
\subsection{Model selection with complete data}
To prevent overfitting we introduce a penalty. $L^M$ denotes the value of the
log-likelyhood function evaluated at $\overset{\wedge}{p}^M$; the ML estimates
of $p$ under model $M$. Akaikes information criterion would ive the follwoing
quality form model $M$: $AIC(M)=-2L^M+2dim(M)$ where $dim(M)$ is thenumber of
parameters of the model. which we divide by $-2$ to get higher value for higher
quality: $AIC(M)=L^M-dim(M)$. AIC gives a relativly low penalty for complex
models therefore Baysian infromation criterion is more popular:
$BIC(M)=L^M-\frac{\log n}{2} dim(M)$
So now we have a well-defined optimzation problem: Given training data, scoring
function (BIC), space of possible models (all DAG's) find a network that
maximaizes the score.
This is however NP hard therefore heuristics such as hillclimbing are used.
The score is decomposable, its a sum of terms where each term contains the
variables $i \cup pa(i)$. This means we only have to recalculate for variables
of which the parent set has changed.
You have the reverse opperation because hillclimbing only looks one step ahead,
therefore adding and deleting can't emulate it.
\section{Freq paternset minig}
\subsection{Association rules: The binary case}
The traditional setting for association rules consists of a set of transactions
in which each transaction is a set of items.
Translated to a relational setting,
we have a table db with schema $R = {I_1 , \dots, I_n }$,
in which each $I_i$ is a binary attribute.
The attributes correspond with the items,
the rows in the table with the transactions
; a row has value 1 for an attribute if and only if the transaction contains
that item.
For $X,Y \subseteq R$, with $X\cap Y=\emptyset$ let $s(X)$ denote the support of
$X$, the number of tuples that have value 1 for all items in $X$. for an
association rule $X \to Y$ define the support is $s(X\cup Y)$, the confidence is
$s(X\cup Y)/s(X)$
The problem is to find all association rules that match or exceed the user
defined lower thresholds for confidence, \emph{minconf}, and support
\emph{minsup}.
In a major abuse of notation,
you could say that the support of X (when expressed as a fraction) corresonds
to $P (X)$ in probability terms,
and the confidence of $X \to Y$ with the conditional probability $P (Y | X)$.
There are two tesholds we have to satisfy:
find all sets $Z$ whoese support exceeds the minimal threshold. These sets are
called frequent. Then test for all non-empty subsets $X$ of frequent sets $Z$
wheter the rul $X \to Z \setminus X(=Y)$ holds with sufficient confidence.
\subsubsection{A priori alogirhtm}
Finding all frequents sets is problematic since all sets of $R$ are
$|\mathcal{P}(R)|=2^{|R|}$. But since $Z$ can only be frequent if all its
(non-empty) subsets are frequent we can do it.
This is known as the A-Priori property. In other words, we can search level wise
for the frequent sets.
The level is the number of items in the set. Denote by $C(i)$ the sets of $i$
items that are potentially frequent (the candidate sets) and by $F (i)$
the frequent sets of $i$ items.
Start with level one all entries of $R$ (in random order),
filter on the level $C(i)$ each entry
that doesn't have enough support, these are the frequent items $F(i)$.
Then $i$ is increased, and we start creating the
next level of candidates $C(i)$ based on $F(i-1)$ (which we just made).
Then they do this:
For each $X \in F (i − 1)$ do
For each $Y \in F (i − 1)$ that shares the first $i − 2$ items with X do
$C(i) := C(i) \cup {X \cup Y }$
If we assume the db is sparse the complexity becomes $O(|R|^{k+1})$ where $k$
is the maximal size of frequent items sets that is $k << |R|$.
To generate association rules:
\emph{flatmap} each freq set $X$ \emph{with} each non empty $Y \subset X$ filtering
$s(X)/s(X\setminus Y)\geq minconf$ as $X \setminus Y \to Y $
\subsection{Drowining in assoc rules}
\subsubsection{Postprocessing the result set}
We can order the resultss we can use conidence and support: they define a
partial order on the set of rules. Given rules $r_1$ and $r_2, r_1A$ iff 
$s(r_1) \leq s(r_2) \wedge conf(r_1) < con (r_2)$ or
$s(r_1) < s(r_2) \wedge conf(r_1) \leq conf(r_2)$
A rule is interesting if it gives usefull infromation. Lots of measures have
been defined as an attempt to do that: lift, interest, conviction, colective
strength, gain, gini, entropy, $\chi^2$ This is lift:
$lift(A\to C)=\frac{P(C|A)}{P(C)}=\frac{P(A,C)}{P(A)P(C)}$
The lift should be bigger than one for a rle to be itneresting.
\subsubsection{Generating less rules}
Closed frequent itemsets are itemsets that completely characterise their
associated set of transactions.
That is, a frequent itemset $I$ is closed if $I$ contains all items that occur in
all transactions in the support of $I$.
For an itemset $I$, denote by $\sigma(I)$ the set of tuples in which all items
in $I$ are bought, that is $\sigma(I)=\{t \in db | \forall i \in I, i \in t\}$
For a set of transaction $T$ let $f(T)$ denote the set of items that are bought
in all transaction in $T$, that is $f(T)=\{i \in R | \forall t \in T, i \in t\}$
The closure can be obtained by $c(I)=f(\sigma(I))$.
To give a procedural description of applying the closure operator to an itemset
$I$:
first get the transactions in which all items in $I$ are bought,
and then see whether there are any more items that are common to all these
transactions.
If so, add them to $I$ and return the result.
Clearly, $I \subseteq c(I)$ and $I$ has the same support as $c(I)$.
An itemset $I$ is closed if and only if $c(I) = I$.
An itemset $I$ is a closed frequent itemset iff it is both frequent and closed.
The A-Close algorithm for finding al frequent closed itemsets consists of 2
phases: One, discover all freq closed itemsets in db. Derive all frequent
itemsets from the freqeunt closed itemsets found in phase 1.
In phase 1 the algorithm determines a set of so-called generators that will
produce all frequent closed itemsets by application of the closure operator $c$.
An itemset $I$ is a generator of a closed itemset $J$ if it is one of the
smallest itemsets with $c(I) = J$.
A-Close performs a levelwise search like Apriori:
$G_{i+1}$ (the set of generators at level $i + 1$) is constructed using $G_i$.
Using their support, and the support of their $i$-subsets in $G_i$,
infrequent itemsets and itemsets that have the same support as one of their
subsets are deleted from $G_{i+1}$. The rational of the secon pruning rule is
that if an itemset has a subset with the same suppport, then this subset also
has the same closure.
Now that the generators have been determined, we compute their closures,
where $c(I)=\cap t \in db: I \subseteq t$. Different generators may produce the
same closure, duplicates are removed.
In phase 2 we determine all frequent itemsets and their support.
We use the following properties:
1. All maximal frequent itemsets are closed.
2. Every frequent itemset is a subset of a maximal frequent itemset.
3. The support of an itemset equals the support of the smallest closed itemset
in which it is contained (its closure).
Our strategy is therefore to select the maximal itemsets,
generate all their sub- sets,
and determine their support using the third property.
\section{Frequent pattern mining}
$\Sigma$ is the set of labels, $s=s_1,_2,\dots,s_n$ where $s_i \in \Sigma$,
prefix $s[1:i]=s_1,s_2,\dots,s_1, 0 \leq i \leq n$. suffix $s[i:n]$.
Let $s=s_1,s_2\dots s_n$ and $r=r_1,r_2,\dots r_m$ be two sequences over
$\Sigma$. We say $r$ is a subsequence of $s$ denoted $r \subseteq s$, if there
exists a one-to-one mapping $\phi:[1,m]\to[1,n]$ such that $r[i]=s[\phi(i)]$ and
$i< j \Rightarrow \phi(i)<\phi(j)$.
Each position in $r$ is mapped to a position in $s$ with the same label and the
orer of labels is preserved. There may however be interverning gapsbetween
consecutive elements of $r$ in the mapping.
Given a database $D={s_1,\dots,s_N}$ of $N$ sequences and given some sequenc $r$
the suport is defined as the total number of seuences in $D$ that contain $r$:
$sup(r)=|\{s_i\in D:r\subseteq s_i\}|$. Given a minimum support threshold minsup,
compute $\mathcal{F}(minsup,D)=\{r|sup(r)\leq minsup\}$.
For two sequences $r_1$ and $r_2$ we have $r_1 \subseteq r_2 \Rightarrow
sup(r_1) \leq sup(r_2)$ becuase $\forall s \in D: r_2 \subseteq s \Rightarrow
r_1 \subseteq s$
Hence in a level wise search for frequent sequences there is no point in
expending infrequent ones.
\subsection{GSP algorthm}
Perform level wise search. Don't extend infrequent sequences.
Candiadate generation for level $k+1$: take two frequent sequences $r_a$ and
$r_b$ of length $k$ with $r_a[q:k-1]=r_b[1:k-1]$ and generate pre-candadate
$r_{ab}=r_a+r_b[k]$. Pre-candidate $r_{ab}$ becomes a candidate (has to be
counted) if all its subsequences of length $k$ are frequent. Note that we allow
$r_a = r_b$.
\subsection{Node labeled Graph}
A node labled graph is a quadruple $G=(V,E,\Sigma,L)$ where: $V$ is the set of
nodes, $E$ is the set of edges, $\Sigma$ is the set of labels, and $\L: V->
\Sigma$ is a labeling function.
A labeled rooted unordered tree $U = (V , E , \Sigma, L, v^r )$ is an $G$ with a
special node $v^r$ called the root of the tree such that there exists exactly
one path betweeen the root node and any other node in $V$.
A labeled rooted rodered tree $T = (V, E, L, v^r, \leq)$ is an unordered tree
$U$ where between all the iblings an orderer $\leq$ is dfined. to every node in
an ordered tree a preorder $(pre(v))$ number is assigned according to the depth
first traversel of the tree.
\subsubsection{Tree inclusion relations}
An induced subtree preserves parent child relationships.
$\pi(v)$ denotes the parent of node $v$.
Given two oreered trees $D$ and $T$ we call $T$ an induced subtree of $D$ if
there exists an injective matching function $\phi$ of $V_T$ into $V_d$ with the
following conditions: $\phi$ preservers the labels: $L_T(v)=L_D(\phi(v))$,
$\phi$ preservers the parent child relation:
$v_i = \pi_T(v_j)\Leftrightarrow \phi(v_i)=\pi_D(\phi(v_j))$
$\phi$ preservers the left to right orderbetween the nodes: $pre(v_i) <
pre(v_j ) \Leftrightarrow pre(\phi(v_i)) < pre(phi(v_j))$  
An induced subtreee $T$ can be obtained from a tree $D$ by rpeatedly removing
leaf nodes, or possibly the root node if it has only one child.
This definition is the same for embedded subtree, except the equal sign become
$\in$ in the relation constiant and $\pi$ talks about the ancestor set.
An embedded subtree preserves ancestor decendent realtionships. 
\subsubsection{Freq tree mining task}
Given a database of trees $D=\{d_1, d_2, \dots, d_n\}$ and a tree inclusion
relation $\preceq$ we define the suppot of a tree $T$ as
$sup(T,D)=|\{d\in D | T \preceq d\}|$. Given a minimum support treshehold
minsup, compute $\mathcal{F}(minsup,D)=\{T|sup(T,D)\leq minsup\}$
\emph{Anti monoticity property}: For a database of trees $D$ and two trees $T_1$
and $T_2$ we have $T_1 \preceq T_2 \Rightarrow sup(T_1, D) \leq sup(T_2, D)$,
because $\forall d \in D: T_2 \preceq d \Rightarrow T_1 \preceq d$
Hence in a level wise search fro frequenttreees there is no point in expanding
infrequent trees.
\subsubsection{Right most extension}
Let $T_k$ denote a tree of size $k$.
Consider the node numbering of $T_k$ according to pre-order (depth-first)
traversal of the tree.
The right-most branch of the tree is the path from the root node to the
right-most leaf (i.e. the node with number $k$).
To expand the tree $T_k$,
it is only allowed to add a node as the right-most child of a node on
the right-most branch of $T_k$.
This node gets number $k + 1$,
as it is the last node in the pre-order traversal of $T_{k+1}$.
The right-most extension technique generates each tree at most once.
Also, the right-most expansion technique generates every possible tree, so each
tree is generated exactly once.
\emph{occurene list}:
To determine whether a pattern tree occurs in a data tree, an occurrence list
is maintained that contains the list of nodes in the data tree to which the
nodes in the pattern tree can be mapped.
FREQT only stores the nodes of the data tree to which the right-most node in
the pattern tree can be mapped.
This is sufficient since only the nodes on the right-most branch are needed
for future extension.
\subsection{Frequent pattern and clasification}
Frequent pattern mining can also be used to extract features for classification
tasks: 1 Find frequent patterns per class.
2 Define discriminating patterns, for example,
as patterns that are frequent in one class but not in the other.
3 Use the presence/absence of such a discriminating pattern as a (binary)
feature for constructing a classifier (e.g. classification tree!).
4 In this way we can include non-tabular data (sequences, trees, graphs)
into an algorithm that requires a tabular data structure.
\end{multicols}
\end{document}
