\documentclass[12pt,landscape]{extarticle}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}
\usepackage{graphicx}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\vDash$}}

\pdfinfo{
  /Title (Datamining cheatsheet)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Jappie Klooster)
  /Subject (Evulotionary computing)
  /Keywords (Evolutionary, Computing, Gentic, Algorithm, Heuristics, Strategy)}

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
    {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
        {\geometry{top=0.5cm,left=0.5cm,right=0.5cm,bottom=0.5cm} }
        {\geometry{top=0.5cm,left=0.5cm,right=0.5cm,bottom=0.5cm} }
    }

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-0.8ex plus -.5ex minus -.2ex}%
                                {0.2ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-0ex plus -.5ex minus -.2ex}%
                                {0.1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\underline{Datamining}} \\
\end{center}
\section{Classification trees}
\subsection{Building classification trees}
The objective of tree construction is to finally obtain nodes that are pure,
in the sense that they contain cases of a single class only.
\subsubsection{Impurity and quality of a split}
Impurity is a function:
\[i(t)=\phi(p_1,p_2,...,p_J)\]
where $p_j(j=1,...,J)$ is the relative frequency of the $J$ different classes.
$\phi$ is the impurity function with the following constraints:
\begin{enumerate}
  \item An impurity measure of a node should be at maximum when the observations
    are distributed evenly over all classes in that node.
  \item It should be at a minimum when all observations belong to a single class.
  \item $\phi$ is a symmetric function of $p_1,...,p_J$.
    a symmetric function of n variables is one whose value at any n-tuple of
    arguments is the same as its value at any permutation of that n-tuple.
\end{enumerate}
The quality of a split is defined as follows:
\[\Delta i(s,t)=i(t)-\phi(l)i(l)-\phi(r)i(r)\]
Where $\phi$ is a function that determines the proportion of cases for a node
and $l$ and $r$ are the left and right nodes of $t$.
\paragraph{Resubstitution error}
It measures what fraction of the class in a node is classified incorrectly if
we assign every case to the majority class in that node.
\paragraph{Gini} $i(t)=\sum_j p(j|t)(1-p(j|t))$
\paragraph{entropy}$i(t)=\sum_j p(j|t)\log p(j|t)$

\subsubsection{Overfiting and pruning}
There are two strategies to avoid overfiting in tree construction:
Stopping rules, don't expand a node if impurity reduction below some threshold
and pruning, grow a large tree than later merge back nodes.
Stopping rules have a disadvantage that they sometimes require you to make
a weak split to get a good split.
\paragraph{Cost complexity pruning}
A problem with pruning is that the number of pruned subtree is $O(e^n)$.
So cost complexity pruning only considers trees that are
``the best of their kind''.
$T$ denotes the tree with leaf nodes $\overset{\sim}{T}$  and leaf node count
$|\overset{\sim}{T}|$
$R(T)$ denotes the fraction of cases in the training sample that are
misclassified by $T$, the re-substitution error of $T$.
We define the total cost: $C_\alpha(T)=R(T)+\alpha |\overset{\sim}{T}|$
The total cost of tree $T$ consists of the $R(T)$ and a penalty for complexity
$\alpha|\overset{\sim}{T}|$ where $\alpha$ is the parameter that determines the
complexity penalty.
a complex tree that has no errors may have a higher cost than a mall tree with
a number of errors depending on the value of $\alpha (\geq 0)$.
$T_{max}$ denotes the large tree that should be pruned to the right size.
If we fix the value of $\alpha$, therer is a smallest minimizing subtree $T(\alpha)$
of $T_{max}$ that fulfills:
$C_\alpha(T(\alpha))=min T \leq T_{max} C_\alpha(T)$ which says that there is no
subtree of $T_[max]$ with lower cost than $T(\alpha)$, and
If $C_\alpha (T) = C_\alpha(T(\alpha))$ then $T(\alpha) \leq T$ which says that
fi there is a tie then we pick the smallest tree (the one that is a subtree of
all other that achieve the minimum).
It can be shown that for every value of $\alpha$ there is  such a smallest
minimizing subtree. But this is non trivial.
Even though $\alpha$ goes trough a continum of values there is only a finite
numer of sutrees of $T_{max}$. We can construct a descreasing sequence of
subtrees of $T_{max}$: $T_1 > T_2 > ... > {t_1} $ where $t_1$ is the root node,
such tat $T_k$ is the smallest minimizing subtree for
$\alpha \in (\alpha_k, \alpha_{k+1})$. $T_1$ is the smallest subtree of $T_{max}$
with the same resubstitution error as $T_{max}$ ($T_1=T(\alpha=0)$).
the algorithm to find $T_1$ is: find any pair of leaf nodes with a common parent
that can be merged back withoutincreasing te resubtition error. Continue until
no more such pair can be found.
\[g(t)=\frac{R(t)-R(T_{1,t})}{|\overset{\sim}{T}_{1,t}|-1}\]
is a function to calculate the next value for $\alpha$ at which $T_k - T_t$
becomes better than $T_k$.
The algorithm for cost complexity is as follows: We start by computing $T_1$ from $T_{max}$
by using $\alpha = 0$. Then we repeat until we reach the root node:
For each node $t$ in he current tree $T_k$ we compute $g_k(t)$. Then we prune
$T_k$ in all nodes for which $g_k$ achieves the minimum to obtain $T_{k+1}$.
\subsubsection{Selection of the final tree}
$R^{ts}(T)$ denotes the estimated error rate of tree $T$. You can use
the entire set and just pick the one with the lowest estimated error.
There is also cross validation, which can be used if the data set is smaller.

\subsection{Missing data}
The easiest way is  to throw away records with missing data.
This has the following disadvantages: Potential bias and loss of power (which
means that our precision is reduced because we loose a lot of examples).
If we want to use a tree with incomplete data we have the following problems:
How to determine the quality of a spiet? Which way gets an observation send
with a missing value?
If the class labe misses its useless anyway so it should be thrown away.
A solution is ignoring the quality aspect and add surogate splits.
\subsection{Computational efficiency}
A categorical attribute with $L$ is distinct values has $2^{L-1}-1$ possible
distinct splits.
For binary class problems and impurity measures that belong to class $\mathcal{F}$
there is a more efficient algorithm for finding the optimal split.
$p(0|x=b_l)$  denotes the realitve frequency of class 0 for observations in the
current node with $x=b_l$. Order $p(0|x=b_l)$. Then one of the $L$ subsets
is the optimal split. Thus the search is reduced to $O(L-1)$
\subsubsection{Tree construction}
   Asuming all $p$ attributes $x_1, ..., x_p$ are numeric we have to sort
   them per attribute, which takes $O(n \log n)$ time.
   Best case (where each split is a perfect subdivision) we traverse the tree
   and have to consider $O(n)$ splits per level resulting in $O(n \log n)$ time.
   Worst case it becomes $O(n^2)$
\section{Graphical models}
\subsection{Probability rules}
sum rule $P(X)=\sum_y P(X,Y)$, product rule $P(X,Y)=P(X|Y)P(Y)$.
To estimate the joint distrubtion of $X$ and $Y$ we use $\overset{\wedge}{P}(x,y)=\frac{n(x,y)}{n}$
\subsection{Independence}
Random vectors $X$ and $Y$ are independent iff $P(x,y)=P(x)P(y) \text{ for all } (x,y)$
Therefore $P(x|y)=P(x)$. Independence can be written as $X \indep Y$.
This gives the factorization criterion for independent random vectors:
random vectors $X$ and $Y$ are independent iff there exists two functions $g$
and $h$ such that: $P(x,y) = g(x)h(y)$ for all $(x,y)$.
A ``log version'' is: $\log P(x,y)=g'(x)+h'(y)$ for all $(x,y)$.
Random vectors $X$ and $Y$ re independent given $Z$ iff
$P(x,y|z)=P(x|z)P(y|z)$ for all $(x,y)$ and for all $z$ for which $P(z)>0$.
This is the same as:
$P(x,y,z)=P(x,z)P(y,z)/P(z)$.
With marginal independence we can state a simple factorization criterion
to establish conditional independence: random vectors $X$ and $Y$ are
conditionally independent given $Z,X \indep Y | Z$ iff there exists
functions $g$ and $h$ such that $P(x,y,z)=g(x,z)h(y,z)$.
\subsection{Independence graphs}
Conditional independence relations can be represented as a conditional
independence graph. Let $X=(X_1,X_2,\dots, X_k)$ be a $k$-dimensional random
vector. The conditional independence graph of $X$ is the undirected graph
$G=(K,E)$ with $K={1,2,...,k}$ and where ${i,j}$ is not in the edge set $E$ 
iff $X_i \indep X_j|rest$.
Global markov property: $a$ seperates $b$ from $c$ ($a,b,c$ disjoint) iff
$X_b \indep X_c | X_a$ where $X_a=(X_i;i\in a)$ and $a$ seperates $b$ from
$c$ if for all $i \in b, j\in c:$ $a$ seperates $i$ from $j$.
Is equal to local markov property: $X_i \indep$ rest | boundary$(i)$ where
the boundary of a vertex $i$ is simply the set of adjecent vertices.
\subsection{Log linear models}
\subsubsection{for binary data}
A random experiment that only disguises between two possible outcomes is
called a \emph{Bernoulli} experiment. Random variable $X$ denotes the number of
successes in a Bernoulli experiment; $X$ therefore has possible values 0 and 1.
The probability distribution of $X$ is completely determined by the probablity of
success which we denote by $p$  and is $P(X=0) = 1 - p $ and $ P(X=1)=p$.
$X$ has the probability function $P(x)=p^x(1-p)^{1-x}$ for $x=0,1$ and
$0 \leq p \leq 1$.
log properities: $\log a^b=b \log a$, $\log ab = \log a + \log b$,
$\log \frac{a}{b}=\log a - \log b$.
The cartesion product 
${0,1} \times {0,1}$ creates a table of the bivariate Bernoulli random
vector $(X_1,X_2)$.
\begin{tabular}{llll}
  $P(x_1,x_2)$ & $x_2=0$ & $x_1=1$ & Total \\
  $x_1=0$ & $p(0,0)$ & $p(0,1)$ & $p_1(0)$ \\
  $x_1=1$ & $p(1,0)$ & $p(1,1)$ & $p_1(1)$ \\
  Total & $p_2(0)$ & $p_2(1)$ & $1$ \\
\end{tabular}
which can be written as
$P(x_1,x_2)=p(0,0)^{(1-x_1)(1-x_2)}p(0,1)^{(1-x_1)x_2}p(1,0)^{x_1(1-x_2)}p(1,1)^{x_1x_2}$
for $x_1=0,1$ and $x_2=0,1$.
Taking logarithms of this identity for $P$ and collecting terms in$x_1$ and
$x_2$ gives:
$\log P(x_1,x_2)= \begin{matrix}
    \log p(0,0) + x_1 \log \frac{p(1,0)}{p(0,0)} +  \\
    x_2 \log \frac{p(0,1)}{p(0,0)} + x_1 x_2 \log \frac{p(1,1)p(0,0)}{p(0,1)p(1,0)}
    \end{matrix}
$
reparameterizing the right hand side leads tothe so-caled \emph{log-linear expension}
$\log P(x_1,x_2)=u_\emptyset+ x_1u_1 + x_2 u_2 + x_1 x_2 u_{12}$
the $u$ coefficients are called $u$-terms, they substitue the log part.
The coefficient of the product $x_1x_2$ is the logarithm of the cross product
ratio:
$u_{12}=\log cpr(X_1,X_2)$
The random variables $X_1$ and $X_2$ are independent iff $u_{12}=0$
If $u_{12}=0$ we can take $g(x_1)=u_\emptyset + x_1 u_1$ and $h(x_2)= u_2 x_2$.
This is not posible with $u_{12}\neq 0$.
We can enforce (conditional) independence constraints by etting the right
$u$-terms to zero.
\subsection{non binary data}
To work with non binary data we make the u terms functions rather than constants:
$log P(x_1,x_2)=u_\emptyset + u_1(x_1) + u_2(x_2) + u_{12}(x_1,x_2)$
This produces to many parameters, and to identify them we have to impose
an extra constraint: $u_a(x_a)=0$ whenever $x_i=0$ and $i\in a$. Here we
assume that if $x_i$ has $d_i$ possible values, these are numbered
$0,1,...,d_i-1$, but not ordered.
\subsection{Hierarchical and graphical}
% TODO: perhaps delete this section?
The log-linear expension of the crossclassified multinominal density function
$P_K$ is $\log P_K(x)=\underset{a \subseteq K}{\sum} u_a(x_a)$ where the sum is
taken over all possible subsets of $K={1,2,...,k}$ and where the $u$-terms are
coorinate projection functions, so that $u_a(x)=u_a(x_a)$ and also satisfy the
constraint that $u_a(x)=0$ whenever $x_i = 0$ and $i\in a$.
\subsubsection{Independence and the u-terms}
$If(X_a,X_b,X_c)$ is a partitioned multinominal random vector then
$X_b \indep X_c|X_a$ if and only if all u-terms in the log-linear expension with
one or more coordinate in $b$ and one or more coordinate in $c$ are zero.
The proof is direct application of the factorisation theorom for conditional
independence.
\subsubsection{Graphical model}
Given an independence graph $G=(K,E)$ the cross-classified multinomal
distrubtion for the random vector $X$ is a graphical model for $X$ if the
distribution of $X$ is an arbitrary apart from constraints of the form that for
all pairs of coordinates not in the edge set $E$ of $G$ the $u$ terms containing
the selected coordinates are identically zero.
\subsection{Maximum likelhood estimation}
The maximum likelihood estimator of gaphical log linear model $M$ satisfies the
likelihood equations: $\overset{\wedge}{n}_a=N\overset{\wedge}{P}_a=n_a$
\section{Baysian networks}
\section{Freq item set mining}
\end{multicols}
\end{document}
