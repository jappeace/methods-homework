\documentclass{article}
\usepackage{amsmath}
\usepackage{txfonts}
\usepackage{booktabs}
\usepackage{color}
\usepackage{bussproofs}
\usepackage[pdftex]{graphicx}
\DeclareGraphicsRule{*}{mps}{*}{}
\usepackage{pifont}
\usepackage{qtree}
\usepackage{emp}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{svg}
\newenvironment{scprooftree}[1]%
{\gdef\scalefactor{#1}\begin{center}\proofSkipAmount \leavevmode}%
{\scalebox{\scalefactor}{\DisplayProof}\proofSkipAmount \end{center} }


\newcommand{\brcell}[2][l]{%
	\begin{tabular}[#1]{@{}l@{}}#2\end{tabular}}
\begin{document}
\begin{empfile}
\begin{empcmds}
input metauml;
\end{empcmds}
\author{Jappie Klooster}
\title{Graph bipartitioning and you}
\maketitle

\section{Introduction}
This is a report of the evolutionary practical assignment 2.
In here I will discuss the execution of the assignment, what it was, which
methodology was used to execute the assignment.

The intended audience is people familiar with programming.

\section{Assignment}
Graphbipartitioning is dividing a graph into 2, with as little edges between
both graphs as possible.

\section{Methodology}
I coppied all the code from the previous practical into this new folder.
Then I went onto a delete rage. Most code was deleted except the Expirment
class, this will probably still be usefull.

I use git, so it doens't really matter to accidently delete to much. After
that I changed the data, I removed everything which was useless. Now the file
only contains the connection numbers. This makes parsing the file a lot
easier. I don't want to do advanced pattern matching when I can quickly modfy
the file with a few vim macros. (in this case I just did a `30x').

While implementing ILS, I had it turn out results much worse than MLS.
The initial implmentation would just stop if it found a worse local optimum.
The description sais it should go back to the previous optimum. So I guess
that you should try again after that? But then I wonder how often you should
try again.

I got much better results with a system where the local search got several chances.
If it failed it would fallback, and then try again. 
With chances around 30 I managed to get up and around 1900.
When I reset the chance counter on a succesfull find
I managed to get consistently 2200, one result was as high as 2310.

I don't think there is a way to do it any better without drastically making
the algorithm more complex.
This current search for improvement was just based on the fact that ILS was
underperforming to MLS, which shouldn't be the case according to my memory
from the classes. Now its overperforming quite drastically so I'm happy.

I implemented GLS with the evolution class. The only thing I had to do really
was write the crossover method, the rest was already functional.

I wanted to see if the GLS popsize and selection pressure would have any
effect on the average fitness found. I suspect that a bigger population size
would find better results and that weaker selection would find better
average scores.

I chose to investigate with population sizes of 25, 50 and 75. I didn't want
to wait to long on this and the runtime for 50 was approaching ~10 minutes
for the slowest. Luckily I have a powerfull laptop and can use paralization.

Doing this 30 times to cancel the staostic effect would result into ~5 hours.
\section{TODO}
MLS with several results
GLS -> I think the localsearch will recursively unecesrly decend into its members
because the local search is packed in the member factory. Maybe remove the
local search from the memberfactory?

Use original file by doing a substr(30)
\end{document}


