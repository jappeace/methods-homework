\documentclass{article}
\usepackage{amsmath}
\usepackage{txfonts}
\usepackage{booktabs}
\usepackage{color}
\usepackage{bussproofs}
\usepackage[pdftex]{graphicx}
\DeclareGraphicsRule{*}{mps}{*}{}
\usepackage{pifont}
\usepackage{qtree}
\usepackage{emp}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{svg}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{seqsplit}

\pgfplotsset{
    compat=1.9,
    compat/bar nodes=1.8
}


\begin{filecontents*}{dataTable.txt}
zero 	name 		firsty 		secondy		errbotstart		errbotend 		errtopstart			errtopend
0 		MLS           0         	1			0.1				0.2				0.1	               0.2
0 		ILS           1         	2			0.3				0.1             1.5               0.4
0 		GLS           2         	3.5			0.5				0.2             0.3               0.4
\end{filecontents*}

\newenvironment{scprooftree}[1]%
{\gdef\scalefactor{#1}\begin{center}\proofSkipAmount \leavevmode}%
{\scalebox{\scalefactor}{\DisplayProof}\proofSkipAmount \end{center} }


\newcommand{\brcell}[2][l]{%
	\begin{tabular}[#1]{@{}l@{}}#2\end{tabular}}
\begin{document}
\begin{empfile}
\begin{empcmds}
input metauml;
\end{empcmds}
\author{Jappie Klooster}
\title{Graph bipartitioning and you}
\maketitle

\section{Introduction}
This is a report of the evolutionary practical assignment 2.
In here I will discuss the execution of the assignment, what it was, which
methodology was used to execute the assignment.

The intended audience is people familiar with programming.

\section{Assignment}
Graph bi-partitioning is dividing a graph into 2, with as little edges between
both graphs as possible.

\section{Methodology}
I copied all the code from the previous practical into this new folder.
Then I went onto a delete rage. Most code was deleted except the Experiment
class, this will probably still be useful.

I use git, so it doens't really matter to accidentally delete to much. After
that I changed the data, I removed everything which was useless. Now the file
only contains the connection numbers. This makes parsing the file a lot
easier. I don't want to do advanced pattern matching when I can quickly modify
the file with a few vim macros. (in this case I just did a `30x').

\subsection{MLS}
MLS basically involved implementing the initial vertex swap first improvement
local search and tying it into the existing framework. I did this by just
using the existing createPopMethods with a new solution generator. Then
I just mapped this population with the local search (so the type is
$IMember \to IMember$, but the solution changes).

\subsection{ILS}
While implementing ILS, I had it turn out results much worse than MLS.
The initial implementation would just stop if it found a worse local optimum.
The description says it should go back to the previous optimum. So I guess
that you should try again after that? But then I wonder how often you should
try again.

I got much better results with a system where the local search got several chances.
If it failed it would fall back, and then try again.
With chances around 30 I managed to get up and around 1900.
When I reset the chance counter on a successful find
I managed to get consistently 2200, one result was as high as 2310.

I don't think there is a way to do it any better without drastically making
the algorithm more complex.
This current search for improvement was just based on the fact that ILS was
under performing to MLS, which shouldn't be the case according to my memory
from class. Now its over-performing quite drastically.

\subsection{GLS}
I implemented GLS with the evolution class. The only thing I had to do really
was write the crossover method, the rest was already functional.

I wanted to see if the GLS population size and selection pressure would have any
effect on the average fitness found. I suspect that a bigger population size
would find better results and that weaker selection would find better
average scores.

Here I also started to think about my own topic to investigate, at first
I just wanted to do varying population sizes and varying selection methods.
But I thinks this takes a little to much time and it doesn't quite fit
on the tTest table so I decided to drop the varying pop-sizes.

\subsection{Counting double}
I had an issue in which my classmates would get much better results than me.
After discussing it for a while it turned out that I was counting double. However
This didn't make sense because I just used Scala combinators to count the
elements. Basically this means that there would have to be a bug in the std
Scala library which would seem highly unlikely. I knew I was counting double
because all my results where even numbers. After thinking about it for a while
I decided to investigate the graph file, it turns out that every connection
is made double, so the double counting was in fact correct.

\subsubsection{Graph pruning}
I decided to use a prune graph to speed up the algorithm. In the beginning I
create 2 graphs. One normal and one pruned, The pruning works by selecting
comparing all connections against the id, if lower we ignore the connection.
To make sure we only do this on doubly linked graphs 2 graphs are created.
If the prune graph has exactly half the score of a random solution we assume
its a doubly linked graph so we can prune it.
The pruning effectively halves the amount of connections that need to be
checked by each iteration (this happens in the evaluation). The
Fiduccia Mattheyses has even more gain because it has a lot of book keeping to
do with the connections.

\subsection{Fiduccia Mattheyses}
I was somewhat sceptical of my local search implementation. I had quite 
different results from my classmates so to compare I decided to implement
the Fiduccia Mattheyses local search to. It was quite a lot of work but it
truly is significantly faster.

Finding the paper was somewhat difficult as the original was behind a paywall.
But someone put a copy on GitHub so I just used that one. The algorithm 
involves a lot of cleaver book keeping. First of all the concept of gain is
introduced, this indicates how much gain there will be from moving a vertex
to the other partition. Then the vertexes are put into an array with as index
their gain. If there are more vertices at a particular index then they're
put into an linked list. To support this I introduced a container class
Cell that holds the vertex and also has an optional previous and next
element.
These cells are also put in another array based on their vertex ID. This
array is used for constant speed access when the connections of a vertex their
gain needs to be updated after moving it to the other partition.
So you can just untie the element of the linked list (or if it has no
previous in my case its in the front of the bucket and I remove it from
there and reinsert the next element).

After knowing the basic data structures the implementation is quite trivial
(although I had some tedious bugs such as index out of bounds errors or
next elements being wrongly updated).
It doesn't find the local optimum in one go, 4 to 5 iterations are
generally required, which is still much and much better than the first
improvement vertex swap.

\section{Own research topic}
I had several ideas for my own research to investigate, first of all I wanted
to do the really easy to implement ones, such as selection pressure and pop
sizes for GLS. But later, when waiting for results, I got curious about the
other local search I heard about in class. With several days left I decided
to try and implement the Fiduccia Mattheyses local search heuristic.
I think I've succeeded so my final research question is how does the 
Fiduccia Mattheyses algorithm compare to first
improvement vertex swap?

As can be seen in the results section the Fiduccia Mattheyses algorithm is
much faster than first improvement vertex swap. The main reason for this is
that a single pass will climb very steeply in valuation. It also appears
to produce better results, I think this is because highest gain is considered
first.

My other research topic is selection pressure, turns out that tournament is
the best for GLS in case of Fiduccia Mattheyses, but it also takes the mot
amount of time.

\subsection{Statistics}
The error margins in the graph are the maximum range of the results so these
are round numbers when subtracted from the low or added to the high. The low
and high values are calculated with something called a confidence interval.
It aligns pretty well with the tTest results so I guess its OK.

\newpage
\section{Results}
Fiduccia Mattheyses dominates vertex swap first improvement in both runtime and
optima found. Combined with GLS it gets such a good score that I became
skeptical and decided to test the answer more thoroughly.

Time wise Fiduccia Mattheyses dominates vertex swap first improvement. This
is because it starts with an incredibly steep hill climb that flattens
out quite quickly. So you just need to pass over the string 4 or 5
times and then you are at the local optimum. Vertex swap first improvement
increases linearly however. So it takes a lot more string iterations
and evaluations to get there.

The improvements found by Fiduccia Mattheyses are also a lot better, especially
the GLS variants. I'm not quite sure why this is the case, in fact I
added a paragraph scepticism and put a gene in there that found a solution
with 4 crossings on the original graph. Vertex swap first improvement gets
noway near to that. The lowest it found is like 280 crossings on the original
graph. I think the GLS variant doesn't find any better is because I always
start at index 0 of the string and take any improvement I can find, this
means the search space is biased. A random start could solve this issue but
then the function is no longer pure. The other bias vertex swap first
improvement has is that it either goes from left to write or from right to
left, you can't really do both. Fiduccia Mattheyses doesn't suffer from both
of these problem because it just picks whatever improvement has the maximum
gain (while keeping balance in mind), so its biased towards maximum gain,
which isn't a bad bias at all.

Something I found interesting is that the Fiduccia Mattheyses local search
doesn't seem to respond well to ILS it just stays in the range of the lower
error margin of MLS. To verify this I cranked up the validation number to
100 (100 times no better improvement found the local search end), and
increased the perturbation to 7 (because initially it looked like 1 to 5
where slowly going down).
However when you look to vertex swap first improvement you can see ILS
responding violently. In fact ILS-1 is tTest similar to GLS:Killparents.

Selection method seems to be statistically irrelevant for the optimum in
Fiduccia Mattheyses, although you pay a much bigger time price with both
tournament and kill parents selection. Tournament selection does seem to
be best for vertex swap first improvement, but again you pay a time price
for this.

\newpage
\subsection{Fiduccia Mattheyses}
\subsubsection{Time}
\pgfplotstableread{fidRuntimes.table}\dataTable
\begin{tikzpicture}
\pgfplotsset{set layers}
\begin{axis}[
	ybar stacked,
	xlabel={Algorithm},
	xticklabels from table={fidRuntimes.table}{name},
	xtick=data,
	x tick label style={rotate=90,anchor=east},
	ylabel={Time}
]
% Layers needed to get error bars on top of lower blue rectangle
\addplot[color=white]
table[on layer=axis grid, x expr=\coordindex, y=firsty]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=zero, y error minus=errbotstart, y error plus=errbotend]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=secondy, y error minus=errtopstart, y error plus=errtopend]{\dataTable};

\end{axis}

\end{tikzpicture}


\input{fidRuntimes.textable}

\newpage
\subsubsection{Optima}
\pgfplotstableread{fidResults.table}\dataTable
\begin{tikzpicture}
\pgfplotsset{set layers}
\begin{axis}[
	ybar stacked,
	xlabel={Algorithm},
	xticklabels from table={fidResults.table}{name},
	xtick=data,
	x tick label style={rotate=90,anchor=east},
	ylabel={Fitness (less is better)}
]
% Layers needed to get error bars on top of lower blue rectangle
\addplot[color=white]
table[on layer=axis grid, x expr=\coordindex, y=firsty]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=zero, y error minus=errbotstart, y error plus=errbotend]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=secondy, y error minus=errtopstart, y error plus=errtopend]{\dataTable};

\end{axis}

\end{tikzpicture}
\\
\input{fidResults.textable}

\paragraph{Scepticism} So these results seem a little to good to be true honestly,
especially compared to vertex swap first improvement. So I decided to catch
one of the results of GLS and throw it trough a bunch of tests to see if I 
could find anything wrong with it. The result is this one and it appears
to be OK with a fitness score of 2 (or 4 if you use the doubly connected graph).
I also checked if there was an equal amount of zeros and ones just to be sure.

\seqsplit{11100000101001111000000100110111101010000000110011100110011110101110110010111110010010010011000000101100111010011100001000111000110111110101110000010011111001110110100110110101110010101011000101000111100010111100010001100100000110001001010010101001110010111110001001010000001111011111111101101000101100000100011011101111001010010111001101101111100001011000001100100001011101000101111000101011101001001010101011011000110000110111111101101000001111000010010001111101000101000010111100101011100111100101}

There is a good chance this is a global optimum.

\newpage
\subsection{Vertex swap first improvement}
\subsubsection{Time}
\pgfplotstableread{vertRuntimes.table}\dataTable
\begin{tikzpicture}
\pgfplotsset{set layers}
\begin{axis}[
	ybar stacked,
	xlabel={Algorithm},
	xticklabels from table={vertRuntimes.table}{name},
	xtick=data,
	x tick label style={rotate=90,anchor=east},
	ylabel={Time}
]
% Layers needed to get error bars on top of lower blue rectangle
\addplot[color=white]
table[on layer=axis grid, x expr=\coordindex, y=firsty]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=zero, y error minus=errbotstart, y error plus=errbotend]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=secondy, y error minus=errtopstart, y error plus=errtopend]{\dataTable};

\end{axis}

\end{tikzpicture}


\input{vertRuntimes.textable}

\newpage
\subsubsection{Optima}
\pgfplotstableread{vertResults.table}\dataTable
\begin{tikzpicture}
\pgfplotsset{set layers}
\begin{axis}[
	ybar stacked,
	xlabel={Algorithm},
	xticklabels from table={vertResults.table}{name},
	xtick=data,
	x tick label style={rotate=90,anchor=east},
	ylabel={Fitness (less is better)}
]
% Layers needed to get error bars on top of lower blue rectangle
\addplot[color=white]
table[on layer=axis grid, x expr=\coordindex, y=firsty]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=zero, y error minus=errbotstart, y error plus=errbotend]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=secondy, y error minus=errtopstart, y error plus=errtopend]{\dataTable};

\end{axis}

\end{tikzpicture}
\\
\input{vertResults.textable}
\end{empfile}
\end{document}
