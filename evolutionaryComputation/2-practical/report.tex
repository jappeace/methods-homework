\documentclass{article}
\usepackage{amsmath}
\usepackage{txfonts}
\usepackage{booktabs}
\usepackage{color}
\usepackage{bussproofs}
\usepackage[pdftex]{graphicx}
\DeclareGraphicsRule{*}{mps}{*}{}
\usepackage{pifont}
\usepackage{qtree}
\usepackage{emp}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{svg}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{seqsplit}

\pgfplotsset{
    compat=1.9,
    compat/bar nodes=1.8
}


\begin{filecontents*}{dataTable.txt}
zero 	name 		firsty 		secondy		errbotstart		errbotend 		errtopstart			errtopend
0 		MLS           0         	1			0.1				0.2				0.1	               0.2
0 		ILS           1         	2			0.3				0.1             1.5               0.4
0 		GLS           2         	3.5			0.5				0.2             0.3               0.4
\end{filecontents*}

\newenvironment{scprooftree}[1]%
{\gdef\scalefactor{#1}\begin{center}\proofSkipAmount \leavevmode}%
{\scalebox{\scalefactor}{\DisplayProof}\proofSkipAmount \end{center} }


\newcommand{\brcell}[2][l]{%
	\begin{tabular}[#1]{@{}l@{}}#2\end{tabular}}

% "define" Scala
\lstdefinelanguage{scala}{
  morekeywords={abstract,case,catch,class,def,%
    do,else,extends,false,final,finally,%
    for,if,implicit,import,match,mixin,%
    new,null,object,override,package,%
    private,protected,requires,return,sealed,%
    super,this,throw,trait,true,try,%
    type,val,var,while,with,yield},
  otherkeywords={=>,<-,<\%,<:,>:,\#,@},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  morestring=[b]',
  morestring=[b]"""
}
\begin{document}
\begin{empfile}
\begin{empcmds}
input metauml;
\end{empcmds}
\author{Jappie Klooster}
\title{Graphs, partitioning and you}
\maketitle

\section{Introduction}
This is a report of the evolutionary practical assignment 2.
In here I will discuss the execution of the assignment, what it was, which
methodology was used to execute the assignment.

The intended audience is people familiar with programming.

\section{Assignment}
Graph bi-partitioning is dividing a graph into 2 partitions, 
with as little crossings between both partitionings as possible.

So we try to reduce the amount of crossing by modifiying which vertex resides
in what partition. The graph itself is however not modified. The amount of
vertexes that reside in on partition should also be even to the other partition.

\section{Methodology}
This time again I chose to use scala to solve this assignment. I was pleasently
surprised last time by the power of scala's combinators and hope this will be
the case again. I've become quite a fan of the langauge.

\subsection{MLS}
MLS basically involved implementing the initial vertex swap first improvement
local search and tying it into the existing framework. I did this by just
using the existing createPopMethods with a new solution generator. Then
I just mapped this population with the local search (so the type is
$IMember \to IMember$, but the solution changes).

\newpage
\subsubsection{Reading the graph}
The first thing I did was reading the graph into a memory structure. That 
looks like this:

\begin{figure}[ht!]
\centering
\begin{emp}[classdiag](20,20)
Class.graph("Graph")("+verteci:Array[Vertex]", "+edgeCount:Int")();
Class.vertex("Vertex")("+id:Int", "+connections:Seq[Int]")();

leftToRight.top(30)(graph,vertex);


drawObjects(graph,vertex);

link(association)(graph.e -- vertex.w);
item(iAssoc)("*")(obj.se = vertex.w);

\end{emp}
\caption{Case classes}
\end{figure}

So the file is stored in an imutable memory structure. This is possible
because the connections are just the number that essently work as pointers
if you have the graph at hand.

\subsubsection{Solution generation}
Then I went on doing the generation of the solutions. I did this by creating
a random string of half the graph length, then copying this string and invert
it. This ensured there were as many ones as zeros. Then I combined these
two strings and randomly shuffled them:

\lstset{language=scala}
\begin{lstlisting}
	val random = Random
	def randomGene = random.nextInt(2).toString
	def randomGenes(geneLength:Int) = 1.to(geneLength).
		foldLeft(""){(b, x) => b + OffspringGenerator.randomGene}
	def randomBalancedGenes(geneLength:Int):String = {
		if(geneLength % 2 == 1){
			throw new Exception("Doesn't work with odd genelength")
		}
		val initial = randomGenes(geneLength/2)
		random.shuffle(
			inverseMirror(initial).toCharArray.toSeq
		).mkString
	}
	def flipBit(c:Char):Char = inverseResult(c,'1','0')
	def inverseMirror(input:String):String ={
		input + input.map(flipBit)
	}
\end{lstlisting}

\subsubsection{Valuation}
After that I did the evaluation function. Which was just folding the vertexes
into a float and then counting the string indexes that did match. I think
I did this wrong by mistake at first, but I realized it was advantageous to
count the connections that are the same instead of different. Because now
we want to maximize again as in the previous assignment, which made it
possible to leave the selection as is (so I don't have to modify that when
implementing GLS). To represent the internal valuation 
(which is trying to be maximized) as the minimizing
problem I simply do $graph.edgecount-valuation$.

\subsubsection{Local search}
Finally I designed the search class, which was first called LocalSearch but
was later generalized to Search:

\begin{figure}
\begin{emp}[classdiag](20,20)
Class.search("Search")(
	"-method:IMember=>IMember", 
	"-stopCondition:(IMember, IMember)=>Boolean",
	"-decideResult:(IMember, IMember)=>IMember"
	)(
	"+search(previous:IMember):IMember"
	);

drawObjects(search);
\end{emp}
\caption{Search}
\end{figure}

A very compact class indeed, we execute method on previous, check the
stopcondition if true we decide result else we call ourself (tail) recursively.
The stopcondition and decide result are by default equal and return the result
of the method. Why these functions are configurable will be discussed in the 
ILS chapter.
Then the method is either filled with vertexSwapFirstImprovement or
FiducciaMattheysesSearch.

\subsection{ILS}
While implementing ILS, I had it turn out results much worse than MLS.
The initial implementation would just stop if it found a worse local optimum.
The description says it should go back to the previous optimum. So I guess
that you should try again after that? But then I wonder how often you should
try again.

I got much better results with a system where the local search got several chances.
If it failed it would fall back, and then try again.
With chances around 30 I managed to get up and around 1900.
When I reset the chance counter on a successful find
I managed to get consistently 2200, one result was as high as 2310. Note:
I was maximizing here and still counting double. For the final run I put
the validation count up to 100. So before we decide something is at a local
optimum it first has to return the same result 100 times.

\subsubsection{Implementation}
At first I started to wrote a separate ILS class for search below the then
local search class. However I noticed the code I was producing looked 
suspiciously similar to what I already had in local search. So I decided
to try and combine the functionalities by making LocalSearch more configurable
and I succeeded. The end result was the Search class (I had to rename
the local search class because it was no longer just doing local search).

To explain how it works the first thing one should do is notice how the method
of search has the same prototype as the search method of the search class.
This means you can nest a search method of one search class inside another
search class. So we now have one inner search class doing the local search
and one outer search class doing iteration and perturbation of solutions.

Actually the perturbation is also nested inside a method like prototype and
I use function composition to let the perturbation be executed just before
the local search can be executed.

as stopcondition of the outer search we have an isWorse function inside a
resetretryOnBetter function. this function has also access to the retryOnResult
object which counts to the validation (and can be rest on a better result).
\subsection{GLS}
I implemented GLS with the evolution class. The only thing I had to do really
was write the crossover method, the rest was already functional.

I wanted to see if the GLS population size and selection pressure would have any
effect on the average fitness found. I suspect that a bigger population size
would find better results and that weaker selection would find better
average scores.

So I also started to think about my own topic to investigate, at first
I just wanted to do varying population sizes and varying selection methods.
But I thinks this takes a little to much time and it doesn't quite fit
on the tTest table so I decided to drop the varying pop-sizes.

\subsection{Counting double}
I had an issue in which my classmates would get much better results than me.
After discussing it for a while it turned out that I was counting double. However
This didn't make sense because I just used Scala combinators to count the
elements. This would mean that there should be a bug in the std
Scala library which seems highly unlikely. I knew I was counting double
because all my results were even numbers. After thinking about it for a while
I decided to investigate the graph file, it turns out that every connection
is made double, so the double counting was in fact correct.

\subsubsection{Graph pruning}
I decided to use a prune graph to speed up the algorithm. In the beginning I
create 2 graphs. One normal and one pruned, The pruning works by
comparing all connections against the id, if lower we ignore the connection.
To make sure we only do this on doubly linked graphs 2 graphs are created.
If the pruned graph has exactly half the score of a random solution we assume
its a doubly linked graph so we can prune it. I know its not a
guaranteed to work.
The pruning effectively halves the amount of connections that need to be
checked by each iteration (this happens in the evaluation). The
Fiduccia Mattheyses has even more gain because it has a lot of book keeping to
do with the connections.

\subsection{Fiduccia Mattheyses}
I was somewhat sceptical of my local search implementation. I had quite
different results from my classmates so to compare I decided to implement
the Fiduccia Mattheyses local search to. It was quite a lot of work but it
truly is significantly faster. Finding the paper was somewhat difficult
as the original was behind a paywall.  But someone put a copy on GitHub so
I just used that one.

The algorithm involves a lot of clever book keeping.
First of all the concept of gain is introduced, this indicates how much
gain there will be from moving a vertex to the other partition. Then the
vertexes are split based on partition and put into an array with as index
their gain (the paper calls this a bucket, as do I).
If there are more vertices at a particular index then they're
put into an linked list. To support this I introduced a container class
Cell that holds the vertex and also has an optional previous and next
element. (Again the paper uses the name cell to).

These cells are also put in another array based on their vertex ID (position
in the solution string). This
array is used for constant access speed when the \emph{connections of a vertex}
their gain needs to be updated after moving the initial vertex to the other
partition.
So you can just untie the element of the linked list (or if it has no
previous in my case, its in the front of the bucket and I remove it from
there and reinsert the next element and itself to the same bucket at the
proper gain index).

After knowing the basic data structures the implementation is quite trivial
(although I had some tedious bugs such as index out of bounds errors or
next elements being wrongly updated).
It doesn't find the local optimum in one go, 4 to 5 iterations are
generally required, which is still much and much better than the first
improvement vertex swap.

\section{Own research topic}
I had several ideas for my own research to investigate, first of all I wanted
to do the really easy to implement ones, such as selection pressure and pop
sizes for GLS. But later, when waiting for results, I got curious about the
other local search I heard about in class. With several days left I decided
to try and implement the Fiduccia Mattheyses local search heuristic.
I think I've succeeded so my final research question is how does the 
Fiduccia Mattheyses algorithm compare to first improvement vertex swap?

As can be seen in the results section the Fiduccia Mattheyses algorithm is
much faster than first improvement vertex swap. The main reason for this is
that a single pass will climb very steeply in valuation. It also appears
to produce better results, I think this is because highest gain is considered
first. But more about this in the results.

\subsection{Statistics}
The error margins in the graph are the maximum range of the results so these
are round numbers when subtracted from the low or added to the high. The low
and high values are calculated with something called a confidence interval.
It aligns pretty well with the tTest results.

\newpage
\section{Results}
Fiduccia Mattheyses dominates vertex swap first improvement in both runtime and
optima found. Combined with GLS it gets such a good score that I became
skeptical and decided to test the answer more thoroughly.

Time wise Fiduccia Mattheyses dominates vertex swap first improvement. This
is because it starts with an incredibly steep hill climb that flattens
out quite quickly. So you just need to pass over the string 4 or 5
times and then you are at the local optimum. Vertex swap first improvement
increases linearly however. So it takes a lot more string iterations
and evaluations to get there.

The improvements found by Fiduccia Mattheyses are also a lot better, especially
the GLS variants. I'm not quite sure why this is the case, in fact I
added a paragraph scepticism and put a gene in there that found a solution
with 4 crossings on the original graph. Vertex swap first improvement gets
noway near to that. The lowest it found is like 280 crossings on the original
graph. I think the GLS variant doesn't find any better is because I always
start at index 0 of the string and take any improvement I can find, this
means the search space is biased. A random start could solve this issue but
then the function is no longer pure. The other bias vertex swap first
improvement has is that it either goes from left to write or from right to
left, you can't really do both. Fiduccia Mattheyses doesn't suffer from both
of these problem because it just picks whatever improvement has the maximum
gain (while keeping balance in mind), so its biased towards maximum gain,
which isn't a bad bias at all.

Something I found interesting is that the Fiduccia Mattheyses local search
doesn't seem to respond well to ILS it just stays in the range of the lower
error margin of MLS. To verify this I cranked up the validation number to
100 (100 times no better improvement found the local search end), and
increased the perturbation to 7 (because initially it looked like 1 to 5
where slowly going down).
However when you look to vertex swap first improvement you can see ILS
responding violently. In fact ILS-1 is tTest similar to GLS:Killparents.

Selection method seems to be statistically irrelevant for the optimum in
Fiduccia Mattheyses, although you pay a much bigger time price with both
tournament and kill parents selection. Tournament selection does seem to
be best for vertex swap first improvement, but again you pay a time price
for this.

\newpage
\subsection{Fiduccia Mattheyses}
\subsubsection{Time}
\pgfplotstableread{fidRuntimes.table}\dataTable
\begin{tikzpicture}
\pgfplotsset{set layers}
\begin{axis}[
	ybar stacked,
	xlabel={Algorithm},
	xticklabels from table={fidRuntimes.table}{name},
	xtick=data,
	x tick label style={rotate=90,anchor=east},
	ylabel={Time}
]
% Layers needed to get error bars on top of lower blue rectangle
\addplot[color=white]
table[on layer=axis grid, x expr=\coordindex, y=firsty]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=zero, y error minus=errbotstart, y error plus=errbotend]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=secondy, y error minus=errtopstart, y error plus=errtopend]{\dataTable};

\end{axis}

\end{tikzpicture}


\input{fidRuntimes.textable}

\newpage
\subsubsection{Optima}
\pgfplotstableread{fidResults.table}\dataTable
\begin{tikzpicture}
\pgfplotsset{set layers}
\begin{axis}[
	ybar stacked,
	xlabel={Algorithm},
	xticklabels from table={fidResults.table}{name},
	xtick=data,
	x tick label style={rotate=90,anchor=east},
	ylabel={Fitness (less is better)}
]
% Layers needed to get error bars on top of lower blue rectangle
\addplot[color=white]
table[on layer=axis grid, x expr=\coordindex, y=firsty]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=zero, y error minus=errbotstart, y error plus=errbotend]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=secondy, y error minus=errtopstart, y error plus=errtopend]{\dataTable};

\end{axis}

\end{tikzpicture}
\\
\input{fidResults.textable}

\paragraph{Scepticism} So these results seem a little to good to be true honestly,
especially compared to vertex swap first improvement. So I decided to catch
one of the results of GLS and throw it trough a bunch of tests to see if I 
could find anything wrong with it. The result is this one and it appears
to be OK with a fitness score of 2 (or 4 if you use the doubly connected graph).
I also checked if there was an equal amount of zeros and ones just to be sure.

\seqsplit{11100000101001111000000100110111101010000000110011100110011110101110110010111110010010010011000000101100111010011100001000111000110111110101110000010011111001110110100110110101110010101011000101000111100010111100010001100100000110001001010010101001110010111110001001010000001111011111111101101000101100000100011011101111001010010111001101101111100001011000001100100001011101000101111000101011101001001010101011011000110000110111111101101000001111000010010001111101000101000010111100101011100111100101}

There is a good chance this is a global optimum.

\newpage
\subsection{Vertex swap first improvement}
\subsubsection{Time}
\pgfplotstableread{vertRuntimes.table}\dataTable
\begin{tikzpicture}
\pgfplotsset{set layers}
\begin{axis}[
	ybar stacked,
	xlabel={Algorithm},
	xticklabels from table={vertRuntimes.table}{name},
	xtick=data,
	x tick label style={rotate=90,anchor=east},
	ylabel={Time}
]
% Layers needed to get error bars on top of lower blue rectangle
\addplot[color=white]
table[on layer=axis grid, x expr=\coordindex, y=firsty]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=zero, y error minus=errbotstart, y error plus=errbotend]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=secondy, y error minus=errtopstart, y error plus=errtopend]{\dataTable};

\end{axis}

\end{tikzpicture}


\input{vertRuntimes.textable}

\newpage
\subsubsection{Optima}
\pgfplotstableread{vertResults.table}\dataTable
\begin{tikzpicture}
\pgfplotsset{set layers}
\begin{axis}[
	ybar stacked,
	xlabel={Algorithm},
	xticklabels from table={vertResults.table}{name},
	xtick=data,
	x tick label style={rotate=90,anchor=east},
	ylabel={Fitness (less is better)}
]
% Layers needed to get error bars on top of lower blue rectangle
\addplot[color=white]
table[on layer=axis grid, x expr=\coordindex, y=firsty]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=zero, y error minus=errbotstart, y error plus=errbotend]{\dataTable};

\addplot[color=red] plot[on layer=axis lines,error bars/.cd, y dir=both, y explicit] 
table[x expr=\coordindex, y=secondy, y error minus=errtopstart, y error plus=errtopend]{\dataTable};

\end{axis}

\end{tikzpicture}
\\
\input{vertResults.textable}
\end{empfile}

\section{Conclusion}
Fiduccia Mattheyses combined with GLS finds the best solution at the cost of
quite some time, but the results are very close to the global optimum.

\end{document}
