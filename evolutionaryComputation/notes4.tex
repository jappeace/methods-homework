\documentclass{article}
\usepackage{txfonts}
\usepackage{booktabs}
\usepackage{color}
\usepackage{bussproofs}
\usepackage{graphicx}
\usepackage{pifont}
\usepackage{qtree}
\usepackage{tikz}
\usepackage{listings}
\usepackage{hyperref}
\newenvironment{scprooftree}[1]%
{\gdef\scalefactor{#1}\begin{center}\proofSkipAmount \leavevmode}%
{\scalebox{\scalefactor}{\DisplayProof}\proofSkipAmount \end{center} }


\newcommand{\brcell}[2][l]{%
	\begin{tabular}[#1]{@{}l@{}}#2\end{tabular}}
\begin{document}
\author{Jappie Klooster}
\title{Evolutionary stretegies}
\maketitle

\section{difference from GA}
GA's are local searches, while ES are global searches.
\section{Comenly used settings}
Coveriance = the angle, no coveriance means semetrical

The closer to the optimum the smaller your variance should be. This reduces
the chance of jumping over the optimum.

Normally distributed = gausian

\section{mutation}

\section{reccombination}
the parents is the entire population

\section{selection}
 methods $(u,\lambda)$ and $(u+\lambda)$, $(u+\lambda)$ appears better from
 an optimization standpoint but self adoptation works worse with this one.

feet4, each edge = a weight, every neuron is a function.

\section{Combinatorial}
is a finite set of solutions and you have to optimize it.

\subsection{Local search}
always try to improve the current solution by applying small changes.

Meta heuristics, for example taboo list


\section{Second practical}

MLS, ILS and GLS for graph Bipartioning.

the easiest thing to do is how long and see on waht
percentage they from the total??


Graph biportitioning, cut the graph into 2 groups
of 250

if the hamming distance is bigger than 2, just take
the complement of the 2?

\section{Knapsack}
Adaptive pursuit, pick one random with probability x, change
probability while running.

\section{Adaptive pursuit}
probability matching is used most often in literature, but its not
really that good.

Markov!

\subsection{Strategy}
the learning pase is a prarameter you have to play with.

\section{Time complexity}
Convergence speed: easy problems easier to calculate

population should be large enough, reasonably.

Gentic programming uses extreme high selection pressure
	creating lisp with GA's.

Using this allows much better scalimg of the algortihm.

\section{Population sizing}
\subsection{Random walks}
\[ P_N(x) = Probablity of reaching state N in postition x?\]
Gamblers ruin

\subsection{Exam}
How does it work. How does it scale up?

\section{Map labeling}
Has a bunch of little local optimum features.

all the little conditions were pulled from the fitness function
and put in the local optimizer. This meant they didn't have to
invent a bunch of weights.

Don't put everything in a fitness function. Especially if you have
to invent weights.

Excact is better, but GA's tend to scale better.


Try to keep the fitness function simple.

\section{Model GA}
Learn structure from good solutions.
Is very broad applicable because few assumptions.

Probability model is a vector(1,0,0.5)
Where 0.5 = random,
0 = 100 percent certainty of 0
1 = 100 percent certainty of 1

Overfitting, you don't learn anything because you
made it perfect.

deception is the problem, not non lineararity.

\section{PMGA}
Modelling baysion networks you should look at the fitting of data *and* model
complexity

Foss can be anything, for example:
  3456
0110110
1234

with the first and alst row being indeci of subsets.

univariate is just one big set.
marginal product = Mutex set
Hiercial tree, multiple level of linkage, the bottom no linkage the top 
uniform and marginal product in between in varying levels.

mutual information is super important concept\ldots

\end{document}
