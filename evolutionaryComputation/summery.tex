\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}


\pdfinfo{
  /Title (example.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Seamus)
  /Subject (Example)
  /Keywords (pdflatex, latex,pdftex,tex)}

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
    {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
        {\geometry{top=0.5cm,left=0.5cm,right=0.5cm,bottom=0.5cm} }
        {\geometry{top=0.5cm,left=0.5cm,right=0.5cm,bottom=0.5cm} }
    }

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\underline{Evolutionary Computing}} \\
\end{center}

\section{Genetic algorithms}
%TODO: crossover methods I don't understand
%TODO: schemas, buidling block theory
\subsection{Schema}
$o(h)$: schema order $o(11\#\#0)$ ie nonwildcard \# elements.
$\delta(h)$: schema defining length, maximum distance between two defining
symbols. $m(h,t)$: number of schema h instances at generation t.
$f(h,t)=\sum_{i\in P}f_i$ schema fitness is average fitness of members.

\emph{Buidling blocks} low order high performance schemate receive exponentiaonally
(geometrically) increasing trials.

\subsubsection{Growth by selection}
reproduction ratio $\phi(h,t)=\frac{m(h,t^s)}{m(h,t)}$.

\emph{Proportionate selection}
$f_i$ fitness ind i. Probability individiual i selected: $\frac{f_i}{\sum f_i}$.
$N$ pop size. Expected number of copies
$\frac{f_i}{\sum f_i} \cdot N = \frac{f_i}{f(t)}$.
Expected number of copies of schema $h$ members:
$m(h,t^s) = m(h,t)\phi(h,t)=m(h,t)\frac{f(h,t)}{f(t)}$.

\emph{Tournament selction}
tournament size $s: 0 \leq \phi(h,t) \leq s$.

\subsubsection{Distruption by mutation}
probability bit flipped: $p_m$.
schema $h$ survives iff all the bit values are \emph{not} mutated
$p_{survival}=(1-P_m)^{o(h)}$. for small values $p_m << 1$,
$(1-p_m)^{o(h)}\approx 1 - o(h)\cdot p_m$.
disruption factor $\epsilon(h,t)$ by mutation: $\epsilon(h,t)=o(h)\cdot P_m$.

\subsubsection{Distruption by recombination}
Probablity crossover applied $P_c$. Bit swap probability: $p_x$.

\emph{1-point crossover} schema $h$ survives iff cutpoint \emph{not}
withing defining length $\delta$: $p_{survival} = 1-\frac{\delta(h,t)}{l-1}$

\emph{Uniform crossover} Schema $h$ survives iff none or all bits
swapped together. $p_{survival}=p^{o(h)}_x + (1-p_x)^{o(h)}$.

\emph{Distruption factor} $\epsilon(h,t)$ by recombination:
$\epsilon(h,t)=P_c\cdot(1-p_{survival})$

\subsubsection{Theorem}
Selection mutation and recombination combined:
$m(h,t+1)\leq m(h,t)\phi(h,t)[1-\epsilon(h,t)]$.
Net growth factor: $\gamma(h,t)=\frac{m(h,t+1)}{m(h,t)}$.
\subsection{Permutation problems}
2-opt m, select 2 points and reverse a subsequance.
adjacency of elements is important, 2-opt has minimal change.
relative ordering is important: 2-opt large change.
\subsubsection{Crossover}
std. crosover won't work cause they can make invalid solution.
ie : $abcd+CBDA\to Cbcd$. Order crossover: rnd select 2 points,
copy subseq between crosspoints from p1, starting at 2nd crosspoint
fill in missing elements retaining order from p2. partially
mapped crosover: rnd select 2 points, copy p2 to child, copy
elements between crospoint from p1 to childe while placing
the replaced element from p2 at the location where the replacer is.
Position crossover: rnd select k postitions, copy unmarked elements from
p1 to child, scan p2 from lft to rght and fill in missing elemnts. Max 
preservative cross: rnd select two points, copy subseq between crospoints
from p1, add succesivily an adjecent element from p2 starting at
last element in child, if already placed take and adjecent elemnt form
p1. Cycle crossover: mark cycles, cross full cycles.

\subsection{Fitness Correlation coefficents}
Genetic operators should perserve usefull fitness characaristics.
Calcualte the fitness coorlation coeficient to quantify this.
$F_p$: mean fitness of the parents. $F_c$: mean fitness of the children.
$\sigma(F_p) =$ standard deviation of fitness parents
$\sigma(F_c) =$ std. dev. of children. coveriance fitness between parents and children:
$cov(F_p,F_c)=\sum^n_{i=1}\frac{(f(p_{gi})-F_p)(f(c_{gi})-f_c)}{n}$.
Operator fitness correlation coeficient
$\rho_{op}=\frac{cov(F_p,F_c)}{sigma(F_p)\sigma(F_c)}$.

\section{Metaheursitics}
Crossover can have mutliple search biases:
1. Random sampling witin a specific subspace. 2. place together for contrast
partial solutions from two parent solutions.

\subsection{PBMS}
Probablistic Modal Building: Probability distributions model dependencies 
between variables present in good solutions. sSelection makes these fitness 
based dependencies stand out. Estimating a probablity modal over the selected
solutions identifies these dependencies. Drawing new samples form the probability
modal will respect the dependencies.
PMBLS = PBM + LS. Bivariate probalbistic modal learns the pairwise dependencies
between problem dependencies. Buildign a dependency tree=maximum spanning tree
over the dependency graph. New solutions obtained by sampling from the dependency
tree.
For graphbipartitioning. Probablistic model: count the frequnceis that two
verticies are in the same parition. Dep tree build over most extreme frequency
values. reduce computational complexity by only considering the
pairwise interactions between connected vertices
\section{ES representation}
Fitness function: $f(x_1,\dots,x_n):\mathbb{R}^n\to \mathbb{R}$.
Genotype representation of an individual solution:
$(x_1,\dots,x_n,\sigma^2_1,\dots,\sigma^2_n, c_{12},\dots,c_{n-1,n})$,
paremeters $(x_1, \dots,x_n)$ need to be optimized.
Individiual solution consists of 3 parts: $\vec{x}$: problem vairables
$\Rightarrow$ Fitness $f(\vec{x})$. $\vec{sigma}$: standard deviations
$\Rightarrow$ variances. $\vec{\alpha}$: rotation angles $\Rightarrow$
covariances. The $n$-dimensional normal probability density function:
$p(X=x_1,\dots,x_n)=\frac{exp(-\frac{1}{2}X^TC^{-1}X)}{\sqrt{(2\pi)^n|C|}}$.
$C$: correlation matrix ($c_{ij}$; $|C|$ determinant $\Rightarrow$ rotation 
angles $\alpha_{ij}$: tan $2\alpha_{ij}=2c_{ij}/(\sigma^2_i-\sigma^2_j)$.
1-dimensional gaussian function:
$f(x)=\frac{1}{\sqrt{2\pi \sigma^2}}exp\frac{(x-\mu)^2}{2\sigma^2}$
\subsection{Mutation}
Complete covairance matrix: $\sigma_1,\dots,\sigma_n$; $c_{ij}\neq 0 (i\neq j)$.
The strategy paremeters are mutated:
$\sigma'_i=\sigma_ie^{
	\frac{\mathbb{N}(0,1)}{\sqrt{2n}}+
	\frac{\mathbb{N}_i(0,1)}{\sqrt{2\sqrt{n}}}
}$
$\alpha'_j=\alpha_j+\beta\mathbb{N}_j(0,1)$.
$\beta = 5$ degree in radians. $\mathbb{N}(0,1)$: standard normal
distribution. Then the problem paremeters are mutated with the new
strategy: $\vec{x}'=\vec{x}+\vec{N}(\vec{0},\vec{\sigma},\vec{\alpha}')$.
$\vec{N}$: n-dimensional normal distribution.
Often this is simplified by not doing angles or even just having a single
standard deviation (ie no index at $\sigma$ and nothing after the + 
in the power $e$).
\subsection{Recombination}
create one offspring from several parents selected randomly. Problem parameters
are averaged. The std devation is selected from one of the parents. Rotation
angles are not recombined
\subsection{Selection}
ES applies a high selection pressure: from $\mu$ parents $\lambda$ offspring
are generated with $\lambda >> \mu$ (~5 to 10 times). The best $\mu$ solutions
of the $\lambda$ offspring are selected, (or from both offspring and parents,
but experiments show just offspring has better selfadaptation).
\section{Ada pursuit}
Given: Set of $K$ \emph{operators} $A=\{a_1,\dots,a_k\}$. 
\emph{Probability vector} $P(t) = \{P_1(t),\dots,P_k(t)\}$. operator $a_i$ 
applied at time t in proportion to probability $P_i(t)$. env. returns rewards
$R_i(t) \geq 0$. Goal: Adapt $P(t)$ so that expected value of cumalitive
reward $\varepsilon[R]=\sum^T_{t=1}R_i(t)$ is maximized.

\subsection{Probabilty matching}
update $P(t)$ so probability of applying operator $a_i$ matches
the proportion of the estimated reward $Q_i(t)$ to the sum of all reward 
estimates $\sum^K_{a=1} Q_a(t)$
\subsubsection{PM: Reward estimate}
compute an estimate of the rewards, and older rewards should be less
influantial in non-stationary envs:
$Q_a(t+1)=Q_a(t)+\alpha[R_a(t)-Q_a(t)]$

\subsubsection{PM: Probability adaptation}
In non-stationary envs the probabilty of applying
any operator should never be less than some minmal \emph{thershold} $P_{min} > 0$.
For $K$ operators maximal probablity $P_{max} = 1-(K-1)P_{min}$. updating rule
for $P(t)$: $P_a(t+1)=P_{min}+(1-K\cdot P_{min})\frac{Q_a(t)}{\sum^K_{i=1}Q_i(t)}$.
\subsubsection{PM: Problem}
If one operator is consistently better probablity will still converge to average.

\subsection{Adaptive Pursuit strategy}
Update $P(t)$ so \emph{operator} $a^*$ that currently has the max estimated
reward $Q_{a^*}(t)$ is \emph{persued}. To achieve this the pursuit method
\emph{increases} selection probability $P_{a^*}$ and decreases all other
probabilities $P_a(t),\forall a\neq a^*$. So here it can deal with non-static envs.
\subsubsection{APS: Adaptive method}
Similiar to probability matching:
$Q_a(t+1)=Q_a(t)+\alpha[R_a(t)-Q_a(t)]$. Different from probablity matching:
Selection probability $P(t)$ is adapted in a greedy way.
\subsubsection{APS: Probability adoptation}
The selection probability of the current best operator
$a^*=argmax_a[Q_a(t+1)]$ is \emph{increased} $(0<\beta<1)$:
$P_{a^*}(t+1)=P_{a^*}(t)+\beta[P_{max}-P_{a^*}(t)]$.
The others are \emph{decreased}: 
$\forall a\neq a^*: P_a(t+1)=P_a(t)+\beta[P_{min}-P_a(t)]$

%TODO: the psuedo code??

\section{Convergence models}
Number of fitness function evaluations is proportional to the
number of generations run and pop size $FitFunc.count = Gen.count * Popsize$.
Selection algorithm and variation operators can be ignored.

\emph{Convergence speed:}
High selection presure is faster covnergence. Bigger popsize give better solutions.
Minimal fitness function evals is a tradeoff between selection presure and popsize.
\subsection{Selection Intensity}
Quantifies the speed of convergence. $I$ selection intensity. Response to
selection $R(t)$ is difference between mean fitness of generation $t+1$ and fitness
of generation $t$. Selection differential $S(t)$ is the diff between parent pop
at generation $t$ and mean fitness of generation $t$. 
$S(t)=\overline{f(t^s)}-\overline{f(t)}$. Assuming the pop fitness is normally
distributed $N(\overline(f),\sigma^2)$ we can scale the selection differential
by thestandard deiation $\sigma(t)$. This scaled selection differential is 
called selection intensity $I(t)$. This is a dimensionless number since 
the std deiation has the same units as the selection response:
$I(t)=\frac{S(t)}{\sigma(t)}=\frac{\overline{f(t^s)}-\overline{f(t)}}{\sigma(t)}$.
Standerdizing the normal distribution $(\overline(f)=0,\sigma=1)$ shows that
selection intensity $I$ is simply the epected average fitness
of the population after applying the selection scheme to a population
with standardized normal distibuted fitness $(N(0,1))$.
The relation between the response to selection $R$ and the selection
differential $S$ is given by the heritability $h^2$: $R(t)=h^2S(t)$ or
$R(t)=h^2\sigma(t)I(t)$.

\subsection{Proportionate selection}
$P_i(t)$ the proportion of occurences of individual $i$ at generation $t$.
$i$ has fitness $f_i$ and the mean fitness of generation $t$ is $\overline{f(t)}$.
$P_i(t^s)$ the proportion of individual $i$ in the parent pool
after applying proportionate selection: $P_(t^s)=P_i(t)\frac{f_i}{\overline{f(t)}}$.
The selection differential of proportiaonate selection is: 
$S(t)=\overline{f(t^s)}-\overline{f(t)}=\frac{\sigma^2(t)}{\overline{f(t)}}$
The selection intensity $I(t)=\frac{S(t)}{\sigma(t)}$ is tus eaual
to the ratio of the std devation of the fitness and the pop mean fitness:
$I(t)=\frac{\sigma(t)}{\overline{f(t)}}$


\end{multicols}
\end{document}
