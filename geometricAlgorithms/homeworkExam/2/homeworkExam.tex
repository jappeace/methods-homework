\documentclass{article}
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{pst-solides3d}
\usepackage{auto-pst-pdf}
\usepackage{tikz}
\usepackage{enumerate}
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}
\algrenewcomment[1]{\(\triangleright\) #1}
\begin{document}
\author{Jappie Klooster}
\title{Stay in shape! Homework exam II}
\maketitle
\section{A disk and some half-planes}
\subsection{Per case, what can you say about $H$}
We number the half-planes of $H$: $h_1, h_2,\dots,h_i$.
Let $H_{i}$ be the set of the first $i$ constraints, and
let $C_{i}$ be the feasible region defined by these constraints:

\[ H_i := \{h_1, h_2,\dots, h_i\}\]
\[ C_i := h_1 \cap h_2 \cap \dots \cap h_i\]

And so $C$ is $\cap H$.

\begin{enumerate}[(i)]
	\item \emph{There is no disk that has its center non the $x$ axis
			that lies inside all half-planes.}

		$C$ does not intersect the x axis. So its either an empty set or entirely above or below the x axis.

	\item \emph{There arbitrary large disk with the specifications.}

		$C$ has no parallel bounds to the x axis and
		at most one bounding line segment crossing the x axis.

	\item \emph{The largest disk exists and is unique.}

		$C$ bounding line segments cross the x-axis 2 times and
			the disk border is intersected at least 2 times by non x axis
			parallel bounding line segments, one line segment intersects the
			left part of the disk and another the right part.
			An alternative definition is not case 1,2 and 4.

	\item \emph{There are several equally large largest disks}

		$C$ produces an area with one or two bounding line segments
		parallel to the x-axis and these line segment intersect the border
		of the possible disks.

\end{enumerate}

\newpage
\subsection{Deal with case ii in linear worst case time}
\begin{algorithmic}[1]
	\Function{hasArbitrarylargeDisc}{$H$}
		\State
		$b_r, b_l \gets None$
		\For{$i \gets 1$ to $n$ with $h_i \in H$}

			\If{$h_i$ is parallel to the x-axis}
				\State
				\Return $False$
			\EndIf
			\State
			\Comment{Check where the x axes crossings are, and write it into
			a variable pair, if both pairs are filled its not case ii.}
			\If{$h_i$ points left}
			\State
				$b_r \gets h_i$
			\EndIf
			\If{$h_i$ points right}
			\State
				$b_l \gets h_i$
			\EndIf
			\If{$b_r$ not equal to $None$ and $b_l$ not equal to $None$}
				\State
				\Return $False$
			\EndIf
		\EndFor
		\State
		\Return $True$
	\EndFunction
\end{algorithmic}
\subsection{Formulate and proof the RIC lemma}
\paragraph{Lemma}
Let $1 \leq i \leq n$, let $C_i$ be the defined as above and let $d_i$ be
the largest disk in $C_i$. Then we have
\begin{enumerate}[(i)]
	\item If $d_{i-1} \subset h_i,$ $then$ $d_i = d_{i-1}$
	\item If $d_{i-1} \not\subset h_{i},$ $then$ $C_i$ does not intersect the x-axis
		or $b_i \cap l_i \neq \emptyset$ where $b_i$ is the boundary of $d_i$ and $l_i$
		is the line bounding $h_i$.
\end{enumerate}
\paragraph{Proof}
\begin{enumerate}[(i)]
	\item Let $d_{i-1} \subset h_i$. Because $C_i = C_{i-1}\cap h_i$ and
		$d_{i-1} \subset C_{i-1}$ this means that $d_{i-1} \subset C_i$.
		Furthermore,the optimal disk in $C_i$
		cannot be better than the optimal disk in $C_{i-1}$, since
		$C_i \subseteq C_{i-1}$. Hence, $d_{i-1}$ is the optimal disk
		in $C_i$ as well.
%TODO: replace with using a line between center(d_{i-1}) and
%		center(d_i) then reason about how d_i will keep growing
%		 until it hits h_i.
	\item Let $d_{i-1} \not\subset h_{i}$. Suppose for a contradiction that
		$C_i$ intersects the x-axis and that $b_i \cap l_i = \emptyset$.
		Consider the line segment $s$ between $center(d_i)$ and 
		$center(d_{i-1})$.
		Where $center$ is a function that returns the center vertex of a disk.
		We have $d_{i-1} \subset C_{i-1}$ and
		since $C_i \subseteq C_{i-1}$, also $d_i \subset C_{i-1}$.
		Together with the convexity of $C_{i-1}$, this implies that
		the line segment $s$ is contained within $C_{i-1}$.
		Since $d_{i-1}$ is the optimal disk in $C_{i-1}$ and the optimum is to
		maximize $r_i$ the radius of $d_i$. Consider $d_i$
		where $s$ is maximized, now to increase $r_i$ we move $d_i$ along $s$,
		we can keep doing this until $b_i \cap h_i$.
		So $d_i$ needs to intersect $h_i$ otherwise $d_i$ is not the optimum
		disk in $C_i$. This shows it cannot be the case that $C_i$ intersects
		the x-axis and $b_i\cap l_i = \emptyset$.

\end{enumerate}
\subsection{Give the RIC algorithm}

\begin{algorithmic}[1]
	\Function{calculateBiggestDisk}{$H$}
		\State
		$d_0 \gets arbitrarlyLargeDisk()$
		\If{hasArbitrarlylargeDisc($H$)}
			\State
			\Return $d_0$
		\EndIf
		\State
		$H_r \gets$ random shuffle $H$
		\For{$i\gets 1$ to $n$ with $h_i \in H_r$}
			\State
			$d_i \gets d_{i-1}$
			\If{$d_{i-1} \not\subset h_i$}
			\State
			$d_i \gets \Call{calculateBiggestDiskFrom}{h_i, \{h_0\dots,h_{i-1}\}}$
			\If{$d_i = None$}
				\State
				\Return $None$
			\EndIf
			\EndIf
		\EndFor
		\State
		\Return $d_n$
	\EndFunction
	\Function{calculateBiggestDiskFrom}{$h_{bound}$,$H_{prev}$}
		\State
		$d_0 \gets arbitrarlyLargeDisk()$
		\For{$i\gets 1$ to $n$ with $h_i \in H_{prev}$}
			\State
			$d_i \gets d_{i-1}$
			\If{$h_i$ does not point in the same direction as $h_{bound}$}
			\If{$d_{i-1} \not\subset h_i$}
				\If{$h_i \cap h_{bound}$ does not intersect the x-axis}
					\State
					\Return $None$
				\EndIf
				\If{$h_i$ is parallel to the x axis}
				\State
				%TODO, the calculation with math?
				$d_i \gets$ Calculate the leftmost largest disk in $h_{bound} \cap h_{i}$
				\Else
				\State
				$d_i \gets$ Calculate the largest disk in $h_{bound} \cap h_{i}$
				\EndIf
				\State
			\EndIf
			\EndIf
		\EndFor
		\State
		\Return $d_n$
	\EndFunction
\end{algorithmic}

\subsection{Prove the expected running time}
It should be noted that both $calculateBiggestDisk(H)$ and
$calculateBiggestDiskFrom(h,H)$ are both linear time functions.
And so worst case $calculateBiggestDisk(H)$ is worst case:

\[O(n^2)\]

We will prove that a$calculateBiggestDisk(H)$ will run in $\Theta(n)$ expected
time with help of backwards analyses: Consider the situation after $h_i$ is
inserted, and $d_i$ is computed (either by $d_i = d_{i-1}$ or with help
of $calculateBiggestDiskFrom(h_i,H)$). If $h_i$ does not bound the feasible
region, or $d_i \subset h_i$, then the addition step was cheap
and took $\Theta(1)$ time. But if $d_i \not\subset h_i$ then the addition
step was expensive.

There are $i$ half-planes that could have been defining $d_i$, and $i-2$
of these are in random order. Since the order was random each of the $i-2$
half-planes has the same probability to be the last one added, and only
$\leq$ 2 of these cause the expensive step.

Expected time for $i$-th addition:

\[\frac{i-4}{i-2}\cdot\Theta(1)+\frac{2}{i-2}\cdot\Theta(i)=\Theta(1)\]

Total expected running time:

\[3\Theta(n) = \Theta(n)\]

\subsection{Explain why you can't use RIC for points}
To create a RIC algorithm with linear expected time we need to have the
following properties, lets go trough each of one to see if they get violated.

\begin{itemize}
	\item \emph{The test of the next input object violates the current
		optimum must be possible and fast.}
		This is true for this problem. Checking if a point is inside a disk
		is trivially easy.
	\item \emph{If the next input object violates the current optimum,
		finding the new optimum must be an easier problem than the general
		problem.}
		This is the violating property. Because you can still move
		the disk having a point violate the current disk may not be a point
		which with the disks boundary has to intersect.

		A more crude way to put it is that you can have multiple optimum
		that don't intersect the same points.
	\item \emph{The optimum must already be defined by O(1) of the input
		objects.}
		I think this is true, you can use some math to figure out a new
		optimum.
	\item \emph{The analysis must work out.}
		It doesn't work out because you can't guarantee that you've just
		found a local optimum instead of a global optimum. I mean you
		could use this as a heuristic but not as an answer.
\end{itemize}
\section{A data structure for horizontal line segments}
\subsection{Analyze the storage requirements}
As can be read from lemma 10.2 (and the suceeding proof) from
the book \cite{book}, an interval tree uses $O(n)$ storage.

Now we have the outer tree that is a binary tree based on $y$ coordinates.
For each interval node $v$ we have a canonical subset $S_v \subseteq S$
of all line segments that appear in the subtree rooted at node $v$.
Note that $subtree(c_{left}(v))\cup subtree(c_{right}(v)) = subtree(v)$
where $subtree$ is a function that returns the subtree of $v$ and 
$c_{right}$ returns the right child and $c_{left}$ returns the left child.
Every level of the outer tree has an $n$ amount of items stored in its
subtrees, and there are $log(n)$ levels because no 2 line segments have the
same $y$ coordinate. This results in the following order of storage:

\[ O(n \cdot log(n))\]


\subsection{Argue that all horizontal line segments are found}
The outer tree will not start reporting until it encounters a
split node. Then it will only report the inner tree if the
value of the nodes fall in the range of the query. So for
the $[y:y']$ part of the query the algorithm is correct.

The interval tree for the $x$ part of the query will start
reporting as soon as it encounters a decision. However it
will only report the line segments that intersect with $x$.

The result also will not contain any duplicates, because
when something is reported to the right in the main tree
the algorithm will go to the left, so the next tree only
contains things that weren't in the original report.

\subsection{Give pseudo code for the query algorithm}
\begin{algorithmic}[1]
	\Function{select}{$v$, $x \times [y:y']$}
		\If{$v$ is a leaf and $y < y_{val}(v) < y'$}
			\State
			\Return \Call{selectRange}{$subtree(v), x$}
		\EndIf
		\If{$y' < y_{val}(v)$}
		\State
		\Return \Call{select}{$c_{left}(v), x \times [y:y']$}
		\EndIf
		\If{$y > y_{val}(v)$}
		\State
		\Return \Call{select}{$c_{right}(v), x \times [y:y']$}
		\EndIf
		\State
		\Comment{If we get here it means we have a split node}
		\State
		\Return $\Call{reportLeft}{v, x\times y, c_{right}}\cup
		\Call{reportRight}{v,x\times y', c_{left}}$
	\EndFunction
	\algstore{tree}
\end{algorithmic}
\newpage
\begin{algorithmic}[1]
	\algrestore{tree}
	\Function{reportRight}{$v$, $x \times y$}
		\State
		$\phi \gets y \leq y_{val}(v)$
		\If{$isLeaf(v)$}
			\If{$\phi$}
				\State
				\Return $\Call{selectRange}{subtree(v),x}$
			\EndIf
			\State
			\Return $\emptyset$
		\EndIf
		\If{$\phi$}
		\State
		\Return $\Call{reportRight}{c_{left}(v), x \times y} \cup
		\Call{selectRange}{subtree(c_{right}(v)),x}$
		\EndIf
		\State
		\Return $\Call{reportRight}{c_{right}(v), x \times y}$
	\EndFunction
	\Function{reportLeft}{$v$, $x \times y$}
		\State
		$\phi \gets y > y_{val}(v)$
		\If{$isLeaf(v)$}
			\If{$\phi$}
				\State
				\Return $\Call{selectRange}{subtree(v),x}$
			\EndIf
			\State
			\Return $\emptyset$
		\EndIf
		\If{$\phi$}
		\State
		\Return $\Call{reportLeft}{c_{right}(v), x \times y} \cup
		\Call{selectRange}{subtree(c_{left}(v)),x}$
		\EndIf
		\State
		\Return $\Call{reportLeft}{c_{left}(v), x \times y}$
	\EndFunction
	\Function{selectRange}{$v$, $x$}
	\If{$v$ is a leaf}
		\State
		\Return $\emptyset$
	\EndIf
	\If{$x < x_{mid}(v)$}
		\State
		$r \gets $ starting at the leftmost endpoint keep
		returning all the intervals in $l_{left}(v)$ until the
		intervals no longer contain $x$. (basically a takeWhile
		operation in your favorite language).
		\State
		\Return $r \cup \Call{selectRange}{c_{left}(v),x}$
	\EndIf
	\State
	$r \gets $ starting at the rightmost endpoint keep
	returning all the intervals in $l_{right}(v)$ until the
	intervals no longer contain $x$.
	\State
	\Return $r \cup \Call{selectRange}{c_{right}(v),x}$
	\EndFunction
\end{algorithmic}
\subsection{Analyze the query time}

We'll start by analysing the inner tree.
At any node $v$ we visit we spend $O(1+k_v)$ time where
$k_v$ is the number of line segments we report at $v$. Summing
over $k_v$ results in $k$. The depth of the tree is $log(n)$,
we visit at most one node at any depth of the tree. This results in:

\[ log(n)+k\]

So in the main tree we spend $O(log(n)+r_v)$ per call, where $r_v$ is
the result for node $v$. Its also worth noting that we visit each level
of the tree a maximum two times. Hence the total time we spend is:

\[ 2\cdot \sum_v(log(n)+r_v)\]

Notice that $\sum_v r_v = r$ and that the search path of $y$ and $y'$
in the main tree have depth $log(n)$. So $\sum_v O(log(n))=log^2(n)$.
Which results in:

\[ O(log^2(n)+r) \]

\subsection{Bonus}
Applying what we've learned above we know we can handle ranges by using
binary trees. For single values we can use interval trees.
So suppose we know that $X$ will be an interval. Then the main tree will
be a binary tree sorting on $x$ where each node has a two sub-binary trees $y$
and $z$.
When we start reporting for x we'll choose the associated subtree which
is the interval query.
If $Y$ is an interval then we choose the $y$ subtree.
In the $y$ tree nodes have associated structures that are interval trees
on $z$.
If $Z$ is the interval then we choose the $z$ subtree. In $z$ subtree
there is again an associated structure for each node which is an interval
tree on $y$.
We can create another structure for when $X$ is not an interval. Similar
to the one above so that we can handle any range query.

The storage requirement is:

\[O(n\cdot log^2(n))\]

Because the main tree is amplified additionally in size by an extra associated
structure depth.
The query time is:

\[ O(log^3(n)+r) \]

Because there are 3 trees to traverse and we still need to report the answers.
\section{Voronoi nearest neighbour\emph{s}}
\subsection{Storage}
The original data structure was already of order:

\[O(n)\]

Storage, this was a given, now we will start analysing how the extension
will impact this order.
For every point $p$ we store a trapezoidal map, which is $O(n)$ size
storage (Theorem 6.3 of the book \cite{book}) where $n$ equals the amount of
Voronoi neighbours $k_p$. So per point we now have a storage of the order:

\[O(1+k_p)\]

However we can put an expectation on $k_p$ by using theorem 7.3 of the book
\cite{book}. It says the number of edges is at most $3n-6$, an edge is a
division between neighbours so the expected amount of neighbours per point
is $\frac{3n-6}{n} \approx 3$. So the storage complexity is:

\[ O(3n) = O(n) \]

Note that this is not an expected complexity because the theorem 7.3 puts
a hard upper bound on the amount of neighbours as long as $n\geq3$.
\subsection{Query time}
The original data structure had an expected query time of the following order:

\[O(log(n))\]

This was given. Now we will analyse if the extension will impact this
performance. The substructure is a trapezoidal map which has an expected
query time of $O(log(k))$ where $k$ is the amount of neighbours for a
particular point. If we combine this with the existing structure
we get an expected query time of:

\[O(log(n) \cdot log(k)) \]

\begin{thebibliography}{9}
\bibitem{book}
de Berg et al,
\textit{Computational geometry and applications }.
3rd edition, 2008. Springer-Verlag Berlin Heidelberg.
\end{thebibliography}
\end{document}
