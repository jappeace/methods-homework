\abstract{
  %TODO abstract
  \todo[inline]{abstract missing}
}
\todo{be more critical? maybe it depends to much on its paremeters?
  Or which game types are good for saticifing play?
}
\todo{misses callouts, intermezzi and quotes}
\section{Introduction}
\toReview
Satisficing play was originally introduced
to try and limit the computation power that earlier models required of human
behavior \citep{herbert}. Originating from the field of economics, they
wanted to use a model that was not ``super intelligent''.

\toReview The classical models from the field of economics had at least one of the
following assumptions:
\begin{itemize}
\itemsep0em 
  \item The structure of the game is known.
  \item Awareness of other player's decisions.
  \item Awareness of being in a game situation
  \item Being able to preform maximization calculations.
\end{itemize}

\toReview These assumptions can be called common knowledge, and are a demanding to
make \citep{binmore}. 

\todo{give paper overview}

\section{Core algorithm}
\toReview The core algorithm as presented by \citep{herbert} and formalized by
\citep{karandikar},
in here we present the simplified version of \citep{stimpson:2001} first:

\begin{enumerate}
  \item $ \begin{cases}
              \text{if } \pi_t \geq \alpha_t & A_{t+1} = A_t\\
              \text{else} & A_{t+1} \neq A_t
              \end{cases}$
  \item $ \alpha_{t+1} = \lambda \alpha_t + (1-\lambda) \pi_t $ \\
\end{enumerate}
\todo{add table caption}
\begin{tabular}{ll}
  Definition & symbol \\ \toprule
  Time & $t$ \\
  Action & $A_t$ \\
  Utility & $\pi_t$ \\
  Aspiration & $\alpha_t$ \\
  Learning rate & $\lambda$ \\ \bottomrule
\end{tabular}

\toReview In the first phase the next action is selected. If the action still holds
according to the satisficing level or ambition it will remain the same, if not another
action will be selected. In the second phase the satisficing level is updated.
In here the constant learning rate is used. This is a variable that can be
modified to adjust the amount of new information being used or how much
information from memory is used.


\section{Satisficing play in action}
\todo{make an intermezzo of this?}
\drafting
To demonstrate a potential use of this algorithm we will use at several example
cases. The first one being the iterated prisoner's dilemma. Consider
the following normal form game:

\todo{add table caption}
\begin{tabular}{lll}
            & Cooperate   & Defect \\
  Cooperate & $(2,2)$     & $(0,3)$  \\
  Defect    & $(3,0)$     & $(1,1)$  \\
\end{tabular}

\drafting The Nash equilibrium of this game lies on Defect,Defect. However, all but
that action are pareto optimal actions. So the Nash equilibrium would be the
``right'' strategy to play if the game is only played once, but if you repeat
the game against the same opponent, interestingly enough you can ``solve''
the prisoner's dilemma by ``teaching'' the opponent to cooperate.
\todo{can one really solve? Is it really teaching? should either add references
or change wording}

\drafting However strategies such as tit for tat or win stay lose shift already know
which actions are winning and which are punishing to try and solve it.
Satisficing play learns this however by updating the aspiration it got from
playing a move. If we consider a game versus always cooperate for example:

\begin{tabular}{lllll}
    t & Cooperate & $A_t$ & $\pi_t$ & $\alpha_t$ \\
    0 & C     & C    & 2       & 3 \\
    1 & C     & D    & 3       & 2.5 \\
    2 & C     & D    & 3       & 2.75 \\
\end{tabular}

\drafting In $t=0$ satisficing play doesn't meet its ambition of 4, therefore, in the
next game it modifies its ambition and changes its action. Here we can see
that satisficing play does get more utility than its ambition, and is satisfied
with its action.
But now if we start with a lower apsiration level:

\begin{tabular}{lllll}
  t & Coop & $A_t$ & $\pi_t$ & $\alpha_t$ \\
  0 & C     & C    & 2       & 1 \\
  1 & C     & C    & 2       & 1.5 \\
  2 & C     & C    & 2       & 1.75 \\
\end{tabular}

\drafting So here satisficing play finds a Pareto optimal action, but it doesn't find
the best possible action it could get against this opponent. From this example
we can also see how dependent satisficing play is on its starting parameters.
\todo{here we can interject an intermezzo, of how karandikar tried to solve
this with trajection and random perturbations}

\drafting Now consider a game versus tit for tat (TT), Tit for Tat is a
strategy that always copies the last action of its opponent.
This game can also be found in the \citep{stimpson:2001} paper:

\begin{tabular}{lllll}
        t & TT & $A_t$ & $\pi_t$ & $\alpha_t$ \\
        0 & C     & C    & 2       & 3  \\
        1 & C     & D    & 3       & 2.5 \\
        2 & D     & D    & 1       & 2.75 \\
        3 & D     & C    & 0       & 1.87 \\
        4 & C     & D    & 3       & 2.435 \\
        5 & D     & D    & 1       & 1.715 \\
        6 & D     & C    & 0       & 0.85875 \\
\end{tabular}

In this table we can see satisficing play and tit for tat ending up in a cycle.
at $t=0$ satisficing gained to little reward for its ambition level $\alpha_t$,
and therefore switches action in the next round. In $t=1$ saticificing play
is satisfied, but int $t=2$ it no longer is so it switches but this even ends
up being worse at $t=3$. After that the cycle resets at $t=4$, the cycle becomes
most obvious when looking at the $\pi_t$ column.
\section{Properties of satisficing play}
\drafting

Using both \citep{karandikar} and \citep{stimpson:2001} works to look deeper
at the properties of satisficing play. 

karandikar used a theoretical proofing method, whereas Stimpson showed some
experimental results.

also showing the contrast graph which supports the work of \citep{arthur}

\section{An extended satisficing play}
\drafting

In here we discuss the work of \citep{stimpson:2003}, where he extends the
algorithm to be played with mutliple action choices.
Stimpson created a game where resources had to be allocated among agents.

\section{Satisficing compared to other learning methods}
\drafting

With help of the work of \citep{crandall} saticificing play is compared
to Q-learning and WOLF-PHC.
Q-learning is explained briefly with help of \citep{sandholm} and WOLF-PHC
with help of \citep{bowling}.
 
Finally some graphs are shown how they compare

\todo{add caption for tables?}
\bibliography{main}

\address{Jappie Klooster\\
  Dept. of Informatics\\
  Universiteit Utrecht\\
  The Netherlands\\}
\email{j.t.klooster@students.uu.nl}
