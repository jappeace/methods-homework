#+TITLE: Rusty reinforcement learning

* Introduction
This document describes an analyses of different reinforcement learning techniques, and
their effects on multi-agent environments.
To complete the multi-agent learning course, it was requested to create such a document.

First the situation will be described, then an implementation to study this situation will
be described, after that the results and observations will be presented from which finally a
conclusion will be drawn.

* Situation description
To study multi-agent reinforcement we'll create a world in the shape of a torus.
In here different entities roam about called skaters. They roam at constant speed
and have a constant collision radius. Every game tick the skaters choose a direction
from a set of predefined directions.

If a skater collides with another skater they won't move and receive a reward. If
they don't collide they will receive a different reward.

** The parameters
$N \in \mathbb{N}$ is the amount of skaters.
$w$ is the width and $h$ height of the surface of the torus. Where $w,h \in \mathbb{N}$
\[
  A=\{0,60,120,180,240,300\}
\]
$A$ is the set of possible actions which is uniform for all skaters. $r > 0$ is the
collision radius. $\delta > 0$ is the speed at which the skater move.
$R_1$ is the reward received when avoiding collision. $R_2$ is the reward received
when colliding (and not moving).

** Task description
The task is to implement this situation in a programming language of choice. Then
suitable values for the parameters have to be decided such that interesting behavior
emerges.

*** Questions
Is  it always the case that $N$ skaters converge to one angle, or is it almost surely
the case, or is this not the case at all?
In any case proof it.

* Approach
First the situation was implemented[fn:: Source can be found here:
https://github.com/jappeace/methods-homework/blob/master/multiAgentLearning/practical/code/src/main.rs]
in Rust.
This language was chosen because the author wanted to learn it. There are many advantages to
Rust described elsewhere[fn:: Rust advantages: https://www.rust-lang.org/].
The use assignment was small enough to try and use an unknown language for implementation. 

First the situation was implemented. Meaning that all variables were created and the
representation of the Skaters. Then a simple updating mechanism was created.
After which the first learning strategy was created, the mixed strategy. This one
was created first because it was believed to be the easiest one to implement.

After that the graph creation was done. This was done with help of Gnuplot[fn::
Gnuplot http://www.gnuplot.info/] and its respective rust library[fn::
Gnuplot rust library: https://github.com/SiegeLord/RustGnuplot].

When the graph creation was done experimentation the real experimentation started.
First the egreedy and greedy strategies were also implemented for comparison.
egreedy means in this case greedy plus exploration, and greedy is just choose
always the best last reward.
However the learn rate was still regressive, meaning there was less learning done
over time, so this was replaced with a constant learn rate, and learn rate itself
became a separate configuration option.

Finally importing the graphs into the org[fn:: Emacs org mode was used to export to latex, source can be found here:
https://github.com/jappeace/methods-homework/blob/master/multiAgentLearning/practical/code/src/report.org]
format proved somewhat problematic,
since the output of the plot library was just a set of instructions for gnuplot.
But a simple shell script solved that:

#+begin_src shell
     for file in `ls *.plot`;
     do gnuplot -e "set terminal pngcairo size 800,400 enhanced font"\
                "'Verdana,10'; set output '"$file".png" $file;
     done
#+end_src

This created png files of the plots.

After the deadline was extended, the decision was made to automate the process of writing plots
from specialized configurations. Previously some global variables had to be set
to let the program create a specific plot, but with the new design certain run
configurations could be pre-created and executed and create all plots.

The design was also put into separate files (called modules), this
made the code a lot more manageable.

it was also decided to put each run configuration on a separate thread.
Rust is made with a-synchronus programming in mind, so doing this was somewhat obligatory.
This final modification allows the creation of graphs of many separate configurations without
it costing to much time, although now the luxury problem arose of having to many plots to fit
in the report (54). I decided to handpick some and write about them.

** A closer look at the parameters
Before talking about which graphs I decided to show I want to clear up some initial
observations I had about the parameters.

*** Speed
$N$, the amount of skaters, $w,h$ the width and height and $r$ the collision radius
of each skaters will all have similar effects on the skater behavior. These parameters
all influence the available movement space. For example if we increase $w$, the end
result will be that the skaters will have more space to move around and thus less
chance to collide with other skaters. So it would be pointless to try and create separate
graphs for all these parameters since in essence they do the same. So I decided just
modify $N$ and leave the others be.

| Variable | Value     |
|----------+-----------|
| $N$      | Specified |
| $w$      | 10        |
| $h$      | 10        |
| $r$      | 0.5       |

*** Speed
$\delta$, the movement speed, doesn't modify space however. So I did decide to create graphs
for different $\delta$. The speed will be specified per graph.

*** Action set
It would be interested to modify and compare $A$, the set of available actions,
but the current code base doesn't support modifying that. Therefore all $A$ is the same.
\[
  A=\{0,60,120,180,240,300\}
\]

*** Rewards
$R_1$ avoiding collision reward and $R_2$ collision reward are also compared,
the reason being, that the natural intuition would be
to try and assign values in which the skaters try to avoid each other. But, you
could also try and make them collide with each other by flipping these
"intuitive" values.
By default the rewards will be the following:

| Variable | Value |
|----------+-------|
| $R_1$     |    20 |
| $R_2$     |   -20 |

*** Learning method
In the implementation I differentiate between two concepts, first the
method of action selection, secondly how rewards are learned.
For action selection 3 different strategies are implemented:

1. Mixed
2. Greedy
3. Explore greedy

Greedy will be used unless mentioned differently. The implemented learn
strategies are:

1. Regressive learning (learn less over time).
2. Constant learning.

Constant learning will be used unless mentioned otherwise.

*** Run count
The program is limited by the amount of simulation steps done. It was
decided to do only 3500 because this creates graphs that fit nicely on
the paper without trouble.

#+LATEX: \newpage
* Results and observations

** The most uninteresting behavior
To have a baseline for interesting behavior, first a graph is shown which is as
boring as I could possibly generated:

#+CAPTION: Baseline interesting
[[./results/greedy-c10-s0.2-rewards.plot.png]]

| Variable | Value |
|----------+-------|
| $N$      |    10 |
| $\delta$      |   0.2 |

So in this graph the skaters have *lots* of space, almost any action results
in the max payoff, and because they're moving slow collisions are almost unheard of.

#+LATEX: \newpage
** Traffic jam
Skipping a few of those graphs we reach the first interesting one, in here you can
see all the skaters having really high payoff for a while on all actions, but
then they get stuck.

#+CAPTION: Something weird
[[./results/greedy-c20-s0.5-rewards.plot.png]]

| Variable | Value |
|----------+-------|
| $N$      |    20 |
| $\delta$      |   0.5 |

What happens here is that at first everyone has enough space,
because they're distributed uniformly across the space. After a
while some collisions start to happen causing some skaters to stop moving
for a while and they change their behavior as a result of it. By doing
so they may cause more collisions (by moving in the "wrong direction"),
and this causes massive uncertainty, and also suffering because they end up dropping
below 0 for the rest of the run.

#+LATEX: \newpage
** Coercion
There is to little space, but after a while the skaters come to a certain (bullied)
agreement:

#+CAPTION: Coercion
[[./results/greedy-c30-s2-rewards.plot.png]]

| Variable | Value |
|----------+-------|
| $N$      |    30 |
| $\delta$      |     2 |

Here you can see what happens if the space is very crowded but the skaters are
moving fast compared to their body sizes (moving 4 times their size).

#+LATEX: \newpage
** Suffering
Its full. Not enough room to go anywhere, the skaters are bumping in each other
all the time no matter which direction they take:

#+CAPTION: There is not enough space
[[./results/greedy-c40-s0.5-rewards.plot.png]]

| Variable | Value |
|----------+-------|
| $N$      |    40 |
| $\delta$      |   0.5 |

This situation is somewhat similar to traffic jam, except there was no moment
of happiness at all. Oddly enough at the speed of $\delta$ 0.2 it is still possible
to get this moment off happiness.

#+LATEX: \newpage
** Inverse
For these I flipped the rewards, thus:

| Variable | Value |
|----------+-------|
| $R_1$     |   -20 |
| $R_2      |    20 |

The short story is that they all end up standing still colliding with each other.

*** Suffering baseline
So they get rewards for colliding, therefore having a sparsely populated
space doesn't give much (if any) reward.

#+CAPTION: Baseline inverse
[[./results/greedy-inverse-c10-s0.5-rewards.plot.png]]

| Variable | Value |
|----------+-------|
| $N$      |    10 |
| $\delta$      |   0.5 |


#+LATEX: \newpage
*** Convergence?
This looks the most like convergence I've seen:

#+CAPTION: Inverse convergence
[[./results/greedy-inverse-c20-s0.2-rewards.plot.png]]

| Variable | Value |
|----------+-------|
| $N$      |    20 |
| $\delta$      |   0.2 |

Basically they roam around for a while and then meet all the sudden.
Being greedy they decide to milk that action and stand still forever.
Its not complete convergence, since some other directions are also still
used (and have to be because someone has to come from the other side to
make sure they keep colliding).

** Other strategies
Here the other strategies will be discussed although not as thorough.
For both its the case that:

| Variable | Value |
|----------+-------|
| $N$      |    30 |
| $\delta$      |   0.5 |

The only things that change are the learn strategy and action select strategy.
The reason for not being so thorough is that the random element of both mixed
and egreedy makes it hard to say what exactly is going on.

*** Egreedy
Exploration greedy with $\epsilon = 0.1$.

#+Caption: Constant learning rate
[[./results/egreedy-rewards.plot.png]]

#+CAPTION: Regressive learning rate
[[./results/egreedy-reg-rewards.plot.png]]

#+LATEX: \newpage
*** Mixed strategy

#+CAPTION: Constant learning rate
[[./results/mixed-rewards.plot.png]]

#+CAPTION: Regressive learning rate
[[./results/mixed-reg-rewards.plot.png]]
* Conclusion
I think its almost surely not the case that skaters converge towards one angle.
The reason for this is that convergence towards one angle is an incredibly unstable
state.
As soon as one skater stop for whatever reason and someone behind him bumps into him
all progress towards the convergence is over.

You can see for example in the baseline, where *any* action gives high reward, this is
opposite of convergence. But even if you add more skaters there is just a chance
they'll tumble into an abyss, as we've seen in the traffic jam example.

Coercion looks somewhat more like convergence, but even this isn't exactly that since they
other directions are still used.

Then if we flip the rewards things change, and it really looks like we get convergence,
but there is a problem for the greedy algorithm, someone has to go against  the flow and
stop all other skaters so that nobody moves. So its basically impossible to get convergence
because at least one skater has to go against the rest to stop everyone from moving. So
convergence here is theoretically impossible (and all the graphs I produced show the same,
there is always another direction being used).

Finally there are the mixed and egreedy strategies, but these won't have convergence by definition.
Because they have their inherit random element.

To get convergence one would need to pick the exactly the right variables, *and* the right
strategy *and* run the simulation probably for a *long* time, and even then its not guaranteed.
