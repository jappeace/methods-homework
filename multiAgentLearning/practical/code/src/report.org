#+TITLE: Rusty reinforcement learning

* Introduction
This document describes an analyses of different reinforcement learning techniques, and
their effects on multi-agent environments.
To complete the multi-agent learning course, it was requested to create such a document.

First the situation will be described, then an implementation to study this situation will
be described, after that the results and observations will be presented from which finally a
conclusion will be drawn.

* Situation description
To study multi-agent reinforcement we'll create a world in the shape of a torus.
In here different entities roam about called skaters. They roam at constant speed
and have a constant collision radius. Every game tick the skaters choose a direction
from a set of predefined directions.

If a skater collides with another skater they won't move and receive a reward. If
they don't collide they will receive a different reward.

** The parameters
$N \in \mathbb{N}$ is the amount of skaters.
$w$ is the width and $h$ height of the surface of the torus. Where $w,h \in \mathbb{N}$
\[
  A=\{0,60,120,180,240,300\}
\]
$A$ is the set of possible actions which is uniform for all skaters. $r > 0$ is the
collision radius. $\delta > 0$ is the speed at which the skater move.
$R_1$ is the reward received when avoiding collision. $R_2$ is the reward received
when colliding (and not moving).

** Task description
The task is to implement this situation in a programming language of choice. Then
suitable values for the parameters have to be decided such that interesting behavior
emerges.

*** Questions
Is  it always the case that $N$ skaters converge to one angle, or is it almost surely
the case, or is this not the case at all?
In any case proof it.

* Approach
First the situation was implemented[fn:: Source can be found here:
https://github.com/jappeace/methods-homework/blob/master/multiAgentLearning/practical/code/src/main.rs]
in Rust.
This language was chosen because the author wanted to learn it. There are many advantages to
Rust described elsewhere[fn:: Rust advantages: https://www.rust-lang.org/].
The use assignment was small enough to try and use an unknown language for implementation. 

First the situation was implemented. Meaning that all variables were created and the
representation of the Skaters. Then a simple updating mechanism was created.
After which the first learning strategy was created, the mixed strategy. This one
was created first because it was believed to be the easiest one to implement.

After that the graph creation was done. This was done with help of Gnuplot[fn::
Gnuplot http://www.gnuplot.info/] and its respective rust library[fn::
Gnuplot rust library: https://github.com/SiegeLord/RustGnuplot].

When the graph creation was done experimentation the real experimentation started.
First the egreedy and greedy strategies were also implemented for comparison.
egreedy means in this case greedy plus exploration, and greedy is just choose
always the best last reward.
However the learn rate was still regressive, meaning there was less learning done
over time, so this was replaced with a constant learn rate, and learn rate itself
became a separate configuration option.

Finally importing the graphs into the org[fn:: Emacs org mode was used to export to latex, source can be found here:
https://github.com/jappeace/methods-homework/blob/master/multiAgentLearning/practical/code/src/report.org]
format proved somewhat problematic,
since the output of the plot library was just a set of instructions for gnuplot.
But a simple shell script solved that:

#+begin_src shell
     for file in `ls *.plot`;
     do gnuplot -e "set terminal pngcairo size 800,400 enhanced font"\
                "'Verdana,10'; set output '"$file".png" $file;
     done
#+end_src

This created png files of the plots.

After the deadline was extended, the decision was made to automate the process of writing plots
from specialized configurations. Previously some global variables had to be set
to let the program create a specific plot, but with the new design certain run
configurations could be pre-created and executed and create all plots.

The design was also put into separate files (called modules), this
made the code a lot more manageable.

it was also decided to put each run configuration on a separate thread.
Rust is made with a-synchronus programming in mind, so doing this was somewhat  obligotary.
This final modification allows the creation of graphs of many separate configurations without
it costing to much time, although now the luxury problem arose of having to many plots to fit
in the report (54). I decided to handpick some and write about them.

** A closer look at the parameters
Before talking about which graphs I decided to show I want to clear up some initial
observations I had about the parameters.

*** Speed
$N$, the amount of skaters, $w,h$ the width and height and $r$ the collision radius
of each skaters will all have similar effects on the skater behavior. These parameters
all influence the available movement space. For example if we increase $w$, the end
result will be that the skaters will have more space to move around and thus less
chance to collide with other skaters. So it would be pointless to try and create separate
graphs for all these parameters since in essence they do the same. So I decided just
modify $N$ and leave the others be.

| Variable | Value     |
|----------+-----------|
| $N$      | Specified |
| $w$      | 10        |
| $h$      | 10        |
| $r$      | 0.5       |

*** Speed
$\delta$, the movement speed, doesn't modify space however. So I did decide to create graphs
for different $\delta$. The speed will be specified per graph.

*** Action set
It would be interested to modify and compare $A$, the set of available actions,
but the current code base doesn't support modifying that. Therefore all $A$ is the same.
\[
  A=\{0,60,120,180,240,300\}
\]

*** Rewards
$R_1$ avoiding collision reward and $R_2$ collision reward are also compared,
the reason being, that the natural intuition would be
to try and assign values in which the skaters try to avoid each other. But, you
could also try and make them collide with each other by flipping these
"intuitive" values.
By default the rewards will be the following:

| Variable | Value |
|----------+-------|
| $R_1$     |    20 |
| $R_2$     |   -20 |

*** Learning method
In the implementation I differentiate between two concepts, first the
method of action selection, secondly how rewards are learned.
For action selection 3 different strategies are implemented:

1. Mixed
2. Greedy
3. Explore greedy

Greedy will be used unless mentioned differently. The implemented learn
strategies are:

1. Regressive learning (learn less over time).
2. Constant learning.

Constant learning will be used unless mentioned otherwise.
* Results and observations

** The most boring behavior
To have a baseline for interesting behavior, first a graph is shown which is as
boring as I could possibly generated,

#+CAPTION: Baseline interesting
[[./results/greedy-c10-s0.2-rewards.plot.png]]

| Variable | Value |
|----------+-------|
| $N$      |    10 |
| $\delta$      |   0.2 |

So in this graph the skaters have *lots* of space, almost any action results
in the max payoff, and because they're moving slow collisions are almost unheard of.

* Conclusion
