#+TITLE: Summarizing arguments
#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage[margin=1.5in]{geometry}

This paper considers a scenario with two agents that
(1) work in the same domain using a shared ontology,
(2) are capable of learning from examples, and
(3) communicate using an argumentative framework.

Traditionally, a learning agent can predict the solution of new problems using
knowledge learnt from past experience.
Existing argumentation frameworks for multiagent systems are based on deductive logic.
However, logicbased argumentation frameworks assume agents with predetermined knowledge
and preference relation.
This paper, focuses on the argumentation framework where both knowledge and
preference relation are learned from experience.
Moreover, learning agents allow the design of techniques that use learned
experience to generate adequate arguments and counterarguments.

A Multi-Agent Case Based Reasoning System (MAC)
$M = \{(A_1 , C_1 ), ..., (A_n , C_n )\}$
is a multi-agent system composed of $A = \{A_i , ..., A_n \}$,
a set of CBR agents, where each agent $A_i \in A$ possesses an individual case
base $Ci$. Each agent is autonomous and the case base is private.

The framework focuses on analytical tasks where solving a problem means
selecting a class from a set of enumerated solution classes $\mathbb{S}=
\{S_i,..., S_k\}$. A case $c=\langle P,S \rangle$ is atupple containing a case description
$P$ and solution class $S \in \mathbb{S}$.

If a CBR method has retrieved a set of cases $C_1,..., C_n$
to solve a particular problem P the justification built will contain
the relevant information from the problem P that made the
CBR system retrieve that particular set of cases.

The meaning of a justification is that most of the cases in the case base
of an agent that satisfy the justification belong to a predicted solution class.
When an agent provides a justification for a prediction, the agent generates
a justification field $J = \langle A,P,S,D \rangle$ where agent $A$ considers $S$ the
correct solution for $P$ and the prediction is justified by a description $D$
such that $J.D \subset J.P$. Justifications are used as arguments in order
to allow learning agents to engage in argumentation processes.

An /argument/ $\alpha$ generated by an agent $A$ is composed of a statement $S$
and some evidence $D$ supporting $S$ as correct.
In the context of MAC systems, agents argue about the correct solution of
new problems and can provide two kinds of information:
a) specific cases $\langle P, S \rangle$, and b) justified predictions: $\langle A, P, S, D\rangle$.
Using this information, and having in mind that agents will only argue about
the correct solution of a given problem,
we can define three types of arguments: justified predictions,
counterarguments, and counterexamples.

A /justified prediction/ $\alpha$ is generated by an agent $A_i$ to argue that
$A_i$ believes that the correct solution for a given problem $P$ is $\alpha.S$,
and the evidence provided is the justification $\alpha.D$.

A /counterargument/ $\beta$ is an argument offered in opposition to another argument
$\alpha$.
A counterargument consists of a justified prediction $\langle A_j , P, S', D' \rangle$
generated by an agent $A_j$ with the intention to rebut an argument
$\alpha$ generated by another agent $A_i$, that endorses a solution class different
from that of $\alpha$ for the problem at hand and justifies this with justification
$D'$.

A /counterexample/ $c$ for an argument $\alpha$ is a case $c$ that contradicts $\alpha$.
specifically: $\alpha.D \subseteq c$ and $\alpha.S \neq c.S$.

The /confidence/ of justified predictions is assessed by the agents via examination
of justifications. The more endorsing cases the more confidence, and the more
counterexamples the lower the confidence.
If two agents want to asses the confidence on a justified prediction $\alpha$ made 
by one of them, they both send their confidence to each other.
The preference relation used in our framework is the following one:
a justified prediction $\alpha$ is preferred over another one $\beta$ if $C(\alpha) \geq C(\beta)$,
where $C$ is the function that returns the confidence value.

Any learning method able to provide a justifed prediction can be used to generate
arguments. In this paper LID was used.
To generate counter arguments a policy based on the /specificity criterion/ is used.
Meaning that the most specific argument prevails since they are more informed.
Thus, counterarguments generated based on the specificity criterion are
expected to be preferable.

LID generates a description by starting at an empty term and heuristically
adding features to that term. Thus at every step the description becomes more
specific. So when the description only covers a single solution class, LID 
terminates.

To generate a counterargument, LID just has to use as a starting point the 
description $\alpha.D$ instead of the empty term, where $\alpha$ is the argument.
In this way, the justification provided by LID will always be subsumed
by $\alpha.D$, and thus the resulting counterargument will be more specific than $\alpha$.

Initially, each agent makes its individual prediction.
Then, the confidence of each prediction is assessed,
and the prediction with the highest confidence is considered the winner.
However, if the agent that has provided the prediction with lower confidence
doesnâ€™t agree, it has the opportunity to provide a counterargument.
Agents keep exchanging arguments and counterarguments until they reach an agreement
or until no agent is able to generate more counterarguments. At the end of the
argumentation, if the agents have not reached an agreement, the prediction with
the highest confidence is considered the joint prediction.
