#+TITLE: No regret learning
#+OPTIONS: toc:nil 

Lets say you played a strategy with a payoff of $3$ for $11$ rounds.
Now if you look back at the game and see a strategy $s$ that pays $5$,
The /regret/ of $s$ is $2/11$.
If there is another strategy $b$ with a regret of $3/11$, we can argue
for future games that we should put more emphasis on playing $b$.
If we do this systematically we can drive the average regrets to zero,
without considering the opponent's strategy.

In a game against nature where we assume Nature's actions are observable the
average payoff trough time $t$ is:

\[ \bar{u}^t=(1/t)\sum_{1\leq s \leq t} u(x^s,y^s)\]

Each action $x \in X$, the average /regret/ from not having played $x$ is:

\[\bar{r}^t_x=(1/t)\sum_{1\leq s\leq t}u(x,y^s)-\bar{u}^t\]
The regrets can also be expressed as a vector $\bar{r}^t=(\bar{r}^t_1 ,\dots,\bar{r}^t_k) \in R^k$, where $\bar{r}^t_x$
is the regret from not having played action x. There is no regret for play $\omega$
if:

\[\forall x \limsup\limits_{x\rightarrow\infty} \bar{r}^t_x(\omega) \leq 0\]

One of the most simplest rules with this property is the following: In each
period $t+1$, the decider plays each action with a probability proportional
to the nonnegative part of his regret up trough time t:

\[ q^{t+1}_x=[\bar{r}^t_x]_+/\sum_{x'\in X}[\bar{r}^t_{x'}]_+ \]
(If the denominator is zero the choice $q^{t+1}\in\Delta(X)$ is arbitrary), this is called
/regret matching/. In a finite game $G$, regret matching by a given player almost
surely yields no regret against every possible sequence of play by the opponents.

To cast regret matching into a rule that is similar to a reinforcement rule:
The /aspiration level/ is the average payoff over the first $t$ periods: $\bar{u}^t$.
The /reinforcement increment/ in period $t$ is the vector $r^t$, where
$r^t_x=u(x,y^t)-\bar{u}^t$ for each $x \in X$. Lastly, the vector of propensities at time
$t+1$ is the non-negative part of the sum of the previous increments:
\[\theta^{t+1}=[\sum_{1\leq s \leq t} r^s]_+ \]

