#+TITLE: A serious communication game with personalities
#+LANGUAGE: en
# TODO: this title is to vague
# Jungian functions as endomorphisms:
  # Personality for dialogue agents
# Pure functional serious communication in-game
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper, drafting]

#+Options: toc:nil ^:nil 
#+Options: title:nil
#+OPTIONS: H:5

#+LATEX_HEADER: \usepackage[square,sort,comma,numbers]{natbib}
#+LATEX_HEADER: \renewcommand{\bibsection}{}

#+LATEX_HEADER: \usepackage[obeyFinal, colorinlistoftodos]{todonotes}
#+LATEX_HEADER: \newcommand{\drafting}{\todo[noline, color=gray]{Working draft}}
#+LATEX_HEADER: \newcommand{\toReview}{\todo[noline, color=yellow]{To review}}
#+LATEX_HEADER: \newcommand{\newlyCleared}{\todo[noline, backgroundcolor=white, bordercolor=red]{Newly cleared}}
# (something cleared that was under discussion last time)
#+LATEX_HEADER: \newcommand{\cleared}{\todo[noline, color=white]{Cleared}}

# Title page
#+LATEX: \input{title}

# The order of this thesis will be done in a way to let future researcher
# decide the value of the thesis quickly
# 1. First the abstract to let a researcher quickly discard this thesis if neccesary.
# 2. The toc, to let a researcher jump to interseting pages quickly.
# 3. The introduction and main body of the thesis. If all else fails a
# reaserhcer can use this as fallback

# smaller code font size (cause mostly boring xml)
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\scriptsize}

** Abstract                                                          
:PROPERTIES:
:UNNUMBERED: t
:END:

#+BEGIN_CENTER

\todo[inline]{abstract}

#+END_CENTER
\todo{keywords}
\todo[inline]{Figures should to be able to be placed in the appendix}
\todo[inline]{Manually add minted source code highligting wherever possibe}

\newpage
#+TOC: headlines 2

\newpage

* Introduction
\cleared
Communication is the foundation of our modern society.
Having good communication skills can help individuals in both professional as
well as in their personal lives.
However training people in communication skills can be difficult.
Another party is required to communicate with,
and a tutor or teacher has to be there to give feedback.
Serious games can be used to train people with these kind of skills
cite:swartout2013virtual.
Therefore a general "communicate!" game was developed.
Wherein teachers can create scenarios to let their students practice with
communication cite:jeuring2015demo.

\cleared
This game was script based, the teacher made a scenario and the student would
follow the choices predefined by the teacher.
A script based game has of course the weakness that a student can't use
creative responses,
all possible responses are scripted into the scenario
and replying correctly relies on a simple /abc/ choice.
To combat this an alternative serious game was made based on the
chatbot Alice cite:weiderveld2016chatbot.
To measure performance a rule engine was used, and to limit the domain
to that of a doctor's appointment social practices were used.
In this thesis we are interested in extending this work with personalities,
where we consider personalities to be a preference for a process rather than
content cite:campos_mabs2009.
This is useful because it turns out that the issues most doctors struggle
with isn't so much being sensitive,
but rather being sensitive to the people who appreciate it. cite:clack2004personality
Therefore extending the game to allow doctors to train for different
personalities will help addressing this issue.

\cleared
Since the personality topic has become quite popular in recent years
some thoughts will be devoted to related work.
After that,
to validate our work according to personality research
we'll look at existing personality theories and their advantages and disadvantages,
keeping practical considerations in mind.
Then we'll have a look at idiomatic AI programming and how in theory personality
could be combined with AI.
We continue by taking a close look at the state of the existing software that
we're planning to extend.
After this we'll discuss the implementation.
Finally we'll compare the different implementations.

** Related work
<<Related work>>
\cleared
  To simulate personality in communication games there have been already several
works proposed.
Etheredge used the OCEAN personality theory to create argumentative
agents cite:etheredge2016personality.
Although argumentation is not the same as communication we can consider the
method used to make the personality.
In this paper a personality model is introduced based on OCEAN.
To move from personality values in OCEAN towards action selection fuzzy logic
is used.
However this has a major disadvantage in that a lot of rules need to be
added to do action selection.
This can make action selection opaque.
It is for example not immediately clear how a higher anxiety will influence
action selection.
Having a lot of rule also make maintenance hard, if for example there is an
unwanted behavior many rules need to be inspected before the change can be made.

\cleared
In agents with with personalities cocu tried making a complete game based
on MBTI cite:cocu2015agents.
However communication was never implemented and personality got reduced to
doing a single action.

\cleared
Van den bosch also chose to use OCEAN to model characters in a serious
communication game cite:van2012characters.
He used a nested probabilistic if else structure to decide on how agents should
interact.
His methodologies had some shortcomings however,
for example a not agreeable person was defined as someone who'd had a high
probability of telling facts about himself.
Which in certain situations could be considered strange,
for example a spy who was captured.
This kind of methodology is called content orientated cite:campos_mabs2009,
the personality, would and should change depending on context.

\cleared
Campos used the MBTI to create BDI based agents cite:campos_mabs2009.
We will use a more fine grained version of MBTI but his architecture is used,
in which personality will be processed orientated rather than content
orientated.

* Background
\cleared
In this chapter we will discuss the work that is the foundation of this thesis.
First we will look at personality theories developed by psychology.
Then we will look at some to the literature in AI and argue for the
methodologies used,
and finally we will look at the serious game in its existing form.

** Personality theories
 <<personality theories>>
   \cleared
   A personality is a set of identifiers that can be used with
   reasonable consistency to predict behavior
   cite:mischel2008introductionp3_definition.
   What we want from this model is a guideline of implementation for the program,
   that is to say,
   the more the theory says about internal workings of a person the better it is.
   We want it to model to be realistic of course,
   but we also want it to be implementable.
   This is where we have a conflict of interest with the field of
   psychology since they do not necessarily care about implementation details.

   \cleared
   This conflict of interest can be seen for example in
   cite:mischel2008introductionp4_defpoints, where a criteria of personality is
   that it should be stable and coherent. However this is a poor
   software specification since there is no unit of measurement
   (how long should it be stable, and what range is acceptably stable),
   but for psychology it is a good definition, because a human can determine out
   of context what these things are.

   \cleared
   The field of psychology has been somewhat active in trying to model human
   personality cite:pervin2008handbook. 
   Several frameworks have been developed to figure out people's
   personality and what this in turn would mean for their lives.
   We are interested in two ways in existing personality theories:
   1. Accuracy, if a personality thoery does not fit the reality at all it won't
       help anyone in the serious game.
   2. Ease of implementation. If the personality theory is too hard (or impossible)
       to implement in the serious game than we can't use it.
   The field of psychology is very interested in the first requirement. 
   However the second requirement not so much.
   Therefore our first job will be to list existing psychology personality
   frameworks,
   and filter out those that are unfeasible to implement.


*** The big five
  <<OCEAN>>
 \cleared
 The first framework we'll discuss is called the big five.
 The term big five first coined in 1981 by Goldberg cite:goldberg1981language.
 The big five were not big because of their intrinsic greatness,
 but rather to emphasize how broad these factors were.

 \cleared
 This framework was not really invented, but rather discovered trough
 lexical analyses by for example Tupes cite:tupes1961recurrent.
 Although the labels used were different,
 they conveyed the same idea as the big five model used now.
 The methodology used is something which is called factor analyses[fn::
 In the paper the term 'varimax rotational program' is used,
 but if we look this term in wikipedia, we can see the result is called factor
 analyses cite:varymaxrotanonalprogram].
 Factor analyses is a statistical methodology that tries to find underlying
 hidden variables.
 This methodology has become widely used in psychology cite:fabrigar1999evaluating.

 \cleared
 The data Tupes used is from Catell cite:cattell1947confirmation and several
 others. Catell used a rating scheme,
 where a trait was introduced and all test subjects then had to rate all other
 test subjects else as average, below or above average for that specific trait.
 Persons could also use one extreme rating per trait for one person.
 These traits in the test were based on the /personality sphere/ concept which
 tried to cover the entire surface of personality by providing many small trait
 areas.
 Examples of the traits are: "Attention getting vs Self sufficient", or
 "Assertive vs Submissive".

 \cleared
 In the begining of the 1990's there were many ways to measure personality that
 didn't agree with each other.
 For example at Berkley alone block used a 2 dimensional ego-resilience and
 ego-control method cite:block1980role,
 whereas Gough measured folk concepts such as self-control, well-being and
 tolerance cite:gough1987california.
 Personality researchers hoped that they would be the one to discover a structure
 that would then be adopted by other researchers cite:pervin2008handbookp114.

 \cleared
 The goal of the big five was not to present a new structure that convinced
 others to use it,
 but rather to provide a taxonomy that all psychologist could agree upon.
 Since the big five was so broad (because of the statistical methods used),
 this worked.
 Therefore the researchers could keep on exploring there niche with their
 proffered structure,
 but once they would present their work they could use the big five to
 communicate clearly what their research meant without having to redefining the
 words every time cite:pervin2008handbookp114..116.

 \cleared
 The big five as in the OCEAN definition
 has the following units of measurement:
 - Openness or originality, if you score high on this you enjoy learning new
   things just for the sake of learning. If you score low then you don't enjoy
   this
 - Conciseness, how tidy you are, if you score high the dishes don't stack up
   in the sink.
 - Extroversion, a high score indicates you enjoy leading the conversation and
   you'll speak up when you disagree with someone.
 - Agreeableness or altruism, a low score would indicate that you don't want to
   share and generally don't trust people.
 - Neuroticism or nervousness, a high score indicates that you like to brag and
   get upset when someone is angry at them.

 \cleared
 The big five has been extensively tested and the result has been replicated
 in multiple studies cite:pervin2008handbookp119.
 One can measure his big five score trough a test called the NEO-PI, or the
 NEO-FFI. The FFI variant is shorter but less precise cite:costa1992revised.

 \cleared
 Although these terms may provide a great taxonomy,
 it does not have any theoretical foundation cite:eysenck1992four.
 This means it becomes difficult to speak about implementation.
 To make this more clear we use a thought experiment:
 Lets say you have a score of 0.8 for Neuroticism,
 how does this influence my decision for selecting action $a$ or $b$?
 Now you could say, use a mixed strategy where in you choose 80% of the time
 the neurotic typical neurotic approach.
 Then we need a valuation function to decide which of the two actions is more
 neurotic.
 But once we've done this we still haven't taken into account any of the
 other factors.
 Solving this is a non-trivial endeavor.

 \cleared
 There are some existing solutions in which OCEAN is implemented, for
 example allbeck used it as a mapping to the EMOTE system cite:allbeck2002toward,
 whereas cite:durupinar2008creating used the OCEAN values as a low level mapping
 in steering behaviors
 and finally cite:etheredge2016personality used the values for action selection
 in a dialogue, but extended the descriptions of OCEAN with IPIP
 with an entire chapter devoted to explaining this.
 Although these implementation are based on the same OCEAN model,
 the influence of it has starkley different effects on their
 respective implementations.
 Since each of them decided to change the OCEAN model in some kind of way
 we can conclude that although OCEAN is good for discussing the psyche,
 it is incomplete for a software specification role. 
 
*** Personality types
 <<sec:types>>
 \cleared
 To address the big five's issue of having no thoeretical foundation we'll
 have a look into personality types.
 We begin with the theoretical foundation proposed by the grandfather of
 personality research, Carl Jung.
 After which we'll look at a thoeretical evolution proposed by Myers and
 Myers-Brigs, which also introduced a structered method of measuring types.
 Then we'll discuss some critique on this method.
 With this critisim in mind we'll look at alternatives to the MBTI that have been
 proposed afterwards.

**** Jung's theory of psychological types
<<Jungian types>>
 \cleared
 Jung describes several concepts, firstly each person has two attitudes:
 /Introversion/ and /extroversion/.
 Extroversion means dealing with the outside world and therfore is called
 objective (or observable).
 Intoversion is the world inside a person, and therefore is subjective,
 or private.
 This privacy however may bo so great that the consiouness can't even access it.
 These attitudes are mutually exclusive,
 you can't do introversion and extroversion at the same time.
 For example if you're day dreaming you're not paying attention to your
 surroundings.
 A person who spends most of his time in the introversion attitude is called
 an /introvert/.
 But he is not totally the one or the other, ie an introvert can still have
 extravert moments and vice versa.
 It should also be noted that the unconsciousness according to Jung is
 flipped in attitude. cite:hall1973primer97-98attitude

 \cleared
 Then there are four functions.
 The first two functions are called the /rational functions/
 because they act as a method of making judgements.
 /Thinking/ is a function that connects ideas with each other to arrive at
 generalizations or conclusions. 
 /Feeling/ evaluates ideas by determinging if its good or bad, pleasant
 or unpleasant, beautifull or ugly.
 Note that this is /not/ the same as being emotional,
 although you can be emotional and use this function.
 The /irrational functions/ are called this because they require no reason.
 /Sensation/ is sense perception created by the stimulation of the senses,
 it can always be rooted to a sense,
 such as "I see a balloon" or "I feel hungry".
 /Intuition/ is like a sensetion but its not produced by a sense.
 Therefore it has no origin in the same way as sensation has,
 by which its explained as "just a hunch" or "I feel it in my bones".
 cite:beauchamp2005communication,hall1973primer98-100functions

 \cleared
 To use these functions they have to be combined with attitudes, producing
 /function attitudes/.
 Therefore a person will never be of a thinking type,
 but rather either a thinking introvert or thinking extrovert.
 cite:hall1973primer100-101combo
 We can now imagine what this means,
 an extroverted thinker will for example make judgement about the real world,
 and therefore be more like a natural scientist or biology researcher,
 where they would study natural objects and behaviors.
 An introverted thinker will make judgement about ideas in his mind,
 and therefore will be an excellent philosopher, or mathematician, where
 consistency of the internal reasoning process is important.

 \cleared
 Let $\mathcal{J}$ denote the set of all possible jungian function attitudes
 such that:
 \[ \mathcal{J} = \{ T_e, T_i, F_e, F_i, S_e, S_i, N_e, N_i\}\]
 Where
 + $T_e$ stands for extroverted thinking, which is thinking about objects in the
   real world. This is thinking with a goal, a problem to solve,
   to check weather certain laws are upheld, or a system to check.
   As said before a typical example of $T_e$ based reasoning would be a
   biologist studying natural behavior.
 + $T_i$ stands for introverted thinking,
   this kind of thinking could be called deductive,
   it tries to construct a framework to explain the world.
   This is consistent reasoning based on internal believes,
   which does not necessarily solve a problem.
   A typical example of $T_i$ based reasoning is a mathematician creating or
   combining new mathematical structures with help of axiomatic logic.
 + $F_e$ stands for extroverted feeling, where objective or external criteria
   is used to judge, for example something is beautifull or ugly.
   Established standards may be used to decide this and therefore its a
   conservative function.
   Decisions are based on interpersonal and cultural values.
   A typical example of $F_e$ based reasoning is about fashion and fads.
   Deciding what is fashionable at the moment is an $F_e$ based process.
   A typical profession would be working at a clothes shop,
   where the knowledge of the latest trends is crucial.
 + $F_i$ stands for introverted feeling, decisions based on personal values and
   believes.
   People who have this as dominant function attitude could be characterized by
   "still waters run deep".
   A typical profession for this type is in counseling or health care, because
   empathy comes rather natural to them cite:fiproffesionadvice.
 + $S_e$ stands for extroverted sensing, Act on concrete data from the here and
   now. Then lets it go.
   People of this type are often realistic and practical.
   A typical profession driver of heavy machinery or athlete cite:seproffesionadvice, 
   because living in the moment is most important for those professions,
   this comes natural to $S_e$ based personalities.
 + $S_i$ stands for introverted sensing, acts on concrete data from memories and
   passed experience.
   A possible profession for the people with $S_i$ as dominant function is in
   quality assurance,
   where the perfect model in their mind can be easily
   compared to the product in question cite:siproffesionadvice.
 + $N_e$ stands for extroverted intuition, try to find possibilities in every
   situation.
   Extroverted intuition can be very good entrepreneurs, seeing ideas in
   almost every situation,
   this also makes them very inspiring leaders because
   they are very excited about their ideas cite:neproffesionadvice.
 + $N_i$ stands for introverted intuition. Looks new possibilities in ideas.
   A typical occupation of this type is artist or visionary
   cite:hall1973primer104nitype,
   this is because connecting ideas with each other comes natural to this type.
   However just like the typical artist it may not always be understood why by
   his peers or even himself.

 \cleared
 <<Jungian alternating functions>>
 Another important concept is the idea of the /principal/ and /auxiliary/
 function cite:hall1973primer105principal.
 The principal function is the one that is most preferred.
 The auxiliary renders its services to the principal function,
 however this function cannot be the opposite of the principal.
 So if /Feeling/ is the principal function than thinking connot be the auxiliary.
 This is also true for the irrational functions.

**** MBTI
 \cleared
 The meyer brigs type indicator is based upon Carl Jung's theory of personality
 types.
 However it brings two important changes, first of all the way
 of measuring personality type is changed. 
 It uses a strutured approach rather than Carl Jungs projective approach.
 The responses to items are finite and therefore can be deduced based on theory.
 In contrast to Jung's technique where he used open ended answering with word
 associations cite:hall1973primer23method.
 Then there is the introduction of an extra index used to order function
 attitudes cite:carlson1985recent.
 Which is either a $J$ for judging (rational in jung terms)
 or a $P$ for perceiving (irrational in jung terms).
 This dimension indicates together with the $I/E$ dimension which function
 attitude is dominant and which is auxiliary.

 \cleared
 <<sec:mbti:order_comparison>>
 Once completed with the MBTI you'll get charatcter string as outcome,
 for example "INTJ".
 This label tells you inderectly which of carl jung's functions is dominant,
 auxiliary, tetriary and inferior cite:mccaulley2000myers.
 In other words it provides a sequence of preferences
 cite:website.mbtitypedynamics.
 In case of INTJ it would be:
 \[N_i > T_e  > F_i > S_e\]
 So the most preferred function to be used by someone of type INTJ would be $N_i$,
 then $T_e$ and so forth.
 These are the same function as Jung used, the MBTI
 just imposed an order on them cite:mccaulley2000myers,website.mbtisequence.
 How much preference there is for a function is not encoded in MBTI, just an
 order of preference.
 An ENTJ would be simialar to INTJ but with a different order:
 \[T_e > N_i > S_e > F_i\]
 With this definition the interplay of the judging/perceiving dimension becomes
 more obvious if we look at INTP: \[T_i > N_e > S_i > F_e\]
 It's similar to an ENTJ, but the attitudes have flipped.

 \cleared
 A possible grouping of the sixteen type exists using the middle letters:
 \[\{NT, ST, NF, SF\}\]
 This grouping goes under the rationale that the first two functions only
 differ in either attitude, order or both.

 \cleared
 Before continuing we would like to say a word about a popular
 interpertation of MBTI which is based on Kersey's book "Please understand me",
 and later "Please understand me II".
 In this interpertation the sixteen types are also placed in general groups
 of four but here the $ST$ and $SF$ distinction is replaced by $SJ$ and $SP$
 cite:keirsey1998please.
 It turns out however that Kersey invented this distinction because
 "He thought it made sense to group them this way" cite:whyaretypesdistinct.
 In doing this he rejected the work of Jung and also that of cognitive functions.
 Which is problematic because the theory he presented then does not make any
 thoeretical sense.
 Therefore Kersey's MBTI will not be used in this thesis.

 \cleared
 The MBTI is extremly popular in a subfield called Organizational Developement
 (OD) cite:sample2004myers. 
 But it has gotton some heavy critism in from the field of psycology.

 \cleared
 MBTI has always used a continues scoring system in the results.
 However the creators insist that type is enough for making assessment judgments.
 Since MBTI reduces the test scores to type,
 it is expected that most of the population would fall into either proposed
 dimensions.
 For example $I$ or $E$.
 This is called a bimodal distribution.
 However cite:bess2002bimodal suggests this is not the case,
 but this could be the result of the scores being biderictional
 cite:salter2005two.
 In an extended investigation cite:arnau2003jungian into weather jungian
 constructs are truly categorical suggested however that this was maybe not
 the case and a continues scale for assessment judgements are required.

 \cleared
 In cite:sipps1985item the MBTI is put trough a method called factor analyses.
 This is the same technique where OCEAN is based upon (see section [[OCEAN]]).
 With this technique the desired outcome is that there are 4 question clusters
 (or factors), one for each dimension.
 These factors should also be independent,
 a question that influences I/E score should not influence S/N.
 Finally we expect the factors to indicate differences between individuals.
 Random questions won't do that.
 However the study indicated that the MBTI had more than 4 factors (6),
 cite:sipps1985item explains the first extra factor as questions that assessed
 people being "unconditional positive",
 but could not explain the other extra factor.
 Something else of note worth cite:sipps1985item indicated was that there
 were questions doing no discrimination at all (not being scored). 

 \cleared
 Reliability indicates how often the same result will come out of the test,
 for example if you take the mbti a 100 times you may be classified the same
 type for 70 times,
 which would be an indication it has a reliability of arround 70%.
 But in psycology another aspect is important,
 namely the interval in between which the tests are taken,
 if for example two tests produce starkly different results but a long time
 has passed between them its not considered a big issue.
 In cite:pittenger1993measuring it is suggested that after a period of 5 weeks 50%
 of the participants changed in score.
 However one should take into consideration that after taking the test a first time 
 people could consciously decide to change their opinion because they think its
 more desirable to have a different type.
 Jung said that type is decided very early on in life cite:hall1973primer106inborn
 so having reliable scoring is important.

**** PPSDQ
 \cleared
 The PPSDQ keeps basically the same theory as MBTI cite:kier1997new,king1999score,
 but uses a different measuring method.
 Instead of forced questions it uses a word-pair checklist for
 $I/E, S/N$ and $T/F$, and for the $J/P$ self describing sentences are used
 cite:melancon1996measurement.
 An example of a word pair checklist can be found in table [[tab:word-pair-example]].
 The word pairs themselves were obtained by prescribing an exploratory test(s) to a
 sample in which the proto PPSDQ was submitted and also the MBTI itself, factor
 analyses was used to determine correlation, this is done in
 cite:thompson1994concurrent.
 The optimal amount of points (options to choose from)
 presented in such a test is a subject for debate.
 Common sense would suggest that more points would give more precision,
 but in cite:matell1971there it is suggested that reliability and validity
 do not increase with more points. In cite:garland1991mid however they
 state the importance of an available midpoint.
 The 5 point choice format in the PPSDQ is not motivated.
 
#+CAPTION: An example of a word pair checklist, where the test taker should choose the  word that he identifies most with
#+NAME: tab:word-pair-example
 | Word          |   |   |   |   |   | Word      |
 |---------------+---+---+---+---+---+-----------|
 | Empathy       | 1 | 2 | 3 | 4 | 5 | Logic     |
 | Dispassionate | 1 | 2 | 3 | 4 | 5 | Emotional |

 \cleared
 The result of the PPSDQ would look something like: I-30 N-20 T-80 J-60, with
 a scale of 0 to 100. To calulate the jungian functions as a probability measure
 some math is required. Our subject is $70\%$ of the time introverted and $30\%$ of the 
 time extroverted. $60\%$ of the time judging and $40\%$ of the time perceiving.
 therefore N_i would be calulated as: 0.7 \times 0.4 \times 0.8 = 0.224 or $22.4\%$.
 N_e would be $0.3 \times 0.4 \times 0.8 = 0.096$ etc.
 From this you can make a preference sequence or create a mixed strategy.

 \cleared
 The PPSDQ is measuring the same thing as MBTI but lacks the critisms of MBTI.
 The reliability is for example between 90% to 95% with a delay of two weeks.
 The internal consistency was also measured which proved to be better than
 MBTI but there was still a dependency between S/N and P/J which remains
 unexplained cite:kier1997new.
 The PPSDQ is internally most consistent of the discussed alternatives
 (excluding OCEAN) cite:arnau1999alternative.

**** SL-TDI
 \cleared
 SL-TDI measures functions by presenting 20 situations and then giving subject
 possible actions which corrolate with the functions.
 The subjects then have to indicate how likely it is that they would choose that
 particular action cite:arnau2000reliability.

 \cleared
 It becomes rather staight forward to make a function preference of the 
 measurement of SL-TDI since the qeustion directly measure the jungian
 functions.
 A possible personality type therefore would be:
 \[ S_i \geq T_i \geq S_e \geq F_e \geq N_i \geq T_e \geq N_e \geq F_i \]
 To determine the preference we just used the observed value in the test.
 Since every situation offers a choice for each function with a 5 point value
 there is no need for normalization.

 \cleared
 This denotion is much less strict than the MBTI or PPSDQ since it does not force
 alternating attitudes or pairing of rational/irrational functions in the
 preference.
 Therefore the amount of personality types SL-TDI supports drastically exceeds
 that of the PPSDQ. In other words, there always exists a mapping from PPSDQ
 to SL-TDI, but not always from SL-TDI to PPSDQ.
 The reason for doing this is because there is experimental evidence
 that there exist personalities outside of the stucture orignally imposed by
 MBTI and the subsequent PPSDQ cite:loomis1980testing.

 
*** Comparison of theories
 \cleared
 To re-iterate, we are interested in a framework that is realistic, and easy to
 implement.
 The Big Five falls short on the easy to implement,
 there is no underlying theoretical framework to support it cite:eysenck1992four,
 therefore we cannot base our implementation on anything except our own
 interpertation.

 \cleared
 The MBTI has been criticized a lot from the field of psychology,
 but it does have a solid theoretical foundation.
 There is some relation between the big five and MBTI cite:furnham1996big.
 Therefore its somewhat realistic, but quite easy to implement.

 \cleared
 Both of the alternatives of MBTI use a continues scale and have a high
 correlation with the big five cite:arnau1997measurement.
 This means is that they are measuring something which is also measured by the
 big five in some way.

 \cleared
 The PPSDQ is based on the same thoery as MBTI, but with scaled type letters.
 To convert the type to function attitudes some extra work has to be done,
 namely calulate their respective probabilities.
 To decide which function attitude to use some kind of mixed strategy
 has to be used.
 The PPSDQ is more realistic, but at the cost of being more difficult to
 implement.

 \cleared
 The SL-TDI is even harder to implement than the PPSDQ because the function
 attitudes no longer have to alternate.
 This either means that functions are independent (thereby rejecting some of Jung's work),
 or that they have to work in some kind of combination.
 If they work in some kind of combination and we have to following preference:
 \[ T_e > T_i > S_i > N_i > F_e > N_e > S_e > F_i\]
 We select the first function to work with, but it requires some information now,
 what to do?
 Select $S_i$, thereby skipping $T_i$, or select $T_i$ and let it decide to
 select $S_i$, but this would basially give $T_i$ censorship rights.
 This is difficult to answer therfore it is a lot more difficult to implement
 than PPSDQ.
 Since SL-TDI drops an assumption, which is shown with experimental evidence
 to be false cite:loomis1980testing, we can say SL-TDI's theory is most realistic.
 This comes however at the cost of being even more difficult to implement.

 \cleared
 Therefore our preference for implementation is the following:
 \[ \text{MBTI} > \text{PPSDQ} > \text{SL-TDI} > \text{OCEAN} \]

 \cleared
 There is another hidden reasoning behind this, the work of PPSDQ can built on
 that of MBTI, and that of SL-TDI can build on that of PPSDQ.
 OCEAN builds on nothing, we'll leave that for future work.

** Artificial intelligence literature
 # How do I measure that the persnoality created is in fact in complience with
 # the personality I aimed for?
 # Can I let the personality take the test somehow?
 \cleared
 In this section we will look at some of the AI based literature.
 For example we will have a look at the intelligent agent approach and the BDI
 architecture.
 We will also look at some theoretical attempts at implementing personality.
 Theoretical attempts are often a logic in contrast to the topics
 discussed in [[Related work]] which include executable implementations.

*** Agents background
 \cleared
 In the literature there is little consensus on what exactly an agent is,
 however there is a general consensus that an agent is /autonomous/
 cite:wooldridge2009introduction.
 To make this more clear we'll use Wooldridges' definition:

 #+BEGIN_QUOTE
 An /agent/ is a computer system that is /situated/ in some /environment/ and
 that is capable of /autonomous action/ in this environment in order to meet its
 delegated objectives. -- Wooldridge
 #+END_QUOTE

 \cleared
 In another older definition cite:wooldridge1995intelligent Wooldridge highlights
 /autonomy/, /social ability/, /reactivity/, and /pro activity/.
 Where autonomy means that no human intervention is required,
 social ability means it can talk to other agents,
 reactivity is that it can reply on input and pro activity means that it can
 show behavior while not reacting to something.
 However he later continues on with a stronger claim about an agent is a
 piece of software that uses concepts which are attributed to humans.
 Such as believes desires and intentions.

 \cleared
 This is the reason why we can't call any program an agent.
 For example an operating system kernel is
 autonomous (a user would never interact with it),
 social (can do networking),
 reactive (it will comply to hardware interprets for example)
 and proactive (a process hogging to much memory will be killed without the
 process asking for it).
 However we won't call a kernel an agent because it doesn't even come close to
 having believes, desires or intentions.

 \cleared
 Something to keep in mind is that there are three "branches" of agent research
 cite:wooldridge1995intelligent.
 The first one is /agent theory/ in which /specifications/ and methods of 
 specifications are developed. They ask what are agents and what are they
 ought to do and how do we tell them that.
 Then there are the /agent architectures/, these address questions of how
 to implement the specifications written by the theorists.
 In this paper we won't discuss architectures since we work in an existing
 system described in section [[The serious game]].
 Finally there are the /agent languages/, which ask the question how to write
 agent programs.
 Again this is mostly preditermined for us, but we will give a small overview.

**** Belief desires and intentions
 \cleared
 The belief desire intention model of human practical reasoning was first
 introduced by bratman cite:bratman1987intention.
 It is based upon a "common sense" framework of human reasoning.

 \cleared
 The idea of BDI is that an agent has believes, these can be anything, such as
 I believe the grass is green, or I believe the keys are on the table.
 Note that we never speak about facts, an agent can believe something to be a
 fact, but that doesn't make it a fact.
 Desires are special kind of believes that give agents a reason to be, they
 may also be called goals.
 Intentions are (partial) plans to make a desire come to fruition.
 How to formalize this properly turns out to be a hard question, which is
 analyzed in the following section [[bdi logics]].

 \cleared
 A number of reasons have been stated to use this methodology.
 The foremost is to make agent orientated systems less expensive in maintenance,
 verification and construction according to Rao and Georgeff cite:rao1995bdi. 
 However they don't cite a source for this.

 \cleared
 Another paper argues in favour of agent orientated design cite:jennings2001agent.
 It has the following major arguments:
 It is effective to divide a complex problem domain into several smaller problems,
 abstracting in an agent orientated way is more "natural",
 and complex systems dependencies and interactions can be easily modeled.
 # A case study is presented as proof of these claims.

**** Logic of BDI
 <<bdi logics>>
 \cleared
 \todo[inline]{We can delete this paragraph if we don't have any connections with these things. We may use it by saying for example that beliefs have certain kind of modalities. We probably aren't gonna use it to prove theoroms for example}
 Logic of BDI is an attempt to formalize how agents behave.
 One of the first formalization of Bratman's theory was that of Cohen and
 Levesque cite:cohen1990intention. It was based on linear time logic and
 used operators for actions and modalities for goals and beliefs cite:meyer2014logics.
 It was also used a tiered formalism, with at the bottom belief goals and
 actions which provided the basis for the higher achievement and persistent goals
 and intentions to do and be.
 Rao and Georgeff introduced a different formalism that used branching time logic. 
 They use modal operators for belief desires and intentions and then put 
 constraints on them to make interactions meaning full cite:meyer2014logics.
 Therefore this formalism is much closer to that of bratman cite:rao1991modeling.
 Finally there is the KARO formalism which is based on dynamic logic.
 This is the logic of actions and computation. They extend this logic with
 epistemics to add believes to it cite:meyer2014logics.

**** Concrete implementation
2apl, jade cite:braubach2003jadex.
 \todo[inline]{Comment on adding this was: If the work becomes agent like, and we have some very clear commanalities between 2apl for example we can fill in this paragraph to fall back onto that}
 \todo[inline]{Perhaps it would be a good idea to sketch commanalities between drools and these kind of things, so that if a reader knows about any of these things he can lay the link easily}
*** Social practices
 <<social practice>>

 \todo[inline]{(extra) possible sources: Weber, Durkleim, Hobermas, latour/reckwitz}
 \todo[inline]{ difference between norms and sp is that sp has no moral value, but you *violate* norms }
 \cleared
 In cite:smolka2001social it is stated that the research in activity theory
 led to the development of social practices.
 It was Karl Marx who made the roots who thought of the "roots" of activity
 theory cite:engestrom1999perspectivesp3_marx,
 Activity theory tries to bridge the gap between a single actor and the system
 it resides in cite:engestrom1999perspectivesp10_broad_definition
 trough the activity in progress.
 Another way of describing activity in this sense is "a way of doing things".
 A problem with this model however was, how do cultures move activities from the
 collective towards the individual cite:smolka2001social.
 Social practices were therefore introduced to make the notion of activity more
 concrete.

 \cleared
 An early adoption of social practice can be found in cite:shove2005consumers,
 where it was used to analyze the spread of Nordic Walking.
 In his analyses he uses the following overarching concepts to analyze the practice:
 1. /Material/, which is just stuff in the real world. Such as cars, lamps etc.
 2. /Meanings/, which covers issues that are relevant to the material and/or the
    practice. Think of health, price or even emotions. Consider the an example
    practice of cycling.
    In cite:shove2005consumers meanings and images is used interchangeably,
    however in cite:holtz2014generating its labeled as just meanings.
    For clarity we will be using the word /Meanings/ since its more descriptive.
 3. /Competence/, it is rather obvious to say you need to be able to cycle to
    participate in the social practice of cycling. This is why this is
    introduced.

\cleared
In cite:dignum2014contextualized a model of social practices for agents was
developed.
This model is extended specifically to allow software agents to use it.
In this model /physical context/ describes the physical environment,
it contains resources, places and actors.
Note that resources is equivelant to material from the model used by
cite:shove2005consumers,holtz2014generating.
/Social context/ contains a social interpretation, roles and norms.
In the previous model this was all part of /Meanings/.
/Activities/ are the normal activities in the social practice,
in nordic waling this can be for example talking with your partner,
or stopping to get a stone out of your shoe.
They don't need to all be performed but are there just as options.
This is the first construct that wasn't covered by the other model.
/Plan patterns/ are blue prints for the eventual plan.
An example of a doctor appointment plan pattern can be seen in
figure [[fig:sp-activity]].
If you go to the doctor the first thing you do is some kind of greeting.
Then the doctor goes onto data gathering and diagnoses mode until he figured
out what's wrong.
After which he will tell in the finishing phase what to do about it.
Now what these phases entail is not clear at all.
Finishing may for example contain the prescription of medicine,
or an appointment to go to the hospital. 
However plan patterns do not describe such an implementation.
The plan pattern construct wasn't represented in the previous model either.
/Meaning/ in this model is soley related to the social effects of activities,
and finally /Competences/ is the same as in the previous model.

 \cleared
 The interest for this model comes from  the potential heuristic use of social
 practices.
 Once in a particular situation that fits for a social practice the amount of
 reasoning can be sped up by having actions and their preconditions be grouped
 under that social practice,
 if no preconditions match an agent could consider trying other social practices
 he knows, or ask its peers for more information.

 \cleared
 The social practice theory in this thesis should be considered as a
 /foundation/ rather than a separate element.
 We will be using it as a domain limiting device,
 however it should be noted that it could act as much more than that,
 potentially it could give the notion of culture to agents.
 In this thesis we are interested in implementing personality for a serious game
 in a single social practice.
 So right now the social practice just gives an ordered overview in what domain our program
 should work.
 We can formulate the social practice that is relevant for this thesis 
 in the following manner:

 + Practice name: Doctor appointment
 + /Physical context/,
   - Resources: Computer, chair, diagnostic tools..
   - Places: waiting room, doctor's office...
   - Actors: doctor, patient, assistant, ...
 + /Social context/,
   - Roles: Doctor, Patient...
   - Norms: doctor is polite, patient is polite, doctor is inquisitive
   - Social interpretation: Can sit on chair, cannot sit on table.
 + /Activities/, share information, do diagnostics, minor treatments,
   prescribing drugs...
 + /Plan patterns/, see figure [[fig:sp-activity]].
 + /Social meaning/, awkwardness, gratitude, ...
 + /Competences/, Give injection, empathetic talk

#+NAME: fig:sp-activity
#+BEGIN_SRC plantuml :cache yes :file img/uml/sp-activity.png :exports results
start
:greeting;
while (has diagnoses?)
fork
:data gathering;
fork again;
:diagnoses;
end fork
endwhile
:finishing;
stop
#+END_SRC
#+CAPTION: Plan pattern example
#+LABEL: fig:sp-activity
#+RESULTS[35679ceefcd43b1884cc8c502d27ae59aaa34043]: fig:sp-activity
[[file:img/uml/sp-activity.png]]


  \cleared
  We can imagine personality should have /a/ influence on social practice
  selection and of course plan influence. As far as the authors are aware however,
  there hasn't been any prior work on this subject. But we can speculate for
  example that when considering physical context someone that is domination by a
  Sensing function would check all artifacts more rigorously than someone
  dominated by an Intuition.

  \cleared
  If the social practices are defined more formally they could be 
  used in a bigger system such as in cite:augello2015social and
  cite:augello2016model.
*** Speech act theory
<<Speech act thoery>>
\cleared
Since a large part of this thesis is about communication we will give here a
brief overview of speech act theory.
There are three levels at which speech acts can be analyzed according to
cite:shoham2008multiagent_speechact_p241..245.
/Locutionary/ acts simply convey information form the speaker to the listener.
All speech acts do this, as long as they carry meaning.
/Illocutionary/ acts are the speech acts that do something by saying it.
It captures the intend of the speaker. This includes giving orders or uttering a
warning.
/Perlocutionary/ acts are the acts that bring an effect to the hearer, such as
scaring or saddening.

\cleared
There are some basic assumptions of conversation, commonly described as the
/rules of conversation/ developed by Grice cite:shoham2008multiagent_speechact_p241..245.
Humans communication happens on the assumption that both parties want to be
clear to each other, even when other motivations apply.
This is called the /cooperation principle/.
To accomplish this share goal the Grice's maxims cite:gricemaxims are
used:
/Quantity/ has to do with the amount of information transferred in a single
utterance, a human wants to transfer just enough to get the right meaning across.
/Quality/ is the assumption where people will say things they believe to
be true.
/Relation/ states that the things uttered should be relevant to the subject
being discussed.
/Manner/ is about being as brief and clear as possible while avoiding ambiguity
and being orderly.

*** Dialogue systems
<<Dialogue systems>>
\cleared
Dialogue systems are the systems that try to analyze how dialogue works.
This is a subfield of AI that tries to combinate linguistics with computer
science.

\cleared
First of all are of course the chat bot systems, which are based upon case based
reasoning. A good example of this is the A.L.I.C.E. bot cite:wallace2001dont.
These are mostly reactive systems that use pattern matching rules paired with
"good" responses,
sometimes with conditions to allow for more variety.
A more extended example of such a system is eliza bot which is described in
cite:galvao2004persona,
where they also added personality to the bot with the OCEAN model.

\cleared
Traum cite:traum2003information describes the information state approach for
dialogues. 
The approach traum proposes is modeling:
+ Informal components, which aren't part
  of the model but are just there. This can include domain knowledge for example.
+ Formal representations, which are data structures.
+ Dialogue moves, which entail the set of possible utterances to make.
+ Update rules, that allow or prohibit the taking of certain moves.
+ Update strategy, to decide what rules to apply at a particular point.
The dialogue is the information state itself cite:walterapproaches.
This is an extremely general way of describing a dialogue system.

\cleared
In cite:wobcke2005bdi a BDI based methodology is proposed to handle dialogue
between a user and an agent.
However we want to point out that this solution fits into the rough model traum
sketched. So we could say its a information state approach too.

*** BDI + Personality
<<BDI + Personality>>
 \cleared
 \todo[inline]{other proposed systems and argue for campos}
 Campos discussed an architecture in which personality emerged not from things
 you like,
 but rather than trying to determine which content a personality preferred,
 the personality was encoded in the process they preferred.
 This was called /process orientated/ rather than contend orientated.
 cite:campos_mabs2009
 For example in their interpretation of MBTI a Sensing agent would make a plan
 in complete details whereas an intuitive agent would just continue planning as
 needed.
 Thinking agents would base their decision process upon their own believes
 whereas feeling agents would consider what other agents want.
 In our model we conceptualize the Jungian functions also as a process.
 We comment more on this in section [[Jungian BDI]].
 
** The serious game

 <<The serious game>>
 \cleared
 This chapter describes the game we inherited from our predecessors.
 We have to discuss precisely what they did for two reasons:
 1. To help understand the design constraints we work under
 2. To distinct our changes from theirs'

  \cleared
 There have been several distinct versions of the "communicate!" game. 
 The first version was a web based game, with a scenario editor.
 cite:jeuring2015demo
 However it had some drawbacks,
 for example each dialog was scripted by the teacher and the answers the student
 could give were specified by the teacher.
 This made practicing on it somewhat unrealistic.
 Practicing in this case would mean memorising what button to click rather
 than to figure out what to say.

 \cleared
 To address this issue the a new implementation was made. 
 This version was based around the idea of a chatbot, in the form of the ALICE
 bot.
 The AIML language was extended to allow emotional reactions of the agent.
 This new language was called S-AIML cite:augello2016model. 

 \cleared
 A specific scenario was created for doctor/patient interaction     
 cite:augello2015social.                                            
 The game in this version also has the ability to judge the skills practiced
 cite:augello2016social,
 such as following certain protocols (politeness, medical standards), and empathy.  

 \cleared
 There is a difference between the architecture in the published papers and
 the source code received.
 This is because the source code is actively being worked on, whereas the
 papers are snapshots of the source code at the time of publishing.
 An example of such a difference can be seen if we take cite:augello2016social
 in consideration,
 the judgement of these practices was for example encoded within the S-AIML
 language, however in the source code AIML has taken a step back.
 It is only used for text processing and not deliberation
 (which is now being taken over by drools as discussed in [[existing architecture]]).
 We will be using the source code as a guideline in discussing the existing
 work because it is more relevant and constructive.

*** Functionality
    \cleared
 There are two major functionality perspectives to consider,
 that of the student, and that of the teacher.
 We will consider these in separate subsections since in game they
 don't interact.
**** Student usage

    \cleared
 For a student to use the application he has to first start a client.
 He can now choose to start a new game.
 There are options to list existing games but these have not been completed.
 Once in game the user enters a screen as can be seen in [[fig:client]]:
  #+CAPTION: Client view
  #+NAME:   fig:client
  [[./img/client.png]]

    \cleared
From here the student can start practicing, the game will track his progress
on the server.
**** Teacher usage
    \cleared
For the teacher there is right now no client.
The way a teacher can setup a scenario is trough modifying AIML and drool files.
The teacher probably needs an expert to do this since these are right now
combined with the war.
It would be difficult to modify these files on a running instance.

*** Abstract architecture
\cleared
An abstract architecture was already in place and described very well
by cite:augello2016social. This can be seen in figure [[fig:abstract-architecture]],
which was directly taken from cite:augello2016social.
 
  #+CAPTION: Abstract architecture as described by cite:augello2016social
  #+NAME:   fig:abstract-architecture
  [[./img/abstract-architecture.png]]
  
\cleared
The interaction module handles user interaction, where the GUI can show the
dialogue and the mood of the agent.
The Dialogue module inside it however handles low level string interpretation
with help of AIML (see [[Text Processing]]), this basically works trough string
matching.
Note that although represented in the abstract architecture as the same module,
the GUI resides in  the implementation on the client side whereas the dialogue
module resides on the server. 

\cleared
The dialogue module calls directly the Representation and interpretation module
with help of specialized tags (see [[Deliberation]]) information can be inserted in
the representation and interpretation module.

\cleared
Both the 'representation and interpretation' module and the score module use
drools to do their respective tasks.
The only real separation in implementation is trough directory and file
structure, but at runtime there is little distinction.
The only other thing of note is the direct connection between the emotion module
and the GUI,
this is because the emotion module sends directly messages to the GUI whilst
ignoring all of AIML.

*** Application Architecture
<<existing architecture>>
    \cleared
    The game uses a client server architecture (see figure [[fig:components]]).
    The client is written in unity and the server is a Java servlet running on
    wildfly.
    Communication between the two applications happens trough a web socket.
    A web socket is used because it allows the chatbot to pro-active,
    which is not possible with a technology such as REST.

#+NAME: fig:components
#+BEGIN_SRC plantuml :cache yes :file img/uml/components.png :exports results
[Unity Client] <--> Websocket : json
[Wildfly Server] <--> Websocket : json
#+END_SRC
#+CAPTION: Component diagram of the application
#+LABEL: fig:components
#+RESULTS[6554c350da9b80944f22f0c6c29686b4608b9b50]: fig:components
[[file:img/uml/components.png]]

**** Source tree
    \cleared
    There are two major source trees tracked in separate version control systems.
    The first manages
    the client[fn::received on commit 40b55c0da1f556ba2b66ea8322d72008c9df1e72]
    and the second the
    server[fn:: received on commit 92f12fc26a7da83554903bfe7c6ed1cc64dd5a53].
    The protocol is tracked separately in the respective client and server
    folders with the folder name "dto".

**** Protocol
    \cleared
    The protocol is setup to be intended for a much larger system.
    There are hints of a registration system but further inspection
    revealed that only logging in only worked and but was required.
    This is tied into the server's ability to run multiple games. 
    there is also a limited monitoring functionality, the active games can
    be listed with a specialized message.
    A typical happy path scenario of protocol messages is listed in
    figure [[fig:sequence]].

#+NAME: fig:sequence
#+BEGIN_SRC plantuml :cache yes :file img/uml/sequence.png :exports results
  actor client
  entity server
  client -> server : login(userid,password)
  client -> server : newGameRequest
  server --> client : newGameResponse(idNewGame)
  client -> server : startGame(idGame)
  server --> client : log(text)
  == Chat start (example) ==
  client -> server: userUtt(text)
  server --> client: agentUtt(text)
  server -> client: agentUtt(text)
  client --> server: userUtt(text)
#+END_SRC
#+CAPTION: Sequence diagram of a typical game
#+LABEL: fig:sequence
#+RESULTS[db5e6bada22b64bf70a330d1219fedc990f11453]: fig:sequence
[[file:img/uml/sequence.png]]

\newpage
*** Server architecture
<<Server architecture>>
  \cleared
We will discuss the server architecture in more detail since it contains the
"brains" of the application.
The most important classes are shown in figure [[fig:class]].
WebSocket is the entry point for the program where the messages from the client
enter.

#+NAME: fig:class
#+BEGIN_SRC plantuml :cache yes :file img/uml/class.png :exports results
  interface ChatBotEngine{
    +String chat(String request)
    +void setSession(Session session)
  }
  class ChatBotEngineImpl {
    -KieSession kSession
    -Chat chatSession
    -Session session
  }
  ChatBotEngine <|-- ChatBotEngineImpl
  class WebsocketService{
    -ChatBotEngine cbe
    +void onMessage(Session session, String message)
    -void chat(Session session, Strin message)
  }
  WebsocketService --> ChatBotEngine

  package org.kie.api.runtime{
  KieSession <-- ChatBotEngineImpl
  class KieSession{
      +Facthandle insert(Object obj)
      +void setGlobal(String identifier, Object value)
  }
  }
  package org.alicebot.ab{
  Chat <-- ChatBotEngineImpl
    class Chat{
      +HashMap<String, Object> predicates
      +String multisentenceRespond(String str)
      +setKieSession(KieSession kie)
    }
  }
#+END_SRC
#+CAPTION: Class diagram of the server, where kie is the engine that handles the drools
#+LABEL: fig:class
#+RESULTS[0b594e175f82f51e7db78f2340ecb9fa14f3e0e6]: fig:class
[[file:img/uml/class.png]]

\cleared
The Websocket uses a ChatBotEngine to determine how to reply to userUtterances,
Where ChatBotEngineImpl is the concrete implementation.
ChatBotEngineImpl uses a KieSession for the drools and a Chat which is the alicebot.
Once the startGame message is received the kie service is started,
which runs on a dedicated thread to do drool deliberation.
At this point facts can be inserted for the drools to react upon, in case
of the anmnesi scenario the GameStart fact was inserted, which was a marker
object to indicate that the game has started.
This allow drools to take the initiative, for example when the user
hasn't replied after 20 seconds the agent will ask the user why he hasn't
replied yet.
A detailed overview of construction can be seen in figure [[fig:construction]].

\cleared
In the class diagram (figure [[fig:class]]), we can see an attribute to the Chat
class called predicates.
This is a bag of variables the drools can use to keep track of the scenario
progression.
The setGlobal method of KieSession is used to expose global objects to drools.
In this case the ChatBotEngineImpl is exposed.
Insert can be used to insert facts.
The difference between facts and globals is that facts are evaluated by
the  rule base, where as globals are used by the rule base.
A fact can be considered as "just a value".
Currently globals are used as communication with external libraries
(for example the websocket and chat session).

#+NAME: fig:construction
#+BEGIN_SRC plantuml :cache yes :file img/uml/construction.png :exports results
|WebSocket|
start
:Receve StartGame message;
:Construct a chatbotengine;
|#CCDDDD|Engine|
:Start kie thread;
:Register engine as controller in kie;
:Insert GameStart fact;
|#AntiqueWhite|Drool|
:Load aiml files;
:Construct a Chat object with help of AIML;
:Chat inserted in controller;
:Log to client;
|WebSocket|
:put game id in websocket user prefs;
stop
#+END_SRC
#+CAPTION: Activity diagram of a server game construction
#+LABEL: fig:construction
#+RESULTS[3acde42e45cb6f546f0d34b2c135845e8f592a48]: fig:construction
[[file:img/uml/construction.png]]

\newpage
**** Text processing
<<Text Processing>>
    \cleared
     Text processing is done with help of the ALICE chat bot.
     This bot does the parsing and validation of AIML,
     with help of the knowledge encoded in AIML it can specify a response.
     For example:
#+BEGIN_SRC xml
	<category>
		<pattern>
			What is the problem
		</pattern>
		<template>
			<srai>why are you here</srai>
		</template>
	</category>
	
    <category>
		<pattern>
			* why are you here
			</pattern>
		<template>
			<srai>why are you here</srai>
		</template>
	</category>
#+END_SRC
\cleared
     In this example the first category indicates that if a user types
     "What is the problem" (pattern tags), then the answer can be found in a
     category with pattern "why are you here".
     The second category does the same but the star indicates that any
     characters before the pattern can be ignored to satisfy the category.

**** Deliberation
<<Deliberation>>
\cleared
     AIML has been extended to allow updating of the drools knowledge base:

#+BEGIN_SRC xml
<category>
    <pattern>why are you here</pattern>
    <preconditions>not healthProblemAsked</preconditions>
    <template>
        <insert packageName="sp.anamnesi.health_problem" typeName="HealthProblemAsked" />
        I'm experiencing a <getDroolsTemplate />. It's quite strong.
    </template>
</category>
#+END_SRC

\cleared
     In this case if a user utters the sentence: "why are you here", the bot
     will check the drool database what his problem is and also update the
     scenario.
     Once the scenario is updated the possible responses of the chat bot are
     changed, as can be seen by the precondition tag.
     The template tag has some extra tags. The insert tag inserts a fact into
     the drools knowledge base, the getDroolsTemplate tag queries the drools
     knowledge base for a string.

**** User utterance processing 
<<user utterance processing>>

\cleared
An important process to describe is the way currently user messages are processed.
Figure [[fig:utterance-proccesing]] gives an overview of utterance processing.

#+NAME: fig:utterance-proccesing
#+BEGIN_SRC plantuml :cache yes :file img/uml/utterance-proccesing.png :exports results
          |WebSocket|
          start
          :Utterance received;
          :call chat;
          |#CCDDFF|Alice|
          if (AIML matched
          results?) then (No)
          :Default
          response;
          else (Yes)
          if (Has insert tag?) then (No)
          else (Yes)
          |#AntiqueWhite|Drool|
          :Insert fact into drools;
          |#CCDDFF|Alice|
          :Combine droolsting
                  with AIML;
          endif
          if (Has getDroolTemplate tag?) then (No)
          :Use template text;
          else (Yes)
          |#AntiqueWhite|Drool|
          while (Has reaction fact?) is (No)
          :Wait;
          endwhile (found reaction)
          |#CCDDFF|Alice|
          :Combine
            reaction
            with
            template;
          endif
          endif
          |WebSocket|
          :Send response
          to client;
          stop
#+END_SRC
#+CAPTION: Activity diagram of user utterance processing
#+LABEL: fig:utterance-proccesing
#+ATTR_LATEX: :width 1.0\textwidth
#+RESULTS[2be41360a975175f4a0734807235d7983de36beb]: fig:utterance-proccesing
[[file:img/uml/utterance-proccesing.png]]

\cleared
As can be seen in the diagram the message processing happens inside the Alice
bot.
Tags were added to AIML to allow the drool engine to be updated.
The drool system can be relatively easily be bypassed.
If there are no tags in the AIML the drool system will be oblivious of chat
messages.
Also note that there is a loop for the getDroolTemplates tag.
This is because a blocking queue is used,
which will block the thread until there is an item in the list.

** Personality influence case study
<<Personality influence case study>>

\cleared
To make the influence of personality more concrete,
and to get a possible expectation of what the chat bot should be able to do.
We want to make a scenario of a doctor appointment where each
patient has different personalities.
First we have Sander the INTJ, secondly Susie the ENFP and Chris the ISTP.
This type selection will give a rough usage of most Jungian functions.
In all cases the patients have the same problem, a back pain.
The cause of this problem in all cases is a worn out back.

\cleared
After the dialogue we will also discuss the motivations for saying things the
way they do.
This is important since because we are doing AI and not just computer science
we need to have an understanding what is going on in the mind of our test
subjects.


*** Sander the INTJ
\cleared
First we should note the dominant and auxiliary functions of the someone with an
INTJ mbti type.
An INTJ has as dominant function introverted intuition $N_i$ and as auxiliary
thinking extroverted $T_e$.
We would expect these function to be most obvious in the dialogue
(as discussed in section [[sec:types]]).
$N_i$ mainly focuses on connecting ideas and extroverted analyses objects
in the external world.
Combined with each other we get a personality that focuses on getting to goals
by analyzing situation far ahead of time.
This results in the expected dialogue which can be seen in table
[[tab:sander-conv-doct]].

#+CAPTION: Sander in conversation with the doctor
#+NAME:   tab:sander-conv-doct
| Who      | Utterance                                         |
|----------+---------------------------------------------------|
| Doctor   | Hi                                                |
| /Sander/ | /Hello/                                           |
| Doctor   | How can I help you?                               | 
| /Sander/ | /I have a back pain./                             |
| Doctor   | When did this first occur?                        |
| /Sander/ | /When I lifted a heavy object./                   |
| Doctor   | Oh, yes then you need some pain killers for this. |
| /Sander/ | /Thank you doctor/                                |

\cleared
Sander gives the doctor the information he needs to come to the conclusion he
himself probably already had drawn.
We could even expect him to ask for the medicine immediately,
however since this could make the doctor question his motives
(he could be addicted for example) he decides not to do this.
Doctor however doesn't go into the source of the problem.
He just assumed the patient overstretched himself because he lifted something
heavy.

*** Susie the ENFP
\cleared
As an ENFP, Susie has the dominant function of extroverted intuition $N_e$ and
as auxiliary function of introverted feeling $F_i$.
Therefore these functions should be most dominant in the dialogue.
$N_e$ focuses on finding possibilities in situations and $F_i$ is a internal
value based judgement function.
Combined with each other they make a personality who has strong ideals and is
enthusiastic about them.
The expected dialogue can be seen in [[tab:suzie-conv-doct]].

#+CAPTION: Susie in conversation with the doctor
#+NAME:   tab:suzie-conv-doct
| Who     | Utterance                                                          |
|---------+--------------------------------------------------------------------|
| Doctor  | Hi                                                                 |
| /Susie/ | /Hello/                                                            |
| /Susie/ | /How are you today doctor?/                                        |
| Doctor  | I'm good, how can I help you?                                      |
| /Susie/ | /I'm afraid I need some medicine/                                  |
| Doctor  | Medicine? Why do you need that?                                    |
| /Susie/ | /Well, I was watering the plants and all the sudden,/              |
| /Susie/ | /I got this pain in my back./                                      |
| /Susie/ | /Do you think I'm allergic to plants?/                             |
| Doctor  | Haha, no, I think we need to make a scan of your back.             |
| Doctor  | Because a watering can is a little to light to get back-pain from. |
| /Susie/ | /Of course doctor./                                                |
| Doctor  | Can you go to the hospital next Friday at 13:00?                   |
| /Susie/ | /Yes, I will go then./                                             |

\cleared
We can now see a stark difference with the INTJ personality.
First of all being dominated by extroversion it was Susie who took the initiative.
Secondly she directly asked for medicine, without thinking about the
consequences but knowing she probably needs it.
Then when explaining the situation she jumped to an idea of why she could have
this sudden pain,
without thinking about if it even makes sense that you are all the sudden
allergic to plants that have been in your home for a while.
The doctor does however come to the conclusion that something is odd about
getting a back pain from lifting a watering can.
So because Susie is more talkative the doctor decides to do more tests rather
than just giving some pain killers.
*** Chris the ISTP
\cleared
With his ISTP type, Chris has the dominant function of $T_i$ and then the
auxiliary function of $S_e$.
We therefore would expect these functions to do most of the work in the dialogue.
$T_i$ uses an internal reasoning structure to make judgments about the world
and $S_e$ uses the senses to gather information.
The conversation can be seen in table [[tab:chris-conv-doct]].

#+CAPTION: Chris in conversation with the doctor
#+NAME:   tab:chris-conv-doct
| Who     | Utterance                                                         |
|---------+-------------------------------------------------------------------|
| Doctor  | Hi                                                                |
| /Chris/ | /Hello/                                                           |
| Doctor  | How can I help?                                                   |
| /Chris/ | /I have back pain doctor./                                        |
| Doctor  | When did this first occur?                                        |
| /Chris/ | /Well I was watering the plants,/                                 |
| /Chris/ | /Perhaps I put to much water in the watering can/                 |
| Doctor  | Yes, that could be the case.                                      |
| Doctor  | However I would like to make a scan of your back just to be sure. |
| /Chris/ | /Can't you just give some pain killers to help me?/               |
| Doctor  | Yes but that will only work temporary.                            |
| Doctor  | So let's plan a scan at the hospital next Friday at 13:00?        |
| Doctor  | I can give you some pain killers meanwhile.                       |
| /Chris/ | /Okay, thanks doctor/                                             |

\cleared
So this dialogue looks a lot more like that Sander (INTJ) than that of Susie (ENFP).
However the motivation for the responses are quite different than that of Sander.
Chris hadn't figured out yet that he needed pain killers when he arrived,
since his auxiliary function is $S_e$, he hadn't thought that deep about the
problem.
He just knew he was in much pain, and knew the doctor could help with that.

\cleared
The difference with the dialogue of Susie is again quite obvious.
He didn't took the initiative because his dominant function isn't extroverted,
and unlike Susie he correctly asserted when the doctor asked about it
that the object he lifted may have been to heavy.

\cleared
The conclusion is again different.
Because one of the main functions of Chris is $S_e$ he wants to deal with the
pain /now/.
Therefore he asks the doctor explicitly for pain killers,
without considering that only the tests could actually solve the problem
permanently. 
However the doctor comes to a middle ground and besides ordering the test also
prescribes painkillers.

*** Influence of personality
\cleared
So we had 3 different doctor appointments all with the same problem but with
different personalities being at play.
The end result was three different outcomes for each patient.
Sander probably will be back next week with the same complaints at the doctor.
However this time his situation may have worsened.
Susie will get her problem eventually diagnosed like Chris,
however Susie won't have access to painkillers meanwhile.
Which may be uncomfortable to her.

\cleared
From this case study we can conclude that training doctors to deal with
different personalities is in fact very desirable because it can allow
patients to be treated sooner and more effective.
Sander could have had his problem diagnosed a week earlier and Susie could have
had access to pain killers for example.

** Software engineering literature
\todo{I guess this one is for if we have extra time}
Quality attributes of systems \url{https://en.wikipedia.org/wiki/ISO/IEC_9126}
\todo[inline]{Optionally discuss some of the patterns used}

*** Type signatures
<<Type signatures explained>>
\todo[inline]{Perhaps I should add citations?}
\todo[inline]{Add haskell prototypes as a reference to clearly mark out difference}
\cleared
If the reader is familiar with functional languages he can skip this section.
In this section we explain type signatures, partial application and functions
as arguments.

\cleared
Building upon the idea of pure functions we use the concept of /type signature/.
A type signature can be seen as a restriction on what goes in and what goes out.
This is analogous to a set
[fn::Strictly this is wrong, but in the formal description I use sets anyway],
or even a type as used in programming languages such as Java.
If you declare a function for example in Java:
#+BEGIN_SRC java
  class A{
      static int func(int a);
  }
#+END_SRC
What you're really saying is that "this function does not accept anything but 
int, otherwise the computation breaks".
Not that we define the class A, simply because we have to in Java.

\cleared
There are some things to keep in mind. First of all, there is a direct
correlation with how precise description is and the amount of things you can do
with the type. The more general the type the less you can do with it.
Secondly a more precise an type allows a /pure function/ to do fewer different
things with the type.
Say you have the following pure function:
#+BEGIN_SRC java
  class A{
      static int func(boolean a);
  }
#+END_SRC
How many different ints can it return? The answer is 2. Note that the function
can't just create a Random object to generate more entropy since we said it was
pure. Java can't enforce this, but a language like Haskell does.

\cleared
Even the return type is deceptive in java, since it can write results to
other objects or even do input/output operations.
This is of course a major problem if you want to do any kind of modeling of the 
domain based upon types.
Only if we assume we're dealing with pure functions,
we can reason about possible computations indirectly.

\cleared
Another really important feature of functions to keep in mind is that the
argument of an function can be a function itself.
This is how we combine Jungian function with each other to create personalities
in section [[Composing types]].
This has an analogy in Java for example:
#+BEGIN_SRC java
  interface B{
      int func(boolean a);
  }
  class A{
      static int func(B otherfunc, boolean a);
  }
#+END_SRC
The interface does not specify what behavior it contains, only the type of the
function. Because Java is object orientated B can be much more then just that
function, but according to the function in A its just that one function.
The only way to get around this knowledge restriction is trough casting in Java.
Since we're assuming pure functions however B can only expose read only
information.

\cleared
Then finally the last technique we use (implicitly) is a process called partial
application.
In which we 
which fill up some arguments of a function to create another function.
this is why the /next/ arguments in section [[Composing types]] can be other
function attitudes, because once inserted as argument the next argument of
themselves has already been satisfied which changed their type signature.
The first one was a unit however, which is just
The Java analogy is:
#+BEGIN_SRC java
  interface B{
      int func(boolean a);
  }
  class Unit extends B{
      int func(boolean a){ return 0; }
  }
  class A{
      static int func(B otherfunc, boolean a){ return a ? otherfunc.func(a) : 2; }
      public static void main(String[] args){
          final B partiallyAppliedA = new B {
              @Override
              int func(boolean a){ return A.func(new Unit(), a); }
          }
      }
  }
#+END_SRC
In this we can see that A becomes an B by using Unit as an argument.
This is done with help of an anonymous class because the Java programming
language  does not support partial application naively.
Therefore the type of A has changed in B by partial application.

\cleared
At this point we can see the Java verbosity really starting to hurt the point.
This is a major reason why the Haskell syntax is used to explain the type
signature in detail.
* BDI and Jung
<<Jungian BDI>>
# In this chapter we talk about the abstract ideas, any information neccasarry
# to execute the thesis without considering implementation details.
# so I guess height and node count aren't neccasarry.
\cleared
This chapter tries to anwser the question,
"what is personality from a computationally perspective".
In where we imagine personality being a preference towards a process rather
than a preference towards content.
We will however not consider yet how to place this in the existing system,
but will consider how to combine Jungian psycology with BDI.

\todo[inline]{Check if all logic contains all information we use in the code excluding implementation details}
\todo[inline]{Can we make monoids of these?}
\todo[inline]{We could add uterances as in the code and do time in that, so that we can make complete endomorphisms of these (as it is in the code)}

** Differences from campos
\cleared
Campos cite:campos_mabs2009 first considered how to combine MBTI with BDI.
His reasoning domain was however in action space (rather than just dialogue),
but we still want to use the idea that personality is a preference for a
process rather than a preference for content as discussed in section
[[BDI + Personality]].
However rather than using MBTI dimensions we want to use Jungian functions.
This is because Jungian function attitudes are the underlying construct of
MBTI and several other instruments (such as the PPSDQ and SL-TDI).

\cleared
There are some differences from the theory discussed in [[sec:types]] and Campos'
process.
The difference is that in the discussed theory we would translate MBTI to the
underlying Jungian functions, whereas Campos used the measured dimensions.
Translating to the functions has some advantages,
by doing so we are for example not bound to just the MBTI.
We also get more accurate descriptions of what Jungian functions are,
Jung described in his work people with that particular function as dominant.
This is harder to do with the dimensions, because if you take an INTJ type and an
INTP type the semantics of both the N and T change because of the P/J dimension, 
as can be seen in their respective order (see [[sec:mbti:order_comparison]]).
Campos avoids this by ignoring the I/E and J/P dimensions, resulting in a
simplified theory.
However we would like to note that it is not an easily extendable simplification.
Therefore we chose to translate types to orders in Jungian function attitudes,
something which is already done by MBTI (see [[sec:mbti:order_comparison]]).

\cleared
Another consideration to make is what are these function attitudes?
By which I mean what do they represent in computer science terms: programs,
objects or functions? What should they be?
Since Jung wasn't much of a mathematician cite:jungonfunctions its just an
informal definition.
However we can make a mapping to certain BDI processes
based upon their description,
but before that is done we need to make several structural observations.

\cleared
Firstly functions attitudes are not independent, by which I mean that
if we have a function attitude $a$, followed by $b$ then the resulting
behavior is different than $b$ followed by $a$ (see [[sec:mbti:order_comparison]]).

\cleared
Secondly all functions should be used and their order matters.
The first function used should be most prevalent.
This means that we can't just execute all functions and use a do preference
selection on the result.

\cleared
We will interpret the Jungian functions attitudes as a mapping from an agents
believes and senses towards an agent action and new believes.
This is then reduced to the scope of a chatbot in the social practice.
After this we will look what extra information the function attitudes need
in an attempt to reduce the amount of possible believes.

** Informal description of Jung + BDI
\cleared
Before diving into the type signature approach, or the formal description we
want to describe it informally.
Firstly we see the Jungian functions as a unit of processing.
This is a clear design choice, there are alternatives.
One could for example choose to make a unit of processing for every possible
combination of jungian functions attitudes which would result in $8!$ different
functions, or specifically just for MBTI which would result in $16$ functions.

\cleared
We also chose to model function attitudes, rather than functions and attitudes.
The reason for taking them as a combination is that there are more precise
descriptions available for function attitudes, rather than functions and
attitudes separated.

\cleared
So a function attitude as a unit of processing is something where information
goes in, the function does its processing and then information comes out.
This is analogous to a mathematical pure function.
Another way of describing such a process is a transformation upon information.

\cleared
From this we used the idea which MBTI uses too, that these small processing
units are in an order,
this order determines the eventual personality.
Then what we set out to do in the rest of the chapter is how to model this
into /type signatures/ and /types/.
/type signatures/ show what information (/types/) goes into a pure function.
In our case this usually contains a believe base for example.
Thus what we explored was what information does the believe base need to contain
for the Jungian functions to do their operations.

\cleared
There are several phases of processing going on.
Firstly we have user message parsing, where we try to figure out what the user
said.
Then, secondly there is action generation, where we use the parsed message to
determine sensible replies.
After that there is action selection, of which the best action is chosen.
This action is finally handled by the surrounding system.

\cleared
The opportunity for personality exists in practically all phases.
In the first phase for example we can do filtering based on the type of
messages received.
For example Thinking based personality may filter the message "how are you" as
an inquiry based on "how is your disease?", or "why are you here?".
Whereas a feeling based personality may retrieve a different meaning,
as in "how are you doing in live generally"?
We chose to not do such kind of personality based filtering because it
requires actual understanding of the message received.
Now there exist techniques such as convolution kernels cite:moschitti2004study
to decide what was said which can be combined with owl cite:world2012owl
to simulate a sense of understanding.
However implementing such techniques is considerably out of scope of this
thesis, and even with the existence of such techniques separately, its still
questionable if you can combine them successfully.

** A type signature approach
<<A type signature approach>>
\cleared
To give a better understanding of the scope of this project we will
try to come up with a type signature of a pure function that models all the
function attitudes.
This is done with a Haskell like syntax (see section [[Type signatures explained]]),
in which the arrows indicate a function,
left of the arrow is called a domain and the right side a codomain.
The domain is also the argument of a function.
If we see a pattern like $a \to b \to c$ means $a \to (b \to c)$ or give an $a$
and return a function $b \to c$, this process is called partial application
cite:haskellpartialapplication.
Capital letters indicate sets.

*** Narrowing the model
\cleared
We will go from an as broad as possible system (while using BDI) to a
precise as possible definition, while still being able to satisfy the domain.
This is desirable because it will restrict the amount of things that can happen
inside the function.
Therefore making it less complex and easier to understand.

\cleared
To make this process more easy to understand we'll postpone modeling interplay
between the $f_a$ function attitudes and define a type signature for them working
individually.
To do this we will define some terms, with which we will go from the broadest
definition possible towards one that just fits the project scope.

\cleared
Let $\mathcal{B}$ denote the set of all
possible believes and let $B_t$ with $B_t \subseteq \mathcal{B}$ denote the
believes of an agent at time $t$. 
$\mathcal{P}$ is the set of all possible sense information, in which $P_t$
with $P_t \subseteq \mathcal{P}$ denotes the perception information of an agent
at time $t$.
$\mathcal{A}$ denotes the set of all possible actions with $A_t$ with
$\Delta_t \subseteq \mathcal{D}$ denoting the set of actions executed by the agent at
time $t$.
With this definition we can define every possible agent configuration[fn::Note
that this is just the deliberation part, there is no memory in a pure function,
but the agent's memories can be stored in the believes.
The believes can be reused in the next call,
its up to the caller to decide how this happens.
This can be done on the thread of control the agent owns for example.
Where it will block until a time $t$ has passed or a new perception $P$ comes in from
the environment.]
as the following pure function type signature:
\[ B_t \to P_t \overset{f_a}{\to} (B_{t+1}, A_{t+1}) \]
This says, we first put in the current believe base, then the sensory
information after which we get a new believe base and a set of actions.
In this the intentions are encoded in the function used, and the desires are
part of the believe base.
We marked the $f_a$ arrow, which indicates the deliberation process of the agent,
so $f_a$ can be any of the function attitudes.

\cleared
This definition is however too general for our domain.
First of all the set of sensory information can be reduced to a String,
since this is the information we get from a user.
However a String is still to broad since going from a textual representation
to a deliberation process is difficult.
Therefore we will introduce another mapping function $g$:
\[ \sigma \overset{g}{\to} s \]
Where $\sigma$ is a string and $s$ a symbol where $s \in \mathcal{S}$ in which
$\mathcal{S}$ stands for the set of all encoded symbols[fn::Originally this was
called meaning with an $m$, but we want to avoid confusion with meaning in the
social practice, and therefore renamed it to symbol, as in symbolic
representation]

\newlyCleared
A symbol $s$, where $s = (\{\sigma\}, \sigma)$ has the first value as a set of potential
returning strings to utter,
and the second is the name of the scene the symbol occurs in.
The scene name is used as a name space and a crude way to measure scenario
progression.

\newlyCleared
With this we can define another function $g'$:
\[ s \overset{g'}{\to} \sigma \]
This allows symbol $s$ to be decoded into string $\sigma$.
Note that in this relation there can be multiple $\sigma$ that map to the same
symbol,
but one symbol produces only a defined set of strings $\{\sigma\}$,
that in turn map to itself,
on this a random selection can be made.
In the previous version this mapping was entirely done by AIML.
How this is done in this version will be discussed in the implementation
section [[From strings to meanings]].

\cleared
So the simplification is now as follows,
firstly we note that $\mathcal{S} \subset \mathcal{P}$,
since understanding meaning is a form of sensation.
Then we can define $S_t \subseteq \mathcal{S}$ which stands for the
symbols the agent understood at time $t$.
To ensure a reactive and proactive we also have to pass the current time
as argument.
This keeps open the possibility of the agent to do deliberation without having
received a symbol (ie empty set).
This produces the following type signature:
\[ B_t \to t \to S_t \overset{f_a}{\to} (B_{t+1}, \Delta_{t+1}) \]

*** The dialogue tree
\cleared
We have some believes, time and meaning going in, some deliberation
going on and a new set of believes and actions going out.
The new believes can be used for the next iteration.
However this type signature isn't enough.
The current agent has to be able to do a game tree like deliberation process
to reason about what the other agent will say so it can pick the meaning that
brings it closest towards its goal.
In our case a goal is a particular meaning the agent wants the doctor to utter,
for example if our agent is sick we want the doctor to utter a /GiveMedicine/
meaning,
or if he is in extreme pain he would like to see the /GivePainKiller/ meaning
uttered.
We also need to mark which agent uttered the dialogue tree node,
therefore we introduce $\Lambda$ as the set of all agents, where $a \in \Lambda$.

\toReview
Therefore we introduce $D$ a dialogue tree:
\todo{I think the idea of utterence should be introduced here since they contain perlocutionary values in the code, this is quite different from the logic we describe now, we can also put time in the utterences to create an endomorphism}
\[ D = (a, s, [D])\]
Where $s \in \mathcal{S}$ for the symbol,
and $[D]$ is the ordered list of dialogue children.
The initial dialogue is just a symbol with an empty list of children.
To consider a reply, we would use the same except with a list of children that
is bigger than zero.
The most preferred reply is the first element in the list of children.
How the actor is decided will be discussed in section [[symbol graph]].
An example of an expended dialogue tree can be seen in figure [[fig:dialoguetree]].

#+NAME: fig:dialoguetree
#+BEGIN_SRC plantuml :cache yes :file img/uml/dialoguetree.png :exports results
object D0{
a = "doctor"
s = "Greeting"
[D] = [D1, D2, D3]
}
object D1 {
a = "patient"
s = "Complaint"
[D] = [D5, D4]
}
object D2 {
a = "patient"
s = "QuestionIdentity"
[D] = [D6]
}
object D3{
a = "patient"
s = "Greeting"
[D] = [D1, D2]
}
object D5{
a = "doctor"
s = "StatusInquiry"
[D] = []
}
object D4{
a = "doctor"
s = "DoDiagnostics"
[D] = []
}
object D6{
a = "doctor"
s = "ShareIdentity"
[D] = []
}
D0 --* D1
D0 --* D2
D0 --* D3
D1 --* D4
D1 --* D5
D2 --* D6
D3 --* D1
D3 --* D2
note "This node is currenlty \n implicitly selected \n as response \n(because it came first \n in D0 as child)" as response
response .. D1
#+END_SRC
#+CAPTION: Object diagram of an expended dialogue tree. The leafs is where deliberation stopped.
#+LABEL: fig:dialoguetree
#+ATTR_LATEX: :width 0.5\textwidth
#+RESULTS[1f1f673a8fc69adbde74aa1a7aeb115abfb4b09a]: fig:dialoguetree
[[file:img/uml/dialoguetree.png]]

\cleared
With this in place we can replace both the $S_t$ and $\Delta_{t+1}$ with the $D_t$ and
$D_{t+1}$ respectively.
This is convenient because now we can model function attitudes as processing
units that take a dialogue tree and just modify it.
With this in place we can model the following type signature:
\[ B_t \to t \to D_t \overset{f_a}{\to} (B_{t+1}, D_{t+1}) \]
So we receive a dialogue tree from the user, which can just be a root node,
and then we put out a dialogue tree plus the replies which are the children,
whereof the first child is the most preferred.

\cleared
Now we should note that this type signature heavily constrains our agent.
It for example can't handle being punched in the face by the doctor unless
there is a meaning encoded for that. 
It also runs into trouble when the agent is asked to sit on the counter.
Movement should be possible, but movement like one does during sky
diving is not interesting because we have the informal constraint of the
[[social practice]].
However once movement becomes a requirement we can just create a new function
type signature that is less restrictive, but still has the option to use these
functions for meaning full replies.

*** Composing type dynamics
<<Composing types>>
\cleared
The first thing a commendable programmer may think of when trying to combine
behavior is of course functional composition.
However there is an important requirement for this to work.
The input type and output type need to be the same of the two functions we
want to combine.
Our current type signature has this feature almost except for the time
argument, it is not difficult to work around this.
What is problematic however is that using functional composition in this
way would make it impossible for function attitudes to inspect results
of their auxiliary functions.
This is an important feature we want to keep because if for example a
judgement function is first in the order of functions and receives
the user meaning it can't do its job yet, more on his in this section
[[Rational and irrational]].
Therefore we consider another approach.

\cleared
Another architecture that was considered would be to store the functions in a
list and then let an external control unit decide which function processes next.
However this would leave the control of the function being called outside of the
control of the function attitudes,
therefore personality wouldn't play a role in deciding the function being called.
It will also create another problem of deciding when a function is called.
So to solve these problems we looked at another possibility.

\cleared
In this approach we will give $f_a$ another argument which is the next $f_a$.
This looks like the following:
\[ \left (\overset{next}{B_t \to t \to D_t \to (B_{t+1}, D_{t+1})}\right ) \to B_t \to t \to D_t \overset{f_a}{\to} (B_{t+1}, D_{t+1}) \]
Note that the function in the next bracket has the same prototype as the codomain.
In this case the /next/ function can play an advisory role to the codomain.
A unit function can be defined that produces empty sets as results for both
believes and action.
By unit function we mean the initial /next/ function
that does nothing and just returns the believes and dialogue tree.
Since the notation has become quite complex at this point we provided 
an activity diagram of this works in the broader system,
which can be seen in figure [[fig:faprocessing]].

#+NAME: fig:faprocessing
#+BEGIN_SRC plantuml :cache yes :file img/uml/faprocessing.png :exports results
  start
  :Interpret string into symbol (s);
  :Call first fa in personality sequence;
  repeat
  :Current fa logic;
  repeat while (Called next?)
  repeat
  :Fa modify results of next;
  repeat while (Another fa on stack?)
  :Extract string from results (D);
  end
#+END_SRC
#+CAPTION: Activity diagram of function attitude (fa) processing.
#+LABEL: fig:faprocessing
#+ATTR_LATEX: :width 0.5\textwidth
#+RESULTS[f6dff7c3b2e64b005de74f12e19c7b2917da613c]: fig:faprocessing
[[file:img/uml/faprocessing.png]]

\cleared
To illustrate the use of this type signature design more clearly we'll sketch
an example with the first two function attitudes of the INTJ type:
\[N_i > T_e \]
So to encode this as a function we start with the least preferred function
attitude namely the $T_e$,
however to let it play an advisory role in the $N_i$ function we first
need to complete the /next/ argument.
Because its the least preferred function we just use the unit.
Now the partially applied type of $T_e$ satisfies that of $N_i$ and we can use
it as /next/.
This methodology can be used for an entire personality type (ie all 8 functions
in some order).
Also as an analogy we could say that we're dealing with an intrusive linked
list.
The next argument is just the next item in the list.
And unit is just the tail item of that list, which merely exists to provide a
start point to create the data structure upon and an endpoint for iterations.

\cleared
With this methodology function attitudes can decide themselves to consult the
next type.
Then they can inspect the result, and even the changed believe base to decide
if its a good idea to use the result.

\cleared
This architecture can be extended with the scale based jungian models
such as SL-TDI and PDQ by introducing a random choice for using the current or
next function.
However this becomes rather messy because we're modeling pure functions,
therefore we leave this as an exercise to the reader.

***  Rational and irrational
<<Rational and irrational>>
\cleared
Up until now we modeled the type signature to have a dialogue tree as input and
output.
However we have not considered yet how children are generated and how the order
is determined.
If we look at the definition (section [[Jungian types]]) of rational and irrational,
we can make a design decision about what these functions should do to the
children.
Rational functions are about making decisions therefore they
should apply order to the children.
irrational are about producing information therefore they should produce
children based.

\cleared
There are however some edge cases to consider when modeling this idea.
Say the primary function is a rational one.
It receives a dialogue with just the root node.
Currently it cannot apply any order since the children list is empty.
Luckily it can still use its next function, which is irrational
(see section [[Jungian alternating functions]]).

\cleared
Another situation to consider is what to do when there are already children.
Should an irrational function extend this list of children or go to some leaf
node?
Same question for a rational functions should it sort everything or just the
children list one layer above the leaf nodes.
At which level a function should opperate is rather fundamental.
So there are three ways of deciding this:
Firstly 

\toReview
In a naive approach we tried an implementation where irrational functions
will by default go down the left (most preferred) path to a leaf node and then
generate more meanings,
but rational functions will only sort the one layer above the leaf layer.
Doing this approach will make a first rational function basically do nothing,
unless he were to check if the option list is full or not to generate new options,
which results in boilerplate code and extra work.

\toReview
We also could use outside information to determine height.
Basically we would put into the believes the order of functions.
With this information and the dialogue tree we can calculate the
right level to operate upon.
A question then remains is if the rational should sort everything even below
its level or just its level. 
This can be answered however.
Rational should sort its level and everything below it,
because we can expect possibly multiple responses if the same actor self comes
up first.
Then we want the lower level also to be of the most important rational function.
The 'deeper' less important rational functions only have a guiding role for 
irrational functions after them.

\toReview
Another approach would be to let the rational functions sort the entire tree,
and irrational always let the most preferred option be extended.
At first glance this idea would make order in Jungian functions irrelevant,
but perhaps this isn't the case.
A rational function would still guide which part of the tree get extended, which
again would guide future rational orderings which again would guide extensions.
Since we have a 2 pass architecture order probably still matters.
However it should be noted that the functions may operate at unexpected level,
for example when going back from the second pass the most preferred
irrational function could find itself operating at the bottom of the tree.

\toReview
So the only thing that remains to do in deciding which architecture maybe
better is by coming with experimental evidence, as we did with the naive
approach to show it will work or not.

\cleared
With this in mind we can still say these things about the conceptualized
architecture:
/rational/ functions change the order of possible replies,
one layer above the leaf layer,
/irrational/ increase the number of children at the left most leaf layer.
So if we start with an irrational function it produces several related meanings
to the inputted meaning in a tree like structure.
The original meaning uttered by the user is the root node and the produced
response meanings are the children.
These then get inserted into the next rational function which modifies the order
of the children.
After doing this it passes this structure to the next
rational functions (because they alternate [[Jungian alternating functions]]),
until all functions in the personality had their chance.
Finally the unit function just returns the Dialogue and believes without
modifying them.
This returns trough all functions from before that can still modify it.
This could happen if a rational function was the first function for example
and didn't have any choices available to decide upon.
This structure still works with SL-TDI's non alternation.

** Mapping function attitudes to a process 
\todo[inline]{The validity of this chapter entirely depends on the result of making a test scenario}
\todo[inline]{So perhaps we should make integration tests to show that this works}
<<Mapping to process>>
\cleared
Now you may argue at this point we haven't refined our types a lot, since
the believe structure was defined as "Every possible believe",
which is basically analogous to "Anything you can think of" or in a Object
Orientated terminology: Object.
Since the believes serve as input of our function and output of the function
we may as well have said $Object \to Object$.
Of course the believes are not intended to be true output but rather just
part of the mind.
In other words, the believes are intended to be kept in a container
whereas the input $M_t$ and the output $R_{t+1}$ would only be visible for the
"outside world".
But still we want to refine our all possible believe to something which is 
less broad in scope.
To do this we will start analyzing the Jungian functions and see what
"extra" information require to function to perform their duties.
Then we will define the believe tuple more formally.

*** Symbol graph
<<symbol graph>>
\toReview
To make sure the agent stays on topic we will make use of a symbol graph.
This graphs gives connections to the symbols described in section [[A type signature approach]].
For a precise description of the implementation see section [[Symbol graph]].
The meaning graph $G$ is a set of connections $c \in G$ where
$c = (\pi, A, s_1, s_2)$, $s_1, s_2 \in \mathcal{S} \wedge s_1 \neq s_2$, $A \subseteq \Lambda$ is the set of
agents that can use the connection (to prevent the patient asking the doctor
about his health problems) and $\pi$ is the perlocutionary information about the
speech act (for example angering or scaring),
which is encoded in the edges because its not the meaning that causes these
but the way you get to those meanings.
In other words, being polite and then telling bad news causes different
perlocutionary values than just telling bad news.

\toReview
From this we can define a function that gets the allowed connections from
a symbol and an agent:
\[ a \to s \to [c] \]
We can retrieve $a$ and $s$ from the current node we are processing in $D$.
The result is a list of connections we can go to from that symbol.
Note that this list still should still be flattened to be attached as option to
dialogue tree $D$, since connection $c$ contains multiple agents $A$ and $D$
just a single agent. So a single connection $c$ can create multiple dialogue
tree $D$ children.
To determine the first dialogue tree $D$ node's agent an outside sensor is
required.

\cleared
The introduction of the symbol graph is probably the most radical change this
thesis proposes.
It moves chat bots away from the idea that responses are many to one relations
always and opens up many to many relations.
Please note that this is probably not only approach available.
It would have been more robust to use owl cite:world2012owl for example.
But this would open up a new problem on how to construct sentences,
of which the theoretical foundation is incomplete.
Another issue is how to interpret meanings, where you could for example use
cite:moschitti2004study.
But again its just a piece of the puzzle.
We think that making those steps are to big and probably simply will result into
failure.
However the symbol graph provides a good middle ground,
in which its relatively easy to implement but offers enough freedom to encode
personality in as a process.
Note that this approach fits nicely into the information state transitions
idea discussed in section [[Dialogue systems]].

*** Irrational
The irrational functions rely heavily on the symbol graph to create new
children in the dialogue tree.
This is under the assumption that connections in the symbol graph are always
on topic.
Please note that all irrational functions need to have an extra argument
to limit their activities.
Since considering the entire graph is unrealistic, and unnecessary.

\cleared
In the initial design of what the $S$ and $N$ functions should do
as algorithm we considered them in the following way.
$S$ would be analyzing all available options rigorously in a forward chaining
process, whereas $N$ would do backward chaining, starting at the goal and going
trough some way points directly to the starting point.

\cleared
This would translate into $S$ going several plies deep into the
meaning graph before calling the $next$ function and returning the result.
And if we assume that the $next$ function brings us closer to the goal we can
use it as a heuristic to let it determine the direction for $N$.
This of course doesn't allow us to do backward chaining since there is hardly a
guarantee that the $next$ function will bring us to the goal, in fact we may get
stuck in a loop.

\cleared
Alternatives to the implementation proposed include the use of
probabilities to determine appropriate responses. 
However this introduces a new problem of how to obtain the probability
distributions.
Machine learning could be used for this, but this raises the question:
"learn on what"?
Since the answer to that question is non-trivial, we consider such a solution
out of scope.

**** Intuition
# http://personalitycafe.com/cognitive-functions/83205-whats-difference-between-ni-ne.html
\cleared
We can consider $N_i$ to be a depth first approach. Going several plies deep and
at each ply consulting the $next$ function which step to take.
$N_e$ on the other hand just takes the top $x$ of the current dialogue options
and expands those, but then next step it will again consider the entire existing
tree to find the best $x$ of each ply.
This will of course be a much more shallow consideration than $N_i$, but 
much more broad. Which is of course the behavior we are looking for in both
$N_i$ and $N_e$ (see section [[Jungian types]]).

**** Sensing
\cleared
The $S_e$ function just receives all possible connections from the current
meaning for several plies and then applies the /next/ function on it.
The $S_i$ however is more conservative and will only pop $x$ random meanings by
default (the first $x$ connections),
however it will construct its own connections of whatever the user said in
response to the bot from previous conversations when at the same meaning (if it
didn't exists already).
Whenever these connections are available they will substitute the random $x$.
So $S_i$ starts of kind off similar to $S_e$ but builds up over time.

*** Rational
\cleared
In the current design the rational functions apply order to the children of a
current dialogue node.
Then once finished they will call the $next$ function on the most preferred
child. This is to ensure all function attitudes can do some processing.

\cleared
Please do note that although we have a game tree,
we're not dealing with a zero sum game.
Dialogue is cooperative rather than competitive (see section [[Speech act thoery]]).
So doing an algorithm such as mini-max is out of the question.
However we will borrow parts of it.
Namely whenever a rational function finishing ordering the input set it will
call the /next/ function to do deliberation on the most preferred item.

\drafting
Also we model the rational functions as local optimizing functions. Only
the current ply and maybe the next ply is considered, but not the entire tree.
The primary reason for this is ease of implementation. However there is no
reason why the entire available tree couldn't be used.

**** Feeling
\cleared
Initially we wanted to create two lookup tables for both feeling functions one.
However this would be confusing to configure,
the scenario creator would need to decide which values are "external" and which are
"internal".
Campos however modeled feeling as a prediction of what the other agents will
do.
This describes $F_e$ rather well, $F_i$ not so much however.
So we adapted and adopted that idea for $F_e$ and for $F_i$ we used the lookup
table.

\cleared
Both feeling functions $F$ use the perlocutionary acts to order the children.
$F_i$ uses a predefined value set $h$: 
\[ \pi \overset{h}{\to} i \].
This valuation is done by a lookup table on all avalaible perlocutionary speech
acts.
$F_e$ tries to figure out what the conversation partners values by
picking the perlocutionary act the other chose most.
This is done by simply keeping track on how many of such speech acts the
partner uttered and picking the that has been uttered most, if that one is not
available we move to the next one. This is similar to fictitious play.

**** Thinking
\cleared
Normally the $T$ function is about reasoning.
There is little reasoning to do in our scenario except to get to the goal as
soon as possible.
The thinking functions $T$ do this without paying any attention to
perlocutionary speech acts.

\toReview
We could say that while feeling is concerned with perlocutionary speech act goals
thinking on the other is concerned with symbolic goals.
To model the goals of the thinking functions we will introduce the set of goals
in an agents believe base $\Phi$. Where a single goals $\phi \in \Phi$ consists of
$\phi = (a, s)$ a symbol uttered by a particular agent.
Then there also exists the function that can compare goals with each other:
\[\phi_1 \to \phi_2 \to b \]
where $b \in \{ \top, \bot \}$ is a boolean, true or false that determines if the first goal is more
important then the second. This function is asymmetric.
Finally there is a function that determines if a goal is completed or not:
\[\phi \to b\]

\toReview
Now to begin with $T_e$.
It sees the problem to solve as the conversation itself.
Therefore it will consistently choose speech acts that could help the partner to
progress the scenario.
So it wants to put the partner in a position where he has
almost no other options except to progress the scenario.
If it encounters a child node with a goal $\phi$ in it it will give priority to that.
If there are multiple goals in the options the comparison function can be used
to determine the most important one.
Scenario progression is measured with help of scenes.
If an option changes scene we assume it progresses scenario.
This comes secondary to finding goals.

\cleared
To model $T_i$ however the most obvious solution would be to implement an
axiomatic logic system.
This is however rather heavy on maintenance.
Every agent would need to have their own axiomatic system to determine what to
do for each node in the symbol graph.
The only real solution would be to create this dynamically somehow,
but this is out of scope of this thesis.
Therefore we looked for an alternative.

\cleared
$T_i$ wants to help the conversation partner to analyze the problem according
to the partner's own internal logic framework,
and to do this it wants to give as much options as possible to the partner.
Therefore it will choose the speech acts that produce the most symbols for the
partner.
To do this it will sort the child nodes according to as much unique symbols as
possible.

*** Believes overview
<<Believes overview>>
\cleared
We listed the function attitudes $f_a$ and their required information into
table [[tab:fa-and-data]]. 
Therefore $B = (h, [s], \Phi, G)$, which looks like a relatively small believe base,
\todo{Should contain all data that can change, therefore most functions probably not}
except for the incredible amount of information encoded in $G$.

#+CAPTION: function attitudes and their required data
#+NAME:   tab:fa-and-data
| Function | required data                                 |
| $T_e$    | the set of goals $\Phi$, scene information and G |
| $T_i$    | the set of goals $\Phi$, and G                   |
| $F_e$    | utterance history [s] and G                   |
| $F_i$    | Personal values  $h$                          |
| $S_e$    | G                                             |
| $S_i$    | Utterance history [s] and G                   |
| $N_e$    | G                                             |
| $N_i$    | G                                             |


*** Turn taking
\toReview
In the naive approach we modeled turn taking with a simple round robin strategy. 
Basically the irrational functions would only consider options that change
actor between plies. 
This makes it however difficult to model agents that hold long monologues,
which happens to for example Susie from the test scenario.
You could do it by making just more symbols that hold all these utterances in
one. However this is very inflexible.
So to solve this problem we make alternation whenever there is a tie between
two options. So irrational would leave out the option that doesn't alternate,
and rational would prefer alternation when possible.

*** Function plie depth
\todo[inline]{Discuss why this is necisarry, why don't we just do the entire tree for rational}
\todo[inline]{Why don't we do from a particular level and then downward? Or why would we do just a particular level (haven't decided)}
\toReview
A big issue that turned up was at which level function ought to operate.
We have a two pass architecture, where functions can inspect the dialogue tree
before passing it to the /next/ function, but they can also inspect the result
of the /next/ function.
The reason for the two pass architecture is explained in section [[Rational and irrational]].
However it didn't treat another issue, which is how do function know at which
level they should operate?
This information isn't available in the current design.
We just assumed that rational should always act 1 level above the leaf nodes,
and irrational at the leaf nodes.
This would however make a rational function in the first position the least
relevant function, but it should be the *most* relevant.

\toReview
The question of at which level should a function operate is surprisingly complex.
Lets start by analyzing what information is needed.
Firstly we need to know the function order, then the function itself and finally
the height of the dialogue tree. Which results in the following:
\[ [F_a] \to F_a \to i_{D_{\text{height}}} \to i_{\text{operate level}} \]
Where $F_a$ is the Jungian function,
$i_{D_{\text{height}}}$ is an integer which indicates the height of the dialogue tree
and $i_{\text{operate level}}$ is the suggested operation height.
The (naive) calculation is:
\[max(0, i_{D_{\text{height}}} - \text{index of }F_a \text{ in } [F_a])\]

\toReview
This does not take into account that rational functions should operate 
a level higher than irrational.
It also doesn't take into account that rational functions don't generate height,
and therefore their indexes have no effect upon height.
So obviously we need information about whether or not a function is rational:
\[ F_a \overset{\text{is rational?}}{\to} b \]

\toReview
Then with this function we can define a precise calculation:
\[max(0, i_{D_{\text{height}}} - \text{count not is rational before} F_a \text{ in }[F_a]) \text{if is rational } F_a \text{ then } +1 \]
This satisfies the properties we want.
** Consistency with theory
\cleared
In this section we will explore if especially INTJ and ENTJ (MBTI) types would
produce different actions by analyzing when the functions would act.
We will only look at the first two functions because it is enough:
The first two functions of INTJ are:
\[N_i > T_e \]
And of ENTJ they are:
\[ T_e > N_i \]

\cleared
What we would expect is that the $T_e$ and $N_i$ produce different results
because of the order they are in the sequence.
So in case of ENTJ if $T_e$ receives an meaning $M_t$ from the user
it will pass it directly to the next function since it can't make decisions
based on a single meaning. Then once the $N_i$ function returns a reply
(which has the entire meaning tree in it) it can judgments based on these 
meanings producing a final reply.
In case of INTJ the $N_i$ function would generate the meanings based on its
data structure and pass this tree with the children to $T_e$ to assign values
to it.
Then it can either return this result or pass it to the /next/ function and
judge these results again.

\cleared
In any case the main difference is that an INTJ $T_e$ function gets to
value before the other functions get a chance to do anything. In case of an
ENTJ it is always just a final judgement.

\cleared
INTJ and INTP are different in attitudes, but have the same order.
Since attitudes produce a different process by definition
(see section [[Mapping to process]]),
we can conclude that they will also behave differently.

* Architecture
\toReview
To combine the ideas discussed in section [[BDI and Jung]] with the existing program,
some big architectural changes were introduced.
For example the Alice bot was completely removed in favor of a new less tightly
coupled scheme.
The drools have become the center of deliberation (which previously was the AIML).
We will discuss these changes in this chapter.

\toReview
In this chapter we will discuss two architectures,
the first is the architecture which is actually implemented, this deals with a
single agent and the user.
Secondly we will discuss the N-agent
architecture which is capable of dealing with more than one agent (and the user).
The reason for discussing both is that the ideas discussed in section [[
BDI and Jung]] could be applied to an N-agent architecture.
However there are some pragmatic problems with this discussed in section
[[Multiple conversation partners]].
Therefore we held of on implementing this and just describe how it could be done
instead.
In section [[Implemented architecture]] we will describe the architectural changes
between the current implementation and the original architecture discussed in
section [[The serious game]]. Note that we want to keep as much of the existing
architecture as possible.

\toReview
There are also several items we won't discuss in this chapter because they
haven't changed, these include the protocol,
and the wildfly server and the unity client.
** Implemented architecture
<<Implemented architecture>>
A deployment diagram of the architecture can bee seen in figure
[[fig:architecture-concept]]. This is a rough overview of how everything interacts
now with each other.

#+NAME: fig:architecture-concept
#+BEGIN_SRC plantuml :cache yes :file img/uml/architecture-concept.png :exports results

  folder bot{
    cloud drools {
      component score
      component scenario
      component interpreter
      storage facts
      scenario -- facts
      interpreter -- facts
      score -- facts
      component emotions
      component personality
      emotions -- facts
      personality -- facts
    }
    database meanings [
      patterns
      ====
      symbol graph
    ]
    entity filereader
    filereader ..> meanings
    drools --> meanings
  }
  node server
  node client

  server "1"-- "1..*" bot
  server  -- scene
  server  -(0)- client
#+END_SRC
#+CAPTION: Deployment diagram of new architecture, where the dashed arrow means constructs, the solid arrow means uses and the other lines mean itneracts.
#+LABEL: fig:architecture-concept
#+RESULTS[f680066f7943af2a2abf4c0b6a487ffa49d6b619]: fig:architecture-concept
[[file:img/uml/architecture-concept.png]]


\cleared
The biggest difference from the original architecture is the removal of
distinction between drools and the chatbot.
In the new architecture we make all information in the files available to
the drools in a big database.
This is starkly different than the architecture used in section [[
Server architecture]].
As can be see in section [[user utterance processing]], 
int the old architecture, the reply for a message is already determined before
drools had a chance to do deliberation.
What's even worse is that if the drools want to utter a spontaneous utterance,
then it has to be encoded in a string inside the drools themselves.
This means the strings facing the user are spread over both the AIML files
*and* the drools.
This is confusing for new scenario creators since completely different folders
have to be accessed to change the strings.
# This violates the changeability design principle quite heavily and unnecessarily. \todo{cite}

\toReview
The changes proposed here, result in a much more simple architecture.
Only one place does deliberation rather than two and only one api is used for
generating responses.

\toReview
Note that although we removed the ability for the bot to use AIML, it should be
relatively easy to convert from the old AIML structure to the new format with help
of a script.
A proof of concept has been made of this in section [[Conversion script]].

*** Jungian class diagram
\toReview
A class diagram[fn::note that we use $[\dots]$ for list and $\{\dots\}$ for set in the
class diagrams to save space]
of the model that was used to implement the ideas from section [[BDI and Jung]] can
be seen in figure [[fig:jungclass]].
All classes in this diagram are immutable.
Meaning that once created their values will never change.
A value can be changed by making a new instance with the modified value.
Setters are often provided for values that need to change regularly,
they return a new instance however.
Using an immutable model has several advantages over the conventional approach.
The biggest one being that the entire model is thread safe, without the use of
locks.
This makes it very easy to use with drools since a drool engine could execute a
rule at any time.

#+NAME: fig:jungclass
#+BEGIN_SRC plantuml :cache yes :file img/uml/jungclass.png :exports results
  enum PerlocutionaryValue
  package db{
    class ConnectionDatabase
    class PatternDatabase 
  }
  PatternDatabase --* "*" PatternSymbol
  PatternDatabase --* "*" Scene
  ConnectionDatabase --* "*" Symbol
  ConnectionDatabase --* "*" Connection
  class Actor {
    +name:String
  }
  class Believes{
    +programmedConnections:ConnectionDatabase
    +learnedConnections:ConnectionDatabase
    +goals:{Goal}
    +values:PersonalValues
    +learnedValues:PersonalValues
    +actors:{Actor}
    +previousUtterances:[Utterance]
  }
  Believes -* "2" ConnectionDatabase
  Believes --* Goal
  Believes --* "2" PersonalValues
  Believes --* "1..*" Actor
  Believes --* "*" Utterance
  class Connection {
    +to:Symbol
    +restricted_to:Actor
    +perlocuationaryValues:{PerlocationaryValue}
  }
  Connection --* Symbol
  Connection --* Actor
  Connection --* "*" PerlocutionaryValue
  class DialogueTree{
    +options:[DialogueTree]
    +utterance:Utterance
    +connection_used:Connection
  }
  DialogueTree --* "*" DialogueTree
  DialogueTree --* Utterance
  DialogueTree --* Connection
  class Goal{
    +utterance:Utterance
    +utility:int
    +isGoal(utterance:Utterance):boolean
  }
  Goal --* Utterance
  class PatternSymbol{
     +pattern:Pattern
     +symbol:Symbol
  }
  PatternSymbol --* Symbol
  class PersonalValues{
     -values:EnumMap<PerlocutionaryValue, Integer>
  }
  PersonalValues --* "*" PerlocutionaryValue
  class Scene {
    +name:String
  }
  class Symbol {
    +name:String
    +scene:Scene
  }
  Symbol --* Scene
  class Utterance{
    +byWhom:Actor
    +what:Symbol
  }
  Utterance --* Actor
  Utterance --* Symbol
  Actor -[hidden]-> PersonalValues
  db -[hidden]-> DialogueTree
#+END_SRC
#+CAPTION: Class diagram of the model
#+LABEL: fig:jungclass
#+RESULTS[f82f6a57ddceddbaf3e480f1aacea3600693a8a9]: fig:jungclass
[[file:img/uml/jungclass.png]]

**** db package
\toReview
Figure [[fig:jungclass]] only shows part of the db package because it couldn't quite
fit,
in figure [[fig:dbclass]] the databases are shown.
This is a sub package of the model.
The database is best seen as an immutable hashmap. It also provides some extra
nice java8 features,
such as returning an optional rather than null for the get method.
The concrete implementations of database can add extra behavior once the type
value of the generic parameters is known which is done by connection database
for example.
#+NAME: fig:dbclass
#+BEGIN_SRC plantuml :cache yes :file img/uml/dbclass.png :exports results
  package model.db{
    abstract class Database<Key,Value>{
      -values:Map<Key,Value>
      +get(key:Key):Optional<Value>
      +getOrThrow(key:Key):Value
      +keys():Stream<Key>
      +values():Stream<Value>
      +entries():Stream<Map.Entry<Key,Value>>
    }
    class ConnectionDatabase<Symbol, {Connection}>{
      +getAllowedConnections(name:Symbol, role:Actor):Stream<Connection>
      +getFromTo(from:Symbol, to:Symbol):Optional<Connection>
      +putInCopy(symbol:Symbol, connections:Connection...):ConnectionDatabase
    }
    class PatternDatabase<Scene, {PatternSymbol}>
    class SymbolDatabase<String, Symbol>
    ConnectionDatabase --|> Database 
    PatternDatabase --|> Database 
    SymbolDatabase --|> Database 
    ConnectionDatabase -[hidden]-> PatternDatabase
  }
#+END_SRC
#+CAPTION: 
#+LABEL: fig:dbclass
#+RESULTS[96b2d711babc6789186ccd98dee8488eaa83a349]: fig:dbclass
[[file:img/uml/dbclass.png]]


\newpage

*** Initialization
\toReview
In the new architecture we use a file reader to construct the 
symbol database, pattern database and connection database
into memory and expose them to the drools as can been seen in figure
[[fig:initjung]].
This allows the drools to utter spontaneous symbols and allows do deliberation
on incoming utterances of the user with help of the interpreter component.
The reactionary based scheme where an incoming user response has been replaced
with a structural scheme.
A symbol has connections to other symbols, which should be sensible responses.
So if a user utters a symbol then rather than having a direct response string
that uses variables to fill in the gap, we get a symbol, which connected to
other symbols which in turn are also connected.
We can do deliberation on this because we have one or more goals.

#+NAME: fig:initjung
#+BEGIN_SRC plantuml :cache yes :file img/uml/initjung.png :exports results
  |WebSocket|
  start
  :Receve StartGame message;
  :Construct a chatbotengine;
  |#CCDDDD|Engine|
  :Start kie thread;
  :Create Personality;
  :Register a controller;
  :Parse YML files and construct databases;
  :insert databases in drools;
  :insert startgame fact;
  |#AntiqueWhite|Drool|
  :construct believes;
  |WebSocket|
  :put game id in websocket user prefs;
  stop
#+END_SRC
#+CAPTION: Activity diagram of a server game construction
#+LABEL: fig:initjung
#+RESULTS[27d9aa994f1480545d631ca9c4495d96e891f520]: fig:initjung
[[file:img/uml/initjung.png]]

**** Parsing YAML
\toReview
To parse the several helper classes are used. These are prepended with the
keyword raw and are designed to work with the library to parse YAML.
From this it is rather easy to create the immutable and non primitive based
objects we want.

\toReview
First the Symbol files are parsed and converted into a SymbolDatabase.
This is the most easy to do since this database is just the symbol name
as key with symbols as value as can be seen in figure [[fig:dbclass]].
The pattern database is also constructed from the raw symbols.
From these the patterns are striped and with help of the symbol database
the right symbols are linked to patterns.
Note that although the rawsymbols
contain patterns strings the eventual symbols don't.
This is because we split up patterns and potential (agent) utterances into
distinct data structures.
Where the patterns just point to the complete symbols.
With help of this the connection database can be easily constructed.

\todo[inline]{Diagram??}

*** Operation
 \toReview
 Once the meaning is extracted from the user utterance we can insert it into the
 drool engine to do deliberation upon (see [[fig:insert_meaning]]).

 \toReview
 It should be noted that at the point of quick reply personality could also be at
 play.
 For example people could have alternative ways of pronouncing the response.
 Thinking people may for example respond with a confident yes, whereas feeling
 people would say it by default in a more doubting tone.

 \toReview
 To ensure rules are executed in a particular order we often wrap and unwrap
 required data into types.
 For example the initial user utterance gets wrapped into an UnparsedUttarence
 type. This is to indicate that the plain string should be parsed first.
 Once this is done all meanings get extracted and inserted into a parsed
 utterance.
 From this rules can decide what to do further with this parsed type.
 Currently we just take the first symbol from the ParsedUttarence and convert it
 into a single utterance, while truncating all other utterances.
 Currently we delete the type after unwrapping it from the facts.
 This is to prevent double execution of rules.
 

 #+NAME: fig:insert_meaning
 #+BEGIN_SRC plantuml :cache yes :file img/uml/insert_meaning.png :exports results
 |WebSocket|
 start
 :Receve message;
 :Insert into KIE;
 |#CCDDDD|Drool|
 :Pattern match symbol from message;
 if (quick response rules?) then (yes)
 :Do quick reply;
 |WebSocket|
 stop
 endif
 |#CCDDDD|Drool|
 :Create initial dialogue tree;
 while (has next function in personality)
 |#AntiqueWhite|Personality|
 :execute jungian function;
 |#CCDDDD|Drool|
 endwhile
 while (has previous function in personality)
 |#AntiqueWhite|Personality|
 :execute jungian function;
 |#CCDDDD|Drool|
 endwhile
 :Get response from dialogue tree;
 |WebSocket|
 stop

 #+END_SRC
 #+CAPTION: Activity diagram of deliberating on a user message
 #+LABEL: fig:insert_meaning
 #+RESULTS[9e2ca9f29a8e620296912b3f55d13052ac51977b]: fig:insert_meaning
 [[file:img/uml/insert_meaning.png]]

 \newpage

** Desired architecture
A deployment diagram of the architecture can bee seen in figure
[[fig:n-agent-arch]].

\todo[inline]{N-agent stuff}
\todo[inline]{Social practices (metnion at least, and say that there are elements of it in drools)}
\todo[inline]{So there is no social practice context taken in account currenlty, mention how we could do that (for example by wrapping the pure jungian function in a context function, or by wrapping the result of the function processing drool in another type to give other drools a chance to look and modify it, or by analyzing the result of the entire personality process (a bit crude though))}

#+NAME: fig:n-agent-arch
#+BEGIN_SRC plantuml :cache yes :file img/uml/n-agent-arch.png :exports results

  folder scene{
    cloud drools {
      component score
      component scenario
      storage facts
      scenario -- facts
      score -- facts
    }
  }

  folder bot{
  cloud bot.drools {
    component emotions
    component personality
    storage bot.facts
    emotions -- bot.facts
    personality -- bot.facts
    score -- bot.facts
  }
  database meanings [
    patterns
    ====
    symbol graph
  ]
  entity filereader
  filereader ..> meanings
  bot.drools --> meanings
  }
  node server
  node client

  server "1"-- "1..*" bot
  server  -- scene
  server  -(0)- client
#+END_SRC
#+CAPTION: Deployment diagram of new architecture, where the dashed arrow means constructs, the solid arrow means uses and the other lines mean itneracts.
#+LABEL: fig:n-agent-arch
#+RESULTS[dcf9326c7d021dc38efdf65d5dbab06d0277c1df]: fig:n-agent-arch
[[file:img/uml/n-agent-arch.png]]


+ Need to change the protocol & userinteface to allow for sending to a particular bot
+ or need to do a hearistic to detect to which agent is talked to
+ Protocol needs to be  able to distinguish which agent says what or which emotional changes occur in which agent.
+ Message processing occurs at either agent or both agents (they need to record history anyway)
+ The functions need to be made aware of dealing with multiple agents, ie generating
  more options if possible (for example any agent wouldn't result in any option
  but n distinct options).
+ Agents should be able to talk to each other, but they need to time this
  properly. Otherwise the user will suddenly see 200 lines of chit chat appear
  while the agents wait upon input.

* Implementation
\drafting
In this chapter we will be disusing in detail the implementation.
This will include a new representation of the dialogue knowledge.
Then we will also described how the BDI+Jung chapter came from a theoretical
construct into realization.
After this we will show how we tested them to make sure they work properly.

\todo[inline]{describe what can be found in chapter}
\todo[inline]{describe structure of chapter}

\cleared
Whenever one starts working on an existing software project the first obstacle
faced is building/executing the project, this is discussed in detail in the
appendix [[building]].

\todo[inline]{Proposed Extension, why this extension}

** Simplifying AIML
# Note taht such descriptions are good, show how it was in the old system and
# the thought process towards the new system, so do more of it like this
<<From strings to meanings>>
\cleared
The first step is to map the user input which is of type String at time $t$
into type $S_t$ for symbol at time $t$.
For this the AIML is used. It can matched, however the language has to be
modified to not just produce a reaction but instead indicate what the symbol 
is of a reaction.
This change will loosen the connection between a pattern and a response.
In other words, it will create some more "space" to do actual deliberation.
We will discuss later in this chapter how we retain plausible conversation
connections.
We will use an example to illustrate the differences:

#+BEGIN_SRC xml
<aiml>
	<category>
		<pattern>
			How are you
		</pattern>
		<template>
			Not doing too well today.
		</template>
	</category>
	<category>
		<pattern>
			How * you
		</pattern>
		<template>
			<srai>How are you</srai>
		</template>
	</category>
</aiml>
#+END_SRC
So the modification would now look like:

#+BEGIN_SRC xml
  <aiml>
	  <category>
		  <pattern>
			  How are you
		  </pattern>
		  <symbol>
            StatusInquiry
		  </symbol>
	  </category>
	  <category>
		  <pattern>
			  How * you
		  </pattern>
		  <symbol>
            StatusInquiry
		  </symbol>
	  </category>
  </aiml>
#+END_SRC

\cleared
These changes loose reactive nature of the chatbot,
rather than saying how to reply directly to certain kind of responses we just
say what they mean in this social context.

*** Symbol to string
\cleared
But now what if the agent wants to do a status inquiry,
well the string into a status inquiry is already available,
we just need to separate the literal strings from the catch all patterns:

#+BEGIN_SRC xml
  <aiml>
	  <category>
		  <literal>
			  How are you?
		  </literal>
		  <patterns>
			  <pattern>How * you</pattern>
			  <pattern>How are you *</pattern>
		  </patterns>
		  <symbol>
            StatusInquiry
		  </symbol>
	  </category>
  </aiml>
#+END_SRC

\toReview
If the agent wants to do a status inquiry it will use the literal,
if a user string needs to be converted to a meaning we first look at all
available literals and then we start with the patterns.
So if the agent wants to reply to this it will use its personality
functions to put in the meaning out of which comes a reply, which contains
a meaning and therefore also a literal with the text it needs to say.

\toReview
Finally we can remove both the aiml tags and category tags and use the value
of the meaning tags as the file name to ensure there is always one meaning.
Thereby we leave the aiml tags and category tags implicit and the meaning
name derived since you need to have a filename anyway.
This results in a much more terse definition:

#+BEGIN_SRC xml
<literal>
    How are you?
</literal>
<patterns>
    <pattern>How * you</pattern>
    <pattern>How are you *</pattern>
</patterns>
#+END_SRC

*** using yaml
\toReview
Now since the above isn't technically valid xml (you can't have multiple root
nodes) we can use a more terse language
such as yml:

#+BEGIN_SRC yaml
literal: How are you
patterns:
  - How * you
  - How are you *
#+END_SRC

\toReview
Finally we can extend the pattern functionally by simply using regex support.
This will make the patterns more terse and even precise:

#+BEGIN_SRC yaml
literal: How are you
regex: How ([a-z])\w+ you(.*)
#+END_SRC

\toReview
This only matches a single word and allows for trailing characters.
Note that the patterns example would've also matched "How did this became you?",
since any character would've matched the star.
Regexes can be much more precise in specifying what a star is, although they are
also a lot more difficult to learn. Therefore both methods will be supported.

\todo[inline]{If we have time we can compare information per character between the new and old method, going from 251 to 131 is quite an improvement}
# https://developer.jboss.org/wiki/FunctionalProgrammingInDrools

*** Re-adding connections
\drafting
Originally we had the idea to re-add the connections in the following way:

#+BEGIN_SRC yaml
literal: How are you
to:
  - reply_good
  - reply_bad
  - ask_mediine
#+END_SRC

where the items in to linked to other symbol files. 
However we later decided to group all connections of a particular scene into 
a single file.
There were two reasons for this, the first one is that we quickly found out
that connections are much more complex than simple one directional links.
They for example also need to contain perlocutionary values and a mark for 
which agent can access the connection (default being all agents).
The second reason was more incedental, it turns out that if you group all
connections into a single file you can get a quick glance of which connections
are made.
Take as an example the following file:
#+BEGIN_SRC yaml
from:
 - ask_reason_here
to:
 - restricted_to: patient
   symbol: need_medicine
 - restricted_to: patient
   symbol: broken_arms
 - restricted_to: patient
   symbol: feel_sick
---
from:
 - need_medicine
to:
 - restricted_to: doctor
   symbol: why_need
#+END_SRC


*** Automatic AIML to YAML
\toReview
AIML is a standard cite:aimlspec.
Note that the spec says: "AIML shall be compatible with XML.",
and xml is also a standard cite:xmlspec.
YAML happens to also be a standard format cite:yamlspec.
although its not a markup langauge, unlike XML and AIML,
but rather a data format cite:yamlvsxml.
The advantage of being "just a dataformat" is
that the syntax can become a lot more terse, while maintaining readability.
Because we have all these standards, its relativly easy to convert between
them. Because libraries exist for parsing them.

**** Legacy AIML
\toReview
An introduction of a new format is nice, but when this is done,
all the work in the old format can become obsolete.
This is obviously not desirable.
There are several ways of circumventing this.
First of all, one could add support of the legacy format to the code base.
The second method is creating a script that will help along the way with
conversion.

\toReview
We chose the second method because the first method will be much harder,
as it will re-introduce the problems we had with AIML in the first place.
The second method provides the user an oppertunity to make their AIML script
comply to the new standard, whilst also reducing the effort it takes.

\toReview
To make the conversion we need to point out some structural observations about
differences between AIML and YAML.
Firstly AIML works strictly from the perspective of the bot.
There is no deliberation about what the user is thinking.
Therefore we can't model what we expect the doctor to say after a patient
uttered something in the test scenario because this information isn't encoded.
You may argue that the mechanism which could be used for this are the /that/ tags.
But they are just an additional restriction on the pattern matching,
ie: If user uttered regex *and* he said /that/ before, we can say template.
Alternativly you count the injection of types (in S-AIML),
but this isn't a formal encoding in AIML itself, but rather a way of informing
drools what's goin on.

**** Mapping AIML
\toReview
What we can do is extract the patterns, and their respective literals into
symbols. For example we have the following category:

#+BEGIN_SRC xml
  <category>
      <pattern>
          How long * pain
      </pattern>
      <template>
          <insert packageName="scenarios.large.global" typeName="SentenceSpoken" />
          <insert packageName="scenarios.large.timelapse" typeName="DurationPain" />
          For a while now
      </template>
  </category>
#+END_SRC

\toReview
So here there are implicitly defined two symbols and a connection.
The first symbol is pattern tag.
The second symbol is the template tag excluding the insertions.
The connection is from the pattern to the template which is restricted to
the patient.
So from this example we can define the following files,
with the file names in comments above:
#+BEGIN_SRC yaml
  # for_a_while_now.yml
  literal: "For a while now"
  # how_long_*_pain.yml
  literal: "How long * pain"
  # _connections.yml
  from:
   - "for_a_while_now"
  to:
   - symbol: "how_long_*_pain"
     restricted_to: patient   
#+END_SRC

\toReview
A lot of category elements simply mean to add a pattern to a symbol, for example:

#+BEGIN_SRC xml
  <category>
      <pattern>
          How long * pain *
      </pattern>
      <template>
          <srai>How long * pain</srai>
      </template>
  </category>

#+END_SRC

\toReview
So what we can do with the srai tags, if they exist, is simply checking if the
symbol exists and then add the pattern to that symbol.
Finally we have to consider the /that/ tags:

#+BEGIN_SRC xml
  <category>
      <that>Why is doctor Aarts not here I am one of his patients.</that>
      <pattern>
          surprised your doctor
      </pattern>
      <template>
          <insert packageName="scenarios.large.global" typeName="SentenceSpoken" />
          <insert packageName="scenarios.large.prehistory" typeName="BadAndLateExplanation" />
          I did expect him to be here, yes.
      </template>
  </category>
#+END_SRC

\toReview
Like we said before this is an extra filter upon the regex.
At the time of writing there was no way of representing /that/ tags. \todo{inline if this is added delete this}
A potential way of dealing with that tags is to add an extra field to the
connections:

#+BEGIN_SRC yaml
  before:
     who: patient
     said: "why_is_docter"
  from:
   - "surprised_your_doctor"
  to:
   - symbol: "expect_him_be_here"
     restricted_to: patient   
#+END_SRC

\toReview
But such a mechanism isn't implemented yet.

**** S-AIML specifics
<<Caviats>>
\toReview
After the conversion is finished, the bot will mostly work, with the added bonus
that the dialogue will be more overseeable.
However there are some caveats, especially in the previously introduced variant
called of AIML called S-AIML.

\toReview
In S-AIML types are injected to track progress of the scenario.
In contrast to the current scheme where the entire user utterance gets
inserted and its up to drools to make a sybmolic understanding from it.
To work around the issue of inserting types we actually have to generate
drools in case of a particular symbol inserted.
For this we can just use the low level reply mechanism, but rather than
replying we insert the specified type.

# A rather peculiar design decision to use JSON inside an xml document.
\toReview
Something which complicates this is that attributes can be set trough json
within the insert tags.
This can be handled inside the drools rules because the inserted types
always have setters.
So there could be java code generated for that.

\toReview
Scene tags can be represented trough folders.
So from the scene tag we can just extract the name and create a folder from it.
Making files from the children as described above.

**** Proof of concept
<<Conversion script>>
\toReview
To show that it is indeed easy to write a conversion script,
with above restrictions kept in mind we made a proof of concept.
This conversion script is located in the conversion folder from the root project
of the git project.
It is excpected to be executed from command line and provides several command
line arguments that are documented in the script itself.

\toReview
The usage of the script can be shown with:
#+BEGIN_SRC sh
  python main.py --help
#+END_SRC

\todo[inline]{Describe missing features}

** Processing

*** Drools

**** Scheduling in Dialogue
\toReview
The naive approach of looking at a dialogue is a round robin scheduling process.
Where someone utters a single sentence,
and then a single reply is made by the conversaton partner.
Even in our simple case study (section [[Personality influence case study]])
we discovered that a dialogue does not conform to a round robin based scheduling
process.
Sometimes the actors would conform,
but at other times one of the actors would say multiple utterances.

\toReview
There are several not so elegant ways to implement this without performing
to much chagnes to the code base.
First, you could just modify the case study to not do this and let be a
'coincedental' round robin process.
Obiously this would 'work',
but modifying once's predetermined tests is something which ought to be avoided
unless there is a good reason to do so.
Second of all you could just create a symbol with a huge,
multisentence literal reply.
This isn't really how symbols are meant to be used, but you could do that.

\toReview
But then our minds started to wonder,
how difficult would it be to emulate this?
To just make the bot properly utter multiple symbols.
What if we just re-insert every reply by the bot into the drool facts,
and let the personality decide how to reply to itself.
To prevent an infinite loop we would preffer alteration by default in the
irrational functions.

\toReview
So after this change we could model ENFP correctly, 
but we found another use in this new implementation.
Since all rational functions now shared a behavrior:
Preffer alteration of actors between levels default,
we could make some more general unit tests.

*** Personality

** Data structures
\todo[inline]{ I think this is an architecture thing ... }
\drafting
<<Data structures>>
There are 4 main datastructures, however one is forgotton once the
initialization phase has been completed.

\drafting
All data structures used in the new implementation are immutable.
This means that as soon as construction is finished of a data structure,
the public api never changes.
To get a change into a data structure a copy has to be made.

\drafting
The reason for doing this is that immutable data structures can used amongst
multiple threads without the use of any locks.
\todo{cite this}
This is an extremely desirable property to have especially with the drools
engine since it can assume arbitrary execution of the rules, depending on which
scheduler you choose.

\drafting
Another advantage is that in combination with pure functions, unit testing
becomes really easy to do.
A problem one often has with unit testing is that you need to get your objects
into the right state to get tested.
\todo{cite this}
However since our objects only have ever one state,
it becomes rather easy to get them in the right state.

\drafting
It should be noted however that trying to do this in java requires quite some
discipline.
Practices such as creating getters and setters can be avoided in favour of
final public fields, if the type of those fields are immutable.
However since java collections aren't immutable \todo{cite}
they always need to be private
and be shallowly copied on construction.
Then to expose the data one should create a copy of the collection, or, expose a
stream (which is also immutable). \todo{cite}

*** Symbol graph 
\todo{this is the old chapter on data structures, we should check what can be salvaged}
 <<Symbol graph>>
 \drafting
 For our bot to do deliberation we need to be able to tell it which meanings
 can logically follow which other meanings. To do this we use a meaning graph.
 However there are multiple ways to represent this.
 First of all we have intrusive graph representation, where the edges a node goes
 to is put inside the node. Non intrusive representation is where edges are
 stored as node tupples in a separate list, note that these are usually directed,
 since you can make an edge undirected by just adding another tupple with the
 values swapped, but if they're all undirected you cannot represent directed
 connections without introducing more terms.

 \drafting
 There are arguments for both representation, an intrusive graph
 representation is more easy to understand for humans, however a non intrusive
 representation is more elegant and can handle things such as backward chainging
 more easily (since not all nodes have to be searched in this case).

 \drafting
 Since we need to do both backward Chaining /and/ make it easy understandable for
 humans, we will reprsent the graph as intrusive in the AIML reprsentation, but
 as non intrusive in memory for easy backward and forward chaining.

 \todo{AIML with graph stuff}

*** Believes
\drafting
We have already quite extensively talked about believes in section
[[Believes overview]] for example.
*** Symbol database
\todo[inline]{This is just wrong}
\drafting
The symbol database is the data structure that is forgotten once construction is
complete.
This may seem strange, but it is important to keep in mind that this database
only refers to symbols by file path.
The symbols themselves aren't forgotten, just these file paths.
This database is extensively used during the initialization phase to construct
both the pattern database and the connection database.
After the initialization phase the pattern and connection databases both never
change.
This is enforced (unless reflection is used to break open private fields)

*** Pattern Database
\drafting
The pattern database is a database of pattern lists. These patterns are tied
to the relevant symbols.
The lists are tied to scene names. If the active scene does not contain any
relevant pattern, the other scenes are searched.
There could be a possible extension where the connected scenes get searched
first.
the user utterance gets inserted all associated symbols of the matched patterns
get inserted as a parsed utterance.

*** Connection database
\drafting
With the parsed utterance the connection database can doe actuall deliberation.
Because this database has as key symbols and as values a list of connections
which have to field that go to symbols.
On this game tree deliberation can occur as described in section
[[Mapping to process]].

**** Actors
\drafting
To make sure certain strange situations don't occur, such as the patient asking
the doctor if the doctor is sick, we added actor based restrictions on
connections.
Currently however this implementation is rather limited, it only allows for an
"any" actor, which basically means any actor can say this, or a specific actor.
This could be extended with some kind of role system,
where more fine grained groups of restrictions can be specified.
For example both the patient and family member should be able to ask the doctor
what's wrong with the patient, however the doctor not.

** Testing
\toReview
To verify our implementation's correctness we use several testing methodologies.
First at the lowest level we use the so called unit tests to verify individual
Jungian functions work as they were described in the thesis.
We test these individually by creating specific believes and a dialogue tree for
input, then we check the output against an expected believe base and dialogue
tree.

\toReview
Integration tests are done by combining parts of the application with each
other and seeing if the input and output is acceptable.
For example we would create a full personality and then see if it replies in the
way we want. By firing up the drools engine.

\toReview
On the highest level we have the so called validation tests.
With these we check if the program we created complies to what we described in
the theory.
To do this a scenario is implemented and the tests are done upon it.

*** Unit tests
\toReview
Unit tests (code that tests code each build)
provide extra guarantees upon behavior where type can't do this.
In essence unit tests are an extension of the type checker.

\toReview
Static typing provides better correctness guarantees than just unit tests cite:unitvsstatic.
It follows that if you have a stronger type system you need fewer unit tests,
Java however has a rather basic type system cite:javavsscala,javavshaskell.
Therefore we implemented over 70 unit tests in the code.
These are executed each build.
A framework called JUnit cite:junit is used to integrate with maven and execute
the tests on compile time.
It also has a handy assertion API.

\toReview
The unit tests don't cover every aspect of the application but are
mainly focused upon the Jungian Functions and their data structures.
For example the Te tests look like this:

#+BEGIN_SRC java
  public class TeTest extends AJungFuncTest {
      @Override
      public JungianFunction getTestTarget() {
          return JungianFunction.Te;
      }
      @Test
      public void no_goals_look_for_ones_that_change_scene(){
          Believes inputBelieves = believes.setGoals(Collections.emptySet());
          builder.addOption(MockBelievesFactory.hellos);
          Utterance expected = builder.addOption(MockBelievesFactory.needmedicine);
          builder.addOption(MockBelievesFactory.whyhere);
          DialogueTree inputTree = builder.getCurrent();

          JungFuncArgs result = apply(inputBelieves, inputTree);

          believes = inputBelieves;
          believesShouldRemainTheSame(result.believes);

          Assert.assertEquals("the scene changing dialogue is preffered",
              Optional.of(expected),
              result.tree.getPrefferedUtterance()
          );
      }
  }

#+END_SRC

\toReview
So in getTestTarget we define which function is under test,
which is handled by the parent class.
Then as can be seen by its annotation, the method
`no_goals_look_for_ones_that_change_scene` defines a test.

\toReview
Tests generally follow a similiar pattern, first an expectation is build,
then the input is created, the function to test is called, trough the `apply`
method in this case, and finally the expectation is asserted.
If the expectation isn't upheld an error will be shown at compile time.

\toReview
Testing is one of the few places we use inheritance.
The reason for this is that testing often involves a lot of boilerplate,
and inheritance provides a really cheap way of getting rid of that.

\toReview
We also provide a builder for making the dialogueTree.
This is a mutable wrapper around the dialogue tree, again to reduce boilerplate.
This boilerplate isn't so much a problem in the Jungianfunctions themselves
because they often deal in terms of collections, or just want to add one.
But with tests you want to be specific.

\toReview
$T_e$ only has one test that is specifically aimed at it, but there are also
several shared tests unleased upon it.
For example we know that rationale functions should preffer alteration if all
things are equal.
So we wrote a test for this and just applied the same test on all rationale
functions.

*** Validation test
\toReview
To do validation tests we will try to model the scenario presented as an example
in section [[Personality influence case study]].
To do this we go trough each utterence in the dialogue and see if they have
unique meanings. These we then translate in their respective symbol files.
If we for example look at the conversation between sander and the doctor seen
in table [[tab:sander-conv-doct]], we for example have two greetings. "Hi" and "Hello".
So we put this in a symbol named greeting under the first scene "introduction".
Therefore greeting is a symbol $s^{\text{greeting}}$ of
\[s^{\text{greeting}}=(\{\text{"Hi"}, \text{"Hello"}\}, \text{"introduction"})\]
We continue doing this process for all the utterances in this dialogue and the
next dialogues until all utterances have at least their own unique symbol, 
or share one with similar utterances in meaning.

\toReview
Next up is deciding which of the symbols should be connected with each other
trough connections.
To decide this we again take the case study dialogue found in 
example dialogue.

\toReview
After this we run the test against the scenario. This can be seen in section:
[[Test Results]]. There were some issues with the scenario itself,
however most issues we found were with the functions and their interactions.
The biggest issues we found were:

1. No low level replies.
2. No way of making the bot utter multiple symbols.
3. The height of which functions ought to operate wasn't well thought out.
4. $T_i$ didn't have good default behavior

* In conclusion
\todo[inline]{Discussion, comparison}
** Future work
*** Give the bot some semantic understanding
\drafting 
A possible alterantive is using cite:moschitti2004study, to parse the semantics
of a sentence. Then a graph can be used to determine how to predicates
corrolate logically with the arguments being just fill in.

\drafting
Something such as cite:mccord1990slot could also be used to get a more deeper 
"understanding" of the user message. However introducing something like that
would require a much more advanced reasoning process. No longer are we
dealing with sentences but instead we use words.
Symbols then would need to indicate how many arguments they need to be completed.

**** Variables in literals
\toReview
A first step would be to add variables to the symbols. Such as a name of an
actor. I guess these things could be extracted from the social practice
which is still defined kindoff implicitly.
We could use a framework like this: http://www.stringtemplate.org/ to substitute
variables with their proper names. (ANTLR seems to strict however).

\toReview
If this is done in the literals however, care should be taken, since these
are also expended into regexes,
therefore all possible inputs of the literals should be available at the time
when the patterndatabase is build.

\toReview
There are 3 ways to create these variables, first of all just collect them
when parsing the symbols and connections,
secondly we insert them from an external source (ie from drools).
Thirdly we use a user defined structure.
For the actors we can collect, but for things such as the healthproblem we need
a user defined structure since drools is kindoff later in the init cycle.

*** More advanced learning
\drafting
Could consider extending the symbol graph dynamically.
For example create a new symbol every time you don't know something,
then ask about it and if the user replies with something you do know connect
them.
Currently there is limited support for "learning". but this is based on knowing
existing symbols, the extension would make symbols dynamically.
This could be done by more advanced language parsing techniques in combination
with an existing knowledge base such as OWL.

*** Multiple conversation partners
<<Multiple conversation partners>>
\drafting
This can be supported by the current architecture technically I think.
Issues that are not addressed for this however are for example timing of replies
and making sure that the bots won't chit chat with each other forever ignoring
the user.
This probably requires that the actor class to be more specific in this case.
Connections could for example be restricted to coalitions of actors rather than just
single actors.
In other words, the yml files shouldn't restrict to any or singular actor,
but rather to groups of actors.

\drafting
Another major issue would be deciding which agent is being talked to by the
user. This could maybe be solved by giving very fine grained restrictions,
but this would become quickly hard to manage with more than 3 agents.
Perhaps a heuristic can be used where the user would have to address agents
by name or have separate buttons to talk to specific agents.

*** Multiple symbols matched
\drafting
better support for if multiple symbols are matched.
Currently just the first one is used. So we truncate the rest.
However if multiple symbols are uttered we could do more advanced heuristics,
such as trying to figure out which one was really meant by using information
theory to select the most precise one.
However this precision also should be encoded or derived somehow
(for example, more words means more information).

*** Social practice support
\drafting
Better social practice support. This is mainly due to a lack of propper
description of the social practice, but it could eventually make the bot very 
salable in use cases.

*** Use GPU for pattern matching

\drafting
It is also entirely possible to make the bot a lot more faster in response time
by using the GPU for regex matching.

*** Graphical scenario editor

\newpage
\appendix

* References
<<bibliography link>>

bibliographystyle:unsrt
bibliography:refs.bib


\newpage
* Building
\todo[inline]{Document about python scripts & shell scripts (how to use), make sure succerrer can take it on easily}
<<building>>
\cleared
To build this project two hurdles need to be overcome, because the
server uses a starkly different tool chain than the client.
In this appendix we will record how the application can be build.
It may seem trivial but the Java EE world is incredibly complex.
We assume a unix-like operating system with a package manager.

** Client
\cleared
The client is relatively easy too setup since its build with a
monolithic environment.
Therefore you need to have the unity editor.
The only issues with the client were an incomplete merge and a dangling
import that produced build errors.
Also note that there exists a Linux editor,
its just not officially supported (yet) but the latest version can be found [[https://forum.unity3d.com/threads/unity-on-linux-release-notes-and-known-issues.350256/][here.]]
Scroll all the way down for the latest release.

\toReview
Note that the unity client currenlty doesn't support multiple replies from an
agent, because the reply is just inserted in a label, rather than showing a chat
history.
** Server
\cleared
The server runs on Java, therefore the first step is to install Java.
In our case java 8 was used. If your system uses portage you can use the
following command:

#+BEGIN_SRC sh
   # emerge dev-java/oracle-jdk-bin
#+END_SRC

*** Maven
\cleared
Then maven needs to be installed since gradle didn't work ([[Gradle attempt]]):

#+BEGIN_SRC sh
   # emerge dev-java/maven-bin
#+END_SRC

\cleared
Maven is the package manager for java software, it downloads and installs
dependencies (and dependency dependencies) automatically based on xml
configuration.
Do note that to use maven you need to setup a \url{\string~/.m2/settings.xml}
file. I based mine on [[https://maven.apache.org/settings.html][this]] with help of [[https://maven.apache.org/ref/3.3.9/maven-settings/settings.html][this.]]
The active profile should have the name local so that the local profile is used
in the maven project (in this case /local/).
Otherwise the wildfly plugin won't deploy the application.
To test if maven works go to the \url{communicate2/communicate/communicate_server}
folder and execute:

#+BEGIN_SRC sh
 $ mvn compile
#+END_SRC

\cleared
If no errors occur it means the settings are configure right.
However we are not done yet since the resulting binary is not executable.
It is something called a servlet which is an api for server like applications.
To use this binary, we need an application server.
Our maven repository and code base has been configured towards /wildfly/,
so we will use that.

**** Gradle attempt
<<Gradle attempt>>
\cleared
it was attempted to replace maven with gradle, since its a lot less verbose
than maven and easier to setup however doesn't have the picketlink extension
which wildfly requires.
Therefore gradle was abandoned and the maven tool was used instead.

*** Get wildfly
\cleared
Download wildfly from [[http://wildfly.org/downloads/][here]], choose the full web distribution
(if you choose the servlet one you'll run into trouble since it doesn't have the
datasource subsystem, it took about two days to figure that out).
Extract this download somewhere which we will call hence forth $WILDFLY.

*** Setup datasource
\cleared
Now its time to configure the persistent datasource.
The code base can handle sessions,
but to deal with user registration and logins and such we need a database.
There are two methods, mariadb and the in ram storage.
Mariadb is what the online application uses and its probably better to
stick to that for active devlopment, but if you just want to have
a quick look at the server you should use look at section [[inmemorydb]].

*** Mariab setup
<<Mariadb setup>>
\cleared
So first install mariadb (or mysql, they are the same, except mariadb has better
defaults):
#+BEGIN_SRC sh
  # emerge dev-db/mariadb
#+END_SRC
\cleared
Then we need to setup the user and database:
#+BEGIN_SRC sh
    $ mysql -u root
    > create database salve;
    > GRANT ALL PRIVILEGES ON salve.* To 'salve'@'localhost'
    IDENTIFIED BY 'salve';
#+END_SRC

*** Mariadb driver
\cleared
Now we need to make the application server aware of the database.
To do this we first need to install a driver from here [[http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.33/mysql-connector-java-5.1.33.jar][here]],
then copy this jar into
\url{$WILDFLY/modules/system/layers/base/com/mysql/driver/main}
you probably need to make everything after base.
Also create another file called \url{module.xml} with the following content:
#+BEGIN_SRC xml
<module xmlns="urn:jboss:module:1.3" name="com.mysql.driver">
 <resources>
  <resource-root path="mysql-connector-java-5.1.33.jar" />
 </resources>
 <dependencies>
  <module name="javax.api"/>
  <module name="javax.transaction.api"/>
 </dependencies>
</module>
#+END_SRC

*** Wildfly datasource
\cleared
Now the /driver/ is installed we need to configure it as a datasource.
To do this we move to \url{$WILDFLY/bin}.
Then execute the following commands:
#+BEGIN_SRC sh
   $ chmod +x add-user.sh jboss-cli.sh standalone.sh
   $ ./standalone.sh &
   $ ./jboss-cli.sh --connect controller=localhost
   --command="/subsystem=datasources/jdbc-driver=mysql:add(driver-name="\
   "mysql,driver-module-name=com.mysql.driver,driver-class-name="\
   "com.mysql.jdbc.Driver)"
   $ ./add-user.sh
   $ xdg-open localhost:9990
#+END_SRC

\cleared
That last command should open the browser. Click then
Configuration \to subsystems \to datasrouces \to non xa \to add  \to mysql \to next.
The name should be GameDS and the JNDI name should be java:/GameDS,
now click: next \to detect driver \to mysql.
The url should be \url{jdbc:mysql://localhost:3306/salve}, the username and pass
should both be salve, now click next \to finish.

*** Deploying
\cleared
first go to the \url{communicate2/communicate/communicate_server} folder.
Then to deploy the application the following command is used:

#+BEGIN_SRC sh
 $ mvn wildfly:deploy
#+END_SRC

\cleared
If your build gets stuck because it tries to find communicate jars from the 
internet it can help to go to the root folder and execute:

#+BEGIN_SRC sh
 $ mvn compile
#+END_SRC

*** Alternative: in memory db
\cleared
<<inmemorydb>>
Now there is a choice to be made, you can either choose to use maria db
or try and point the appliation to the in ram storage of wildfly.
To do this go to:
\url{communicate2/communicate/communicate_server/src/main/resources/META-INF}
and then replace everything with:
#+BEGIN_SRC xml
  <?xml version="1.0" encoding="UTF-8"?>
  <persistence version="2.1"
  xmlns="http://xmlns.jcp.org/xml/ns/persistence"
  xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"
  xsi:schemalocation=
  "http://xmlns.jcp.org/xml/ns/persistence 
http://xmlns.jcp.org/xml/ns/persistence/persistence_2_1.xsd"
  >
      <persistence-unit name="salve_persistence_unit"
          transaction-type="JTA">
          <jta-data-source>java:jboss/myDs</jta-data-source>
          <properties>
              <property name="hibernate.dialect"
                        value="org.hibernate.dialect.H2Dialect" />
              <property name="hibernate.max_fetch_depth" value="3" />
              <property name="hibernate.hbm2ddl.auto" value="update" />
              <property name="hibernate.show_sql" value="true" />
          </properties>
      </persistence-unit>
  </persistence>
#+END_SRC

** Ubuntu issues
At some point my laptop crashed and I had to fall back to an ubuntu based
distribution.
Installing mysql came with the gotcha, the default root on mysql is only
accessible from:
#+BEGIN_SRC sh
sudo mysql -u root
#+END_SRC
I thought I could do this as the normal user but it appears this not the case.
Also an online forum said that you had to run
#+BEGIN_SRC sh
sudo mysql_install_db
#+END_SRC
However I think apt handles that for you.

**** utf8 problem
\cleared
I got an error: \\ 
#+BEGIN_SRC sh
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException:
  Specified key was too long; max key length is 767 bytes
#+END_SRC
This is because it uses an utf8 encoding (see [[http://stackoverflow.com/questions/10748155/specified-key-was-too-long-max-key-length-is-767-bytes][this website]]).
To solve this change the tables to use a smaller utf8 instead.
In `/etc/mysql/mariadb.conf.d/50-server.cnf` change the variables to

#+BEGIN_SRC sh
character-set-server  = latin1                                                            
collation-server      = latin1_swedish_ci
#+END_SRC

\cleared
We also need to modify `/etc/mysql/mariadb.conf.d/50-client.cnf`
with:
#+BEGIN_SRC sh
default-character-set = latin1
#+END_SRC

\cleared
After which we do a restart of the mysqld:

#+BEGIN_SRC sh
sudo systemctl restart mysql
#+END_SRC
\cleared
Note: make sure to stop your running wildfly instance, otherwise wildfly will keep it alive

\cleared
Now drop the entire salve database as root, the tables were created with the
wrong charset so we need to just remove them all.
#+BEGIN_SRC sh
mysql -u root -ppassword -e "DROP DATABASE salve;:
#+END_SRC

\cleared
Now recreate the database as was done in section [[Mariadb setup]] (alternativly
logging in as root and pressing arrow up a couple of times should get you the
right commands).

** Notes
\cleared
If you're located in the communicate_server folder, The rebuild everything
command is:
#+BEGIN_SRC sh
rm -R ~/.m2/repository/cnruithof; (cd ../ && mvn clean && mvn install) && mvn wildfly:deploy
#+END_SRC
There is also a shell script "rebuild.sh".
This will re-install the entire project thereby also rebuilding all dependencies.
mvn clean in there for good measure.

\cleared
There is a python client script for quick debugging, therefore its unnecessary
to keep unity running (or use at all).

\newpage
* Test Results
<<Test Results>>
\toReview
All these test follow the scenario which is presented as case study in section
section [[Personality influence case study]].
Each time all tests are executed to prevent regressions, even if they did manage
to pass a test cycle before.

** Initial test
| Commit | 65e704a0f9ec6f5d7052e0de285a79baf420db7d |
| Date   | 2017-03-28 |

\toReview
We believe most of the higher level reasoning is done at this point.
Therefore we attempted this acceptance test to see how well the system would
hold up.

*** INTJ
| Who      | Scripted Utterance                                | Issues |
|----------+---------------------------------------------------+--------|
| Doctor   | Hi                                                |        |
| /Sander/ | /Hello/                                           |      1 |
| Doctor   | How can I help you?                               |      2 |
| /Sander/ | /I have a back pain./                             |        |
| Doctor   | When did this first occur?                        |        |
| /Sander/ | /When I lifted a heavy object./                   |        |
| Doctor   | Oh, yes then you need some pain killers for this. |        |
| /Sander/ | /Thank you doctor/                                |        |

\toReview
1. Actually said "How are you?", This is unexpected,
   however since replying with hello is useless to INTJ (it'll just loop back),
   such a reply may actually be better for progress high level process.
   However, this doesn't mean this is right, the intention was to make the hello
   reply be done on a lower level (ie first item said and understanding matches
   hello short-circiuts into replying hello)
2. We use the other scripted response (perhaps it thinks its ENFP),
   this didn't match however,
   turns out this symbol didn't have any regexes,
   after adding the regexes it still didn't match,
   further investigation is required
   into the regexes why they don't match,
   I'm suspecting its either a construction problem (not all are added),
   Or some standard java regex problem.

*** ENFP
| Who     | Utterance                                                          | Issues |
|---------+--------------------------------------------------------------------+--------|
| Doctor  | Hi                                                                 |        |
| /Susie/ | /Hello/                                                            | 1      |
| /Susie/ | /How are you today doctor?/                                        | 1      |
| Doctor  | I'm good, how can I help you?                                      | 2      |
| /Susie/ | /I'm afraid I need some medicine/                                  |        |
| Doctor  | Medicine? Why do you need that?                                    |        |
| /Susie/ | /Well, I was watering the plants and all the sudden,/              | 3      |
| /Susie/ | /I got this pain in my back./                                      |        |
| /Susie/ | /Do you think I'm allergic to plants?/                             |        |
| Doctor  | Haha, no, I think we need to make a scan of your back.             | 4      |
| Doctor  | Because a watering can is a little to light to get back-pain from. |        |
| /Susie/ | /Of course doctor./                                                |        |
| Doctor  | Can you go to the hospital next Friday at 13:00?                   |        |
| /Susie/ | /Yes, I will go then./                                             |        |

\toReview
1. Skipped saying hello (again we miss the shortcircuit rule, same as INTJ)
2. Didn't match with I'm good, probably samme issue as INTJ.
3. Only says the first sentence. This is because we don't keep on popping
   sentences from the dialogue tree if the patient actor still is preferred.
4. We skipped the intermidate sentences (because they didn't pop),
   What we would expect is to get some confusion back after doing this.
   This didn't happen so rules to give confusion when skipping parts of the
   graph should be implemented.

*** ISTP
| Who     | Utterance                                                         | Issues |
|---------+-------------------------------------------------------------------+--------|
| Doctor  | Hi                                                                |        |
| /Chris/ | /Hello/                                                           |      1 |
| Doctor  | How can I help?                                                   |        |
| /Chris/ | /I have back pain doctor./                                        |      2 |
| Doctor  | When did this first occur?                                        |        |
| /Chris/ | /Well I was watering the plants,/                                 |      3 |
| /Chris/ | /Perhaps I put to much water in the watering can/                 |        |
| Doctor  | Yes, that could be the case.                                      |        |
| Doctor  | However I would like to make a scan of your back just to be sure. |        |
| /Chris/ | /Can't you just give some pain killers to help me?/               |        |
| Doctor  | Yes but that will only work temporary.                            |        |
| Doctor  | So let's plan a scan at the hospital next Friday at 13:00?        |        |
| Doctor  | I can give you some pain killers meanwhile.                       |        |
| /Chris/ | /Okay, thanks doctor/                                             |        |

\toReview
1. Again no hello
2. Replied with, I'm afraid I need some medicine, I think this is because Ti
   is the first function, and I think the two pass effect isn't implemented
   correctly yet. It just goes deeper now rather than finding the right level
   to modify. Since telling about back pain is a goal and we have a unit test that
   ti prefers goals its almost surely this broken two pass behavior.
3. It says when lifting a heavy object, but I'm pretty sure it just always picks
   the first option because two pass logic is broken.
   So I'll end this test prematurely.

** Second test

| Commit | 1de05450c187c51df780a975cc4f0cec7c69dba1 |
| Date   |                               2017-04-05 |

\toReview
After solving the issues from the initial test, we redid them.

*** INTJ
| Who      | Scripted Utterance                                | Issues |
|----------+---------------------------------------------------+--------|
| Doctor   | Hi                                                |        |
| /Sander/ | /Hello/                                           |        |
| Doctor   | How can I help you?                               |        |
| /Sander/ | /I have a back pain./                             |        |
| Doctor   | When did this first occur?                        |        |
| /Sander/ | /When I lifted a heavy object./                   |        |
| Doctor   | Oh, yes then you need some pain killers for this. |        |
| /Sander/ | /Thank you doctor/                                |        |

\cleared
Conversation went according to script.

*** ENFP
| Who     | Utterance                                                          | Issues |
|---------+--------------------------------------------------------------------+--------|
| Doctor  | Hi                                                                 |        |
| /Susie/ | /Hello/                                                            |        |
| /Susie/ | /How are you today doctor?/                                        |      1 |
| Doctor  | I'm good, how can I help you?                                      |      2 |
| /Susie/ | /I'm afraid I need some medicine/                                  |        |
| Doctor  | Medicine? Why do you need that?                                    |        |
| /Susie/ | /Well, I was watering the plants and all the sudden,/              |        |
| /Susie/ | /I got this pain in my back./                                      |      3 |
| /Susie/ | /Do you think I'm allergic to plants?/                             |        |
| Doctor  | Haha, no, I think we need to make a scan of your back.             |        |
| Doctor  | Because a watering can is a little to light to get back-pain from. |        |
| /Susie/ | /Of course doctor./                                                |        |
| Doctor  | Can you go to the hospital next Friday at 13:00?                   |        |
| /Susie/ | /Yes, I will go then./                                             |        |


\toReview
1. Wasn't uttered, probably because we handle hello as a low level reply.
   Perhaps we should let replies also be passed to themselves so higher level
   functions have a chance to interact?
   (note that alteration is the default, so unless a goal is directly below it
    nothing will be said)
2. We just said 'how can I help you' instead.
3. Goes into "Perhaps I put to much water in the watering can".
   Probably because ENFP is an Fi rather than Fe, Fi is a learning function.
   We should analyze what has been learned at this point.
   It appears also that in either case no values are attached.
   After this point the bot follows the ISTP script.
   This is wrong, Fe is the learning function, Fi uses perlocutionary values.
   So this was easily fixed by modifying the perlocutionary value utility.

*** ISTP
| Who     | Utterance                                                         | Issues |
|---------+-------------------------------------------------------------------+--------|
| Doctor  | Hi                                                                |        |
| /Chris/ | /Hello/                                                           |        |
| Doctor  | How can I help?                                                   |        |
| /Chris/ | /I have back pain doctor./                                        |        |
| Doctor  | When did this first occur?                                        |        |
| /Chris/ | /Well I was watering the plants,/                                 |      1 |
| /Chris/ | /Perhaps I put to much water in the watering can/                 |        |
| Doctor  | Yes, that could be the case.                                      |        |
| Doctor  | However I would like to make a scan of your back just to be sure. |        |
| /Chris/ | /Can't you just give some pain killers to help me?/               |        |
| Doctor  | Yes but that will only work temporary.                            |        |
| Doctor  | So let's plan a scan at the hospital next Friday at 13:00?        |        |
| Doctor  | I can give you some pain killers meanwhile.                       |        |
| /Chris/ | /Okay, thanks doctor/                                             |        |


\toReview
1. Says 'when lifting a heavy object'.
   This derails the entire script so the test was stopped.
   The reason for this became apparent after a full tree dump after each function.
   First time the Ti function has nothing to sort, so it does nothing.
   Se generates all available options in the order they came,
   Ni just goes down whatever Se preferred.
   Fe sorts everything twice, but most have no perlocutionary value.
   Then once we come back to Ti, the most obvious choice is of course the first
   option provided by Se, since it was expended by Ni, twice.

\toReview
This is obviously not what we want, since Se is deciding which action is taken
here.
Se could try and get a sane ordering function. However it only has next
available and trying to use previous would lead to a stack overflow.
Ti probably should get a better ordering mechanism.
For example rather than using the DialogueTree for direct options we just
list all available options.
We ended up doing this but note that this breaks the design idea of the
architecture. 

\toReview
To fix this properly we need to reconsider the design at a deeper level,
for example give ti the opportunity somehow in with direction Ne will go down.
A potential better way of solving this would to allow irrational to either
produce on the level of previous irrational,
or if the produced action already exists, go one lower.
But since we're starting to run into time constraints we just leave it at this
'solution'.

** Third test

| Commit | 893f54f6942a65e3c22126949091d096c3691445 |
| Date   |                               2017-04-10 |

\toReview
After solving the issues from the second test we did another run of all tests.
Even though this time we tested quite closely against the issues from the
previous test, going trough the entire battery is necessary to prevent
regressions.


*** INTJ
| Who      | Scripted Utterance                                | Issues |
|----------+---------------------------------------------------+--------|
| Doctor   | Hi                                                |        |
| /Sander/ | /Hello/                                           |        |
| Doctor   | How can I help you?                               |        |
| /Sander/ | /I have a back pain./                             |        |
| Doctor   | When did this first occur?                        |        |
| /Sander/ | /When I lifted a heavy object./                   |        |
| Doctor   | Oh, yes then you need some pain killers for this. |        |
| /Sander/ | /Thank you doctor/                                |        |

\cleared
Conversation went according to script.

*** ENFP
| Who     | Utterance                                                          | Issues |
|---------+--------------------------------------------------------------------+--------|
| Doctor  | Hi                                                                 |        |
| /Susie/ | /Hello/                                                            |        |
| /Susie/ | /How are you today doctor?/                                        |        |
| Doctor  | I'm good, how can I help you?                                      |        |
| /Susie/ | /I'm afraid I need some medicine/                                  |        |
| Doctor  | Medicine? Why do you need that?                                    |        |
| /Susie/ | /Well, I was watering the plants and all the sudden,/              |        |
| /Susie/ | /I got this pain in my back./                                      |        |
| /Susie/ | /Do you think I'm allergic to plants?/                             |        |
| Doctor  | Haha, no, I think we need to make a scan of your back.             |        |
| Doctor  | Because a watering can is a little to light to get back-pain from. |        |
| /Susie/ | /Of course doctor./                                                |        |
| Doctor  | Can you go to the hospital next Friday at 13:00?                   |        |
| /Susie/ | /Yes, I will go then./                                             |        |

\cleared
Conversation went according to script.

*** ISTP
| Who     | Utterance                                                         | Issues |
|---------+-------------------------------------------------------------------+--------|
| Doctor  | Hi                                                                |        |
| /Chris/ | /Hello/                                                           |      1 |
| Doctor  | How can I help?                                                   |        |
| /Chris/ | /I have back pain doctor./                                        |        |
| Doctor  | When did this first occur?                                        |        |
| /Chris/ | /Well I was watering the plants,/                                 |        |
| /Chris/ | /Perhaps I put to much water in the watering can/                 |        |
| Doctor  | Yes, that could be the case.                                      |        |
| Doctor  | However I would like to make a scan of your back just to be sure. |        |
| /Chris/ | /Can't you just give some pain killers to help me?/               |      2 |
| Doctor  | Yes but that will only work temporary.                            |        |
| Doctor  | So let's plan a scan at the hospital next Friday at 13:00?        |        |
| /Chris/ | /Yes, I will go then./                                            |        |
| Doctor  | I can give you some pain killers meanwhile.                       |        |
| /Chris/ | /Okay, thanks doctor/                                             |        |

\toReview
1. Said hello twice?!
   This was caused by the reinserting part of the shortcut, it used anyactor
   rather than self. Fixing this caused a regression in ENFP which wouldn't
   ask about the doctor's day anymore.
   This was solved by adding an extra value to that connection
   (making it more appealing for Fi).
2. Said "of course" rather than give me some pain killers
   After inspection of the scenario it appears that there is no reason to say
   "can't you give me painkillers" because there weren't any connection
   leading out of it.
   I also made give painkillers a low priority goal, to force ISTP in that
   direction.

** Fourth test

| Commit | 8ea894d0da28addd8067c341e842ca2d91296586 |
| Date   |                               2017-04-10 |

\toReview
Its the same day and we try another round to try and pass this.
Although we modified ISTP slightly, because it had to parse a large amount and
produce a single reply for no obvious reason. Unlike ENFP which for example is
expected to produce multiple replies because its talkative.

*** INTJ
| Who      | Scripted Utterance                                | Issues |
|----------+---------------------------------------------------+--------|
| Doctor   | Hi                                                |        |
| /Sander/ | /Hello/                                           |        |
| Doctor   | How can I help you?                               |        |
| /Sander/ | /I have a back pain./                             |        |
| Doctor   | When did this first occur?                        |        |
| /Sander/ | /When I lifted a heavy object./                   |        |
| Doctor   | Oh, yes then you need some pain killers for this. |        |
| /Sander/ | /Thank you doctor/                                |        |

\cleared
Conversation went according to script.

*** ENFP
| Who     | Utterance                                                          | Issues |
|---------+--------------------------------------------------------------------+--------|
| Doctor  | Hi                                                                 |        |
| /Susie/ | /Hello/                                                            |        |
| /Susie/ | /How are you today doctor?/                                        |        |
| Doctor  | I'm good, how can I help you?                                      |        |
| /Susie/ | /I'm afraid I need some medicine/                                  |        |
| Doctor  | Medicine? Why do you need that?                                    |        |
| /Susie/ | /Well, I was watering the plants and all the sudden,/              |        |
| /Susie/ | /I got this pain in my back./                                      |        |
| /Susie/ | /Do you think I'm allergic to plants?/                             |        |
| Doctor  | Haha, no, I think we need to make a scan of your back.             |        |
| Doctor  | Because a watering can is a little to light to get back-pain from. |        |
| /Susie/ | /Of course doctor./                                                |        |
| Doctor  | Can you go to the hospital next Friday at 13:00?                   |        |
| /Susie/ | /Yes, I will go then./                                             |        |

\cleared
Conversation went according to script. 

*** ISTP
| Who     | Utterance                                                         | Issues |
|---------+-------------------------------------------------------------------+--------|
| Doctor  | Hi                                                                |        |
| /Chris/ | /Hello/                                                           |        |
| Doctor  | How can I help?                                                   |        |
| /Chris/ | /I have back pain doctor./                                        |        |
| Doctor  | When did this first occur?                                        |        |
| /Chris/ | /Well I was watering the plants,/                                 |        |
| /Chris/ | /Perhaps I put to much water in the watering can/                 |        |
| Doctor  | Yes, that could be the case.                                      |        |
| Doctor  | However I would like to make a scan of your back just to be sure. |        |
| /Chris/ | /Can't you just give some pain killers to help me?/               |        |
| Doctor  | Yes but that will only work temporary.                            |        |
| Doctor  | So let's plan a scan at the hospital next Friday at 13:00?        |        |
| /Chris/ | /Yes, I will go then./                                            |        |
| Doctor  | I can give you some pain killers meanwhile.                       |        |
| /Chris/ | /Okay, thanks doctor/                                             |        |

\toReview
Conversation went according to script. Note however that we split up the
utterances of the doctor compared to the scenario.
This is because there were to many different symbols in the input
(scan and give painkillers), this made the bot reply to both of them.


* Ideas
:PROPERTIES:
:UNNUMBERED: t
:END:
\todo[inline]{To circumvent typing errors of users we can put input trough a spell checker taking the first correction}
\todo[inline]{How to deal with long inputs? I think sentence splitting was mentioned?}
\todo[inline]{Perhaps we can use argumentation research on top of the graph to figure out more semantics (this statment is attacking that or inquirying this)}
\todo[inline]{Extracting information from meanings isn't very DRY right now, for example saying how is your foot? Could perhaps be splitup into statusInquiry + foot. However I don't know how to do this better right now, perhaps it could be a natural result of dubble matching but we lose the order inforamtion.}
** Rough planning.
*** Implement
**** that tags
\drafting
The that tags in AIML have a rather unforntunate name.
They refer to something *that* was said before the current pattern.
Therefore in our YAML semantics, they are a restiriction on connections.

\drafting
To model that tags we add an extra optional field to the RawConnections called
before, with two required fields, `who' and `said', who indicates the actor
and said indicates what was said:

#+BEGIN_SRC yaml
  before:
     who: patient
     said: "why_is_docter"
  from:
   - "surprised_your_doctor"
  to:
   - symbol: "expect_him_be_here"
     restricted_to: patient   
#+END_SRC

\drafting
However we can easily extend this mechanism by making the rawbefore type self
recursive with an optinal field before of its own type:

#+BEGIN_SRC yaml
  before:
     who: patient
     said: "why_is_docter"
     before:
        who: doctor
        said: "im_the_doctor"
  from:
   - "surprised_your_doctor"
  to:
   - symbol: "expect_him_be_here"
     restricted_to: patient   
#+END_SRC

\drafting
The default value of the before field will be no before. Which just means no
additional restrictions.
In this example making such an explicit definition won't be very constructive.
However since we need an identity (no-op) before value anyway for the default
restriction. Adding a recursive definition isn't that big of an extension.

\drafting
The AIML tag specification never defined when a that tag match would be
active cite:aimlspec_that. However it can be derived implicitly from the
reference cite:aimleference_that that it should always be on the previous
utterance.
We could add options for when an utterance was made, for example just now or
any previous utterence. However in the interest of time we won't do this.

**** scene switching
1. Make sure to retest
**** star
1. Find precise definition
***** match on symbol A, fill in symbol B problem
Figure out how to solve the "match at symbol A, fill in template from symbol B"
problem.
For example we do this with symbols as nameaspaces of the filled in
template.
But this becomes more difficult when an agent says "what is your name?",
which is replied with "my name is X", now in "hello X, how are you today",
how do we locate X from within a symbol?
Because the symbol isn't aware of different agents.
It could be done from "connection", where we say:
#+BEGIN_SRC yaml
from:
- my_name_is
to:
- symbol: hello_x
  restricted_to: patient
  templates:
    x: doctor.name # ??
#+END_SRC

But now we run into the problem how does the template name doctor.name get
filled in the first place?
We could say something like, okay, symbol templates that are matched are
always uttered by "somebody", so our namespace is just the
name of the utterer + symbol name:
#+BEGIN_SRC yaml
from:
- my_name_is
to:
- symbol: hello_x
  restricted_to: patient
  templates:
    x: doctor.my_name_is
#+END_SRC

So there still needs to be a mechanism for extracting multiple variables
from the regexes, because regexes support that.
But that's just syntax

*** Report
     In thesis we should also mention the effects of values for functions
*** Repair OCC module & scoring module
*** Additional experimentation?
     For this I wanna setup an experiment like thing, where we research what
     the system does etc.
     + Perhaps we should look in what cases the deeper functions have effect (if any)
     + Are slight differences in order relevant? [Ne, Ti] vs [Ti, Ne]

    I'm pretty sure current design doesn't work that great with 'deeper' levels,
    because we assumed irir or riri,
    but now the theory says riir or irri.
    We should analyze this difference I think


** Random thoughts
+ Goals should perhaps be able to be finished?
  + Ti and Te shouldn't preffer this double
+ Manuel made the argument this isn't BDI because we don't use a rule based
  engine. We should find a theoretical source that mentions this and then attack
  it by saying that we use pure functions or something? Alternatively the result
  of each function gets inserted at each step in the rule engine,
  therefore we could argue that aside from the actual processing step it *is*
  BDI.
  The laborious approach would be to refactor all the code to be in drools.
+ Manuel also said that social practices may not fit with this.
  For example when you expect an event in a particular scene,
  but I don't think this isn't true either since at every step of deliberation
  the information gets reinserted.
+ We should note the utterence is used by goals a little weird in that
  percolutionary values aren't used.
  Actually goals need less information, so an idea would be to splitup
  Utterences, but the issue is what to name the splitup (protoutterence?)
+ Perhaps it shouldn't be possible to skip to a certain part of a dialogue
  without punishment?
  Although this is handy for debugging right now.

** Tooling stuff
+ Protocol buffers could be used for protocol. Its strongly typed and has both
  C# and Java bindings
** Security
+ Don't send passwords as plain text over the network.
+ Use SSL.
+ On release, should get a security expert to do an audit. I'm just vaguely
  aware of security issues, but I've seen several already (above for example)
** Protocol
+ Start game and new game could be merged?
+ Why logging towards the client? This seems like a security hole and a
  possible performance bottleneck. (it maybe was Lucas doing debugging)
+ Perhaps a command message should be added, in previous version just certain
  patterns were rigged to do commands (which isn't very pretty). With a
  separate message you could just create another client (or button) to send
  commands

** Thesis
+ Add UML references?
+ Add "architecture quality attribute" references?
+ Could try and figure out which symbol medhi's paper used and then use the same
  ones? I don't know, seems like a little wastefull
+ We could discuss the dilemma of representing values as either class or enum.
  keeping enum for now cause that's what I already have. However it maybecome a
  maintenance problem
** Extra
+ Replace java.util.logger* with slfj
+ Repalce all std.out with propper logging calls
  (not that it matters as long as java.util is used)
+ I get the regular out of memory exception when deploying, this probaly means
  that there is a memory leak which doesn't get cleaned up at redoploy

  
